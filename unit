def mock_data_attribute():
    dictionary_attribute_input = {
        'data_observation_uuid':['0rf3h574-234i-x439-4hdc8dfkls87','r1f3h574-10x4-x439-4hdc8djfhy13','hdf1sd5y-se89-1m4n-9dja9ce8u4hu'],
        'data_observation_attributes_name':['radius','comment',None],
        'data_observation_attributes_value':['0.323','Noticed tear on contract face',None],
        'data_observation_attributes_adHoc': [None,None,None]
    }
    df_input = spark.createDataframe([dictionary_attribute_input])
    df_input.createOrReplaceTempView('raw_table_attribute')
    dictionary_attribute_expected = {
        'observation_uuid':['0rf3h574-234i-x439-4hdc8dfkls87','r1f3h574-10x4-x439-4hdc8djfhy13','hdf1sd5y-se89-1m4n-9dja9ce8u4hu'],
        'attributes_name':['radius','comment',None],
        'value':['0.323',None,None],
        'comment':[None,'Noticed tear on contract face',None]
    }
    expected_df = spark.createDataframe([dictionary_attribute_expected])




# Databricks notebook sourcefrom pyspark.sql import DataFrame, functions as F
from pyspark.sql.types import StringType, FloatType
from pyspark.sql.window import Window
import re

# COMMAND ----------

# MAGIC %run ./common_functions

# COMMAND ----------

def event_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
        -- where processed = False
    """)

    df_event = df_event.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_event.display()
    save_table(
        df=df_event,
        table_name="event",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def event_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
        -- where processed = False
    """)

    df_event = df_event.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_event.display()
    save_table(
        df=df_event,
        table_name="event",
        data_layer="curated",
        version=v
    )


test_df = 

raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

df = spark.read.table(raw_table)


def raw_to_curated_event(df: DataFrame)

    # df_event = spark.sql(f"""
    #         select distinct(data_event_id) as event_id,
    #             data_event_name as event_name,
    #             data_event_engineProgram as engine_program
    #         from {raw_table}
    #         -- where processed = False
    #     """)
    df_event = df.select(event_id, etc.)


    df_event = df_event.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    return df_event


# COMMAND ----------

@pytest.fixture
def event_df():
    dictionary = {"row1": ["v1", "v2", "v3"], "row2": ["v4", "v5", "v6"]}
    return spark.createDataframe(dictionary)


def test_event_raw_to_curated():
    assert event_raw_to_curated(event_df()) == expected_dataframe





# COMMAND ----------

def inspection_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_inspection = spark.sql(f"""
        select distinct(data_inspection_uuid) as insp_uuid,
            data_inspection_createdAt as insp_timestamp,
            data_inspection_name as insp_name,
            data_inspection_template_techData_id as insp_description,
            data_inspection_part_partNumber as insp_part_number,
            trim(data_inspection_part_serialNumber) as insp_serial_number,
            data_inspection_part_batchNumber as insp_bath_number,
            data_inspection_part_tac as insp_part_tac,
            data_inspection_part_eot as insp_part_eot,
            data_inspection_part_efh as insp_part_efh,
            data_inspection_part_cso as insp_part_cso,
            data_inspection_part_tso as insp_part_tso,
            data_inspection_part_engineModel as insp_engine_model,
            trim(data_inspection_part_engineSerialNumber) as insp_engine_serial_number,
            data_inspection_disposition_name as insp_disposition_name
        from {raw_table}
    """)

    df_inspection = df_inspection.withColumn(
        "insp_timestamp",
        F.to_timestamp("insp_timestamp", "M/d[d]/yyyy H[H]:mm")
    ) \
        .withColumn("load_timeStamp", F.current_timestamp()) \
        .withColumn("processed", F.lit(False))

    # df_inspection.display()
    save_table(
        df=df_inspection,
        table_name="inspection",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def observation_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_observation = spark.sql(f"""
        select distinct data_observation_uuid as obs_uuid,
            -- not in current data set
            -- data_observation_inspectionTable as obs_description, 
            data_observation_areaPath as obs_distress_name,
            data_observation_attributes_type as obs_type,
            data_observation_adHocArea as obs_adHocArea,
            data_observation_conditionName as obs_condition_name,
            data_observation_standardConditionName as obs_condition_standard_name,
            data_observation_disposition as obs_disposition_type,
            -- cot in current data set
            -- data_observation_conditionAdHoc as obse_conditionAdHoc,
            data_observation_noneObserved as obs_none_observed,
            -- data_observation_attributes_adHoc as obs_attribute_ad_hoc,
            data_observation_servicableLimit as obs_serviceable_limit_description,
            data_observation_repairableLimit as obs_repairable_limit_description
        from {raw_table}
    """)

    df_observation = df_observation.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_observation.display()
    save_table(
        df=df_observation,
        table_name="observation",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def attribute_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    # The following query returns duplicate records without the group by all columns clause
    df_attribute = spark.sql(f"""
        select  data_observation_uuid as observation_uuid,
            data_observation_attributes_name as attribute_name,
            -- data_observation_areaPath as area_path,
            -- regexp_extract(data_observation_attributes_value, r'^(\d*\.\d+|\d+)') as attribute_value,
            -- regexp_extract(data_observation_attributes_value, r'^(?!(\d*\.\d+|\d+))') as attribute_comment,
            case
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then data_observation_attributes_value
                else null
            end as value,
            case   
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then null
                else data_observation_attributes_value
            end as comment,
            data_observation_attributes_adHoc as attribute_ad_hoc
        from {raw_table}
        group by observation_uuid,
            attribute_name,
            value,
            comment,
            attribute_ad_hoc
    """)

    df_attribute = df_attribute.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_attribute.display()
    save_table(
        df=df_attribute,
        table_name="attribute",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def limit_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"


    def extract_min_max(limit: str):
        _max = "null"
        _min = "null"
        regex = u"([0-9]+\.[0-9]+)\s+inch"
        try:
            limits = re.findall(regex, limit)
        except TypeError:
            return
        
        limit = limit.lower()
        if "maximum" not in limit and "no longer than" not in limit:
            regex = u"([0-9]+\.[0-9]+)(?:\s*-\s*([0-9]+\.[0-9]+))?\s+inch"
            limits = re.findall(regex, limit)
            try:
                _min = limits[0][0]
                _max = limits[0][1]
            except IndexError:
                pass

        else:
            _max = limits[0]
            if "minimum" in limit:
                _min = limits[1]
        return f"{_min}, {_max}"


    min_max_udf = F.udf(lambda x: extract_min_max(x), StringType())

    df_limits = spark.sql("""
        with latest_inspection as (
            select
                data_observation_areaPath,
                data_observation_conditionName,
                max(data_inspection_createdAt::timestamp) as latest_insp
            from pwi_raw.initial_10_17
            group by 1, 2
        ),

        full_table_latest_inspection as (
            -- join cte back to raw table in order to find the servicable limit with an inspection date that matches the latest
            select
                raw.data_observation_areaPath as lim_distress_name,
                raw.data_observation_conditionName as lim_condition_name,
                raw.data_inspection_createdAt::timestamp as lim_date_changed,
                raw.data_observation_servicableLimit as lim_comment
            from pwi_raw.initial_10_17 as raw
            left join latest_inspection
                on raw.data_observation_areaPath = latest_inspection.data_observation_areaPath
                and raw.data_observation_conditionName = latest_inspection.data_observation_conditionName
            where raw.data_inspection_createdAt::timestamp = latest_inspection.latest_insp
            group by
                lim_distress_name,
                lim_condition_name,
                lim_date_changed,
                lim_comment
        ),

        limits_w_count as (
            -- add a count of limits (i.e. count of different lim_comment values) per lim_distress_name/lim_condition_name/lim_date_changed combination
            select lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment,
            count(*) over (partition by lim_distress_name, lim_condition_name, lim_date_changed order by lim_distress_name) as limit_count
            from full_table_latest_inspection
        )

        -- for conflicting servicable limits (identified by having limit_count > 1), exclude those with value of 'not permitted'; this resolves most limit conflicts
        select
            lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment
        from limits_w_count
        where limit_count = 1 or (
            limit_count > 1 and lower(lim_comment) != 'not permitted'
        )
        order by limit_count desc, lim_distress_name, lim_condition_name
    """)

    df_limits = df_limits.withColumn("min_max", min_max_udf(F.col("lim_comment")))
    split_col = F.split(df_limits["min_max"], ", ")
    df_limits = df_limits.withColumn(
            "lim_minimum",
            F.when(split_col.getItem(0) == "null", None) \
                .otherwise(split_col.getItem(0)) \
                    .cast(FloatType())
    ) \
        .withColumn(
            "lim_maximum",
        F.when(split_col.getItem(1) == "null", None) \
            .otherwise(split_col.getItem(1)) \
                    .cast(FloatType())
        )

    df_limits = df_limits.select(
        "lim_distress_name",
        "lim_condition_name",
        "lim_minimum",
        "lim_maximum",
        "lim_date_changed",
        "lim_comment"
    )

    # df_limits still contains a few conflicting limits, so we select the lowest lim_minimum value per lim_distress_name/lim_condition_name/lim_date_changed combination and choose the record with the matching lim_minimum
    window_spec = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_distress_name")

    df_limits = df_limits.withColumn("min_minimum", F.min("lim_minimum").over(window_spec))

    df_filtered = df_limits.filter((F.col("lim_minimum").isNull()) | (F.col("lim_minimum") == F.col("min_minimum")))

    # in at least one case, two conflicting servicable limits have the same lim_min and lim_max values; in such cases, we simply pick the first

    window_spec_1 = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_date_changed")

    df_filtered = df_filtered.withColumn("row_num", F.row_number().over(window_spec_1))
    df_filtered = df_filtered.filter(F.col("row_num") == 1).drop("row_num")

    save_table(
        df=df_filtered,
        table_name="limit",
        data_layer="curated",
        version=v
    )
    

# COMMAND ----------

version = ""
raw_version = "10_17"

# event_raw_to_curated(version, raw_version)
inspection_raw_to_curated(version, raw_version)
# observation_raw_to_curated(version, raw_version)
# attribute_raw_to_curated(version, raw_version)
# limit_raw_to_curated(version, raw_version)



# COMMAND ----------

drop_table_delete_data("pwi_curated.inspection")

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- query to show Chris on 10/30
# MAGIC
# MAGIC select *
# MAGIC from pwi_curated.limit_10_25
# MAGIC where lim_comment != 'Not permitted'
# MAGIC qualify count(*) over (partition by lim_distress_name, lim_condition_name, lim_date_changed order by lim_distress_name) > 1

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC select lim_condition_name
# MAGIC from pwi_curated.limit
# MAGIC group by 1

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC select obs_distress_name, count(*)
# MAGIC from pwi_curated.observation
# MAGIC group by 1
# MAGIC order by 1

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC select obs_distress_name, count(*)
# MAGIC from pwi_curated.observation
# MAGIC where obs_distress_name
# MAGIC ilike '%ID EDGES%'
# MAGIC group by 1

