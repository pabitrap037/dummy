======================================================================
# Databricks notebook sourcefrom pyspark.sql import DataFrame, functions as F
from pyspark.sql.types import StringType, FloatType
from pyspark.sql.window import Window
import re

# COMMAND ----------

# MAGIC %run ./common_functions

# COMMAND ----------

def event_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
        -- where processed = False
    """)

    df_event = df_event.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_event.display()
    save_table(
        df=df_event,
        table_name="event",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def event_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
        -- where processed = False
    """)

    df_event = df_event.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_event.display()
    save_table(
        df=df_event,
        table_name="event",
        data_layer="curated",
        version=v
    )


test_df = 

raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

df = spark.read.table(raw_table)


def raw_to_curated_event(df: DataFrame)

    # df_event = spark.sql(f"""
    #         select distinct(data_event_id) as event_id,
    #             data_event_name as event_name,
    #             data_event_engineProgram as engine_program
    #         from {raw_table}
    #         -- where processed = False
    #     """)
    df_event = df.select(event_id, etc.)


    df_event = df_event.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    return df_event


# COMMAND ----------

@pytest.fixture
def event_df():
    dictionary = {"row1": ["v1", "v2", "v3"], "row2": ["v4", "v5", "v6"]}
    return spark.createDataframe(dictionary)


def test_event_raw_to_curated():
    assert event_raw_to_curated(event_df()) == expected_dataframe





# COMMAND ----------

def inspection_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_inspection = spark.sql(f"""
        select distinct(data_inspection_uuid) as insp_uuid,
            data_inspection_createdAt as insp_timestamp,
            data_inspection_name as insp_name,
            data_inspection_template_techData_id as insp_description,
            data_inspection_part_partNumber as insp_part_number,
            trim(data_inspection_part_serialNumber) as insp_serial_number,
            data_inspection_part_batchNumber as insp_bath_number,
            data_inspection_part_tac as insp_part_tac,
            data_inspection_part_eot as insp_part_eot,
            data_inspection_part_efh as insp_part_efh,
            data_inspection_part_cso as insp_part_cso,
            data_inspection_part_tso as insp_part_tso,
            data_inspection_part_engineModel as insp_engine_model,
            trim(data_inspection_part_engineSerialNumber) as insp_engine_serial_number,
            data_inspection_disposition_name as insp_disposition_name
        from {raw_table}
    """)

    df_inspection = df_inspection.withColumn(
        "insp_timestamp",
        F.to_timestamp("insp_timestamp", "M/d[d]/yyyy H[H]:mm")
    ) \
        .withColumn("load_timeStamp", F.current_timestamp()) \
        .withColumn("processed", F.lit(False))

    # df_inspection.display()
    save_table(
        df=df_inspection,
        table_name="inspection",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def observation_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    df_observation = spark.sql(f"""
        select distinct data_observation_uuid as obs_uuid,
            -- not in current data set
            -- data_observation_inspectionTable as obs_description, 
            data_observation_areaPath as obs_distress_name,
            data_observation_attributes_type as obs_type,
            data_observation_adHocArea as obs_adHocArea,
            data_observation_conditionName as obs_condition_name,
            data_observation_standardConditionName as obs_condition_standard_name,
            data_observation_disposition as obs_disposition_type,
            -- cot in current data set
            -- data_observation_conditionAdHoc as obse_conditionAdHoc,
            data_observation_noneObserved as obs_none_observed,
            -- data_observation_attributes_adHoc as obs_attribute_ad_hoc,
            data_observation_servicableLimit as obs_serviceable_limit_description,
            data_observation_repairableLimit as obs_repairable_limit_description
        from {raw_table}
    """)

    df_observation = df_observation.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_observation.display()
    save_table(
        df=df_observation,
        table_name="observation",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def attribute_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    # The following query returns duplicate records without the group by all columns clause
    df_attribute = spark.sql(f"""
        select  data_observation_uuid as observation_uuid,
            data_observation_attributes_name as attribute_name,
            -- data_observation_areaPath as area_path,
            -- regexp_extract(data_observation_attributes_value, r'^(\d*\.\d+|\d+)') as attribute_value,
            -- regexp_extract(data_observation_attributes_value, r'^(?!(\d*\.\d+|\d+))') as attribute_comment,
            case
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then data_observation_attributes_value
                else null
            end as value,
            case   
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then null
                else data_observation_attributes_value
            end as comment,
            data_observation_attributes_adHoc as attribute_ad_hoc
        from {raw_table}
        group by observation_uuid,
            attribute_name,
            value,
            comment,
            attribute_ad_hoc
    """)

    df_attribute = df_attribute.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))

    # df_attribute.display()
    save_table(
        df=df_attribute,
        table_name="attribute",
        data_layer="curated",
        version=v
    )

# COMMAND ----------

def limit_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"


    def extract_min_max(limit: str):
        _max = "null"
        _min = "null"
        regex = u"([0-9]+\.[0-9]+)\s+inch"
        try:
            limits = re.findall(regex, limit)
        except TypeError:
            return
        
        limit = limit.lower()
        if "maximum" not in limit and "no longer than" not in limit:
            regex = u"([0-9]+\.[0-9]+)(?:\s*-\s*([0-9]+\.[0-9]+))?\s+inch"
            limits = re.findall(regex, limit)
            try:
                _min = limits[0][0]
                _max = limits[0][1]
            except IndexError:
                pass

        else:
            _max = limits[0]
            if "minimum" in limit:
                _min = limits[1]
        return f"{_min}, {_max}"


    min_max_udf = F.udf(lambda x: extract_min_max(x), StringType())

    df_limits = spark.sql("""
        with latest_inspection as (
            select
                data_observation_areaPath,
                data_observation_conditionName,
                max(data_inspection_createdAt::timestamp) as latest_insp
            from pwi_raw.initial_10_17
            group by 1, 2
        ),

        full_table_latest_inspection as (
            -- join cte back to raw table in order to find the servicable limit with an inspection date that matches the latest
            select
                raw.data_observation_areaPath as lim_distress_name,
                raw.data_observation_conditionName as lim_condition_name,
                raw.data_inspection_createdAt::timestamp as lim_date_changed,
                raw.data_observation_servicableLimit as lim_comment
            from pwi_raw.initial_10_17 as raw
            left join latest_inspection
                on raw.data_observation_areaPath = latest_inspection.data_observation_areaPath
                and raw.data_observation_conditionName = latest_inspection.data_observation_conditionName
            where raw.data_inspection_createdAt::timestamp = latest_inspection.latest_insp
            group by
                lim_distress_name,
                lim_condition_name,
                lim_date_changed,
                lim_comment
        ),

        limits_w_count as (
            -- add a count of limits (i.e. count of different lim_comment values) per lim_distress_name/lim_condition_name/lim_date_changed combination
            select lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment,
            count(*) over (partition by lim_distress_name, lim_condition_name, lim_date_changed order by lim_distress_name) as limit_count
            from full_table_latest_inspection
        )

        -- for conflicting servicable limits (identified by having limit_count > 1), exclude those with value of 'not permitted'; this resolves most limit conflicts
        select
            lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment
        from limits_w_count
        where limit_count = 1 or (
            limit_count > 1 and lower(lim_comment) != 'not permitted'
        )
        order by limit_count desc, lim_distress_name, lim_condition_name
    """)

    df_limits = df_limits.withColumn("min_max", min_max_udf(F.col("lim_comment")))
    split_col = F.split(df_limits["min_max"], ", ")
    df_limits = df_limits.withColumn(
            "lim_minimum",
            F.when(split_col.getItem(0) == "null", None) \
                .otherwise(split_col.getItem(0)) \
                    .cast(FloatType())
    ) \
        .withColumn(
            "lim_maximum",
        F.when(split_col.getItem(1) == "null", None) \
            .otherwise(split_col.getItem(1)) \
                    .cast(FloatType())
        )

    df_limits = df_limits.select(
        "lim_distress_name",
        "lim_condition_name",
        "lim_minimum",
        "lim_maximum",
        "lim_date_changed",
        "lim_comment"
    )

    # df_limits still contains a few conflicting limits, so we select the lowest lim_minimum value per lim_distress_name/lim_condition_name/lim_date_changed combination and choose the record with the matching lim_minimum
    window_spec = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_distress_name")

    df_limits = df_limits.withColumn("min_minimum", F.min("lim_minimum").over(window_spec))

    df_filtered = df_limits.filter((F.col("lim_minimum").isNull()) | (F.col("lim_minimum") == F.col("min_minimum")))

    # in at least one case, two conflicting servicable limits have the same lim_min and lim_max values; in such cases, we simply pick the first

    window_spec_1 = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_date_changed")

    df_filtered = df_filtered.withColumn("row_num", F.row_number().over(window_spec_1))
    df_filtered = df_filtered.filter(F.col("row_num") == 1).drop("row_num")

    save_table(
        df=df_filtered,
        table_name="limit",
        data_layer="curated",
        version=v
    )
    

# COMMAND ----------

version = ""
raw_version = "10_17"

# event_raw_to_curated(version, raw_version)
inspection_raw_to_curated(version, raw_version)
# observation_raw_to_curated(version, raw_version)
# attribute_raw_to_curated(version, raw_version)
# limit_raw_to_curated(version, raw_version)



# COMMAND ----------

drop_table_delete_data("pwi_curated.inspection")

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- query to show Chris on 10/30
# MAGIC
# MAGIC select *
# MAGIC from pwi_curated.limit_10_25
# MAGIC where lim_comment != 'Not permitted'
# MAGIC qualify count(*) over (partition by lim_distress_name, lim_condition_name, lim_date_changed order by lim_distress_name) > 1

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC select lim_condition_name
# MAGIC from pwi_curated.limit
# MAGIC group by 1

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC select obs_distress_name, count(*)
# MAGIC from pwi_curated.observation
# MAGIC group by 1
# MAGIC order by 1

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC select obs_distress_name, count(*)
# MAGIC from pwi_curated.observation
# MAGIC where obs_distress_name
# MAGIC ilike '%ID EDGES%'
# MAGIC group by 1


=========================================
@pytest.fixture
def event_df():
    dictionary = {"row1": ["v1", "v2", "v3"], "row2": ["v4", "v5", "v6"]}
    return spark.createDataframe(dictionary)

def test_event_raw_to_curated():
    assert event_raw_to_curated(event_df()) == expected_dataframe

======================================
CombinedImages = 
CONCATENATEX(
    YourTable,
    "<img src='" & YourTable[ImageURL1] & "'/> <img src='" & YourTable[ImageURL2] & "'/>",
    " "
)
=============================================
import requests
from PIL import Image

# URL of the image you want to download
image_url = 'https://images.app.goo.gl/RtAhjjr7YPK7J8mW7'

# Send an HTTP GET request to download the image
response = requests.get(image_url)

if response.status_code == 200:
    # Convert the response content into an image
    image = Image.open(io.BytesIO(response.content))

    # Save the image to your Databricks workspace
    image.save('/dbfs/path_to_save_image.jpg')
=============================


from PIL import Image, ImageDraw

# Load the image
image_path = "/dbfs/path_to_your_image.jpg"
image = Image.open(image_path)

# Create a drawing context
draw = ImageDraw.Draw(image)

# Define the line coordinates (x1, y1, x2, y2)
line_coordinates = (50, 50, 200, 200)

# Draw the line on the image with a curve joint and fill set to black
draw.line(line_coordinates, fill='black', width=3, joint='curve')

# Save the modified image
output_image_path = "/dbfs/path_to_output_image.jpg"
image.save(output_image_path)

# Display the image (optional)
image.show()
==================================================

https://dummy-3lf.pages.dev/
==================================================================
New-SelfSignedCertificate -DnsName "localhost" -CertStoreLocation "cert:\LocalMachine\My" -Subject "localhost" -FriendlyName "localhost" -NotAfter (Get-Date).AddYears(10)
==============================================================

// Get the canvas element
const canvas = document.getElementById("renderCanvas");




// Create the Babylon.js engine
const engine = new BABYLON.Engine(canvas, true);




function parseURLParameters() {
    const params = new URLSearchParams(window.location.search);

    const pointsParam = params.get('points');
    const sizesParam = params.get('sizes');
    const messagesParam = params.get('messages');

    const points = pointsParam ? JSON.parse(pointsParam) : [];
    const sizes = sizesParam ? JSON.parse(sizesParam) : [];
    const messages = messagesParam ? JSON.parse(messagesParam) : [];

    return { points, sizes, messages };
}

function createWireBox(center, edgeSize, message, scene) {
    // Create a box
    const box = BABYLON.MeshBuilder.CreateBox("wireBox", { size: edgeSize }, scene);

    // Set the box position to the provided center
    box.position = center;

    // Create a bright green material
    const wireMaterial = new BABYLON.StandardMaterial("wireMaterial", scene);
    wireMaterial.diffuseColor = new BABYLON.Color3(0, 1, 0); // Bright green
    wireMaterial.emissiveColor = new BABYLON.Color3(0, 1, 0); // Bright green
    wireMaterial.wireframe = true; // Set wireframe mode

    // Apply the material to the box
    box.material = wireMaterial;

    box.actionManager = new BABYLON.ActionManager(scene);
    box.actionManager.registerAction(
        new BABYLON.ExecuteCodeAction(
            BABYLON.ActionManager.OnLeftPickTrigger,
            function () {
                // Update the description text
                description.text = `${message}\nCenter: (${center.x}, ${center.y}, ${center.z})`;

                // Show the panel
                panel.isVisible = true;

                if (scene.activeCamera instanceof BABYLON.ArcRotateCamera) {
                    scene.activeCamera.target = center;
                }
            }
        )
    );

    return box;
}

function showAxis(size, scene, startingPoint = new BABYLON.Vector3(0, 0, 0)) {
    const makeTextPlane = function (text, color, textSize) {
        const dynamicTexture = new BABYLON.DynamicTexture("DynamicTexture", 50, scene, true);
        dynamicTexture.hasAlpha = true;
        dynamicTexture.drawText(text, 5, 40, "bold 36px Arial", color, "transparent", true);

        const plane = new BABYLON.Mesh.CreatePlane("TextPlane", textSize, scene, true);
        plane.material = new BABYLON.StandardMaterial("TextPlaneMaterial", scene);
        plane.material.backFaceCulling = false;
        plane.material.specularColor = new BABYLON.Color3(0, 0, 0);
        plane.material.diffuseTexture = dynamicTexture;

        return plane;
    };

    const axisX = BABYLON.Mesh.CreateLines("axisX", [
        startingPoint,
        startingPoint.add(new BABYLON.Vector3(size, 0, 0)),
        startingPoint.add(new BABYLON.Vector3(size * 0.95, 0.05 * size, 0)),
        startingPoint.add(new BABYLON.Vector3(size, 0, 0)),
        startingPoint.add(new BABYLON.Vector3(size * 0.95, -0.05 * size, 0))
    ], scene);
    axisX.color = new BABYLON.Color3(1, 0, 0);
    const xChar = makeTextPlane("X", "red", size / 10);
    xChar.position = startingPoint.add(new BABYLON.Vector3(0.9 * size, -0.05 * size, 0));

    const axisY = BABYLON.Mesh.CreateLines("axisY", [
        startingPoint,
        startingPoint.add(new BABYLON.Vector3(0, size, 0)),
        startingPoint.add(new BABYLON.Vector3(-0.05 * size, size * 0.95, 0)),
        startingPoint.add(new BABYLON.Vector3(0, size, 0)),
        startingPoint.add(new BABYLON.Vector3(0.05 * size, size * 0.95, 0))
    ], scene);
    axisY.color = new BABYLON.Color3(0, 1, 0);
    const yChar = makeTextPlane("Y", "green", size / 10);
    yChar.position = startingPoint.add(new BABYLON.Vector3(0, 0.9 * size, -0.05 * size));

    const axisZ = BABYLON.Mesh.CreateLines("axisZ", [
        startingPoint,
        startingPoint.add(new BABYLON.Vector3(0, 0, size)),
        startingPoint.add(new BABYLON.Vector3(0, -0.05 * size, size * 0.95)),
        startingPoint.add(new BABYLON.Vector3(0, 0, size)),
        startingPoint.add(new BABYLON.Vector3(0, 0.05 * size, size * 0.95))
    ], scene);
    axisZ.color = new BABYLON.Color3(0, 0, 1);
    const zChar = makeTextPlane("Z", "blue", size / 10);
    zChar.position = startingPoint.add(new BABYLON.Vector3(0, 0.05 * size, 0.9 * size));

    return {
        xChar: xChar,
        yChar: yChar,
        zChar: zChar
    };
}

function computeCentroidOfVisibleMeshes(meshes) {
    let totalCentroid = new BABYLON.Vector3(0, 0, 0);
    let visibleCount = 0;

    meshes.forEach(mesh => {
        // Ensure the bounding info is up-to-date
        mesh.computeWorldMatrix(true);

        // Check if the mesh is visible
        if (mesh.isVisible) {
            const meshCentroid = mesh.getBoundingInfo().boundingBox.centerWorld;
            totalCentroid.addInPlace(meshCentroid);
            visibleCount++;
        }
    });

    // Average out the centroids
    if (visibleCount > 0) {
        totalCentroid.scaleInPlace(1 / visibleCount);
    }

    return totalCentroid;
}


function addMeshCentroidClickHandler(mesh, centroid) {
    mesh.actionManager = new BABYLON.ActionManager(scene);
    mesh.actionManager.registerAction(
        new BABYLON.ExecuteCodeAction(
            BABYLON.ActionManager.OnLeftPickTrigger,
            function () {
                if (scene.activeCamera instanceof BABYLON.ArcRotateCamera) {
                    scene.activeCamera.target = centroid;
                }
            }
        )
    );
}

// Create a basic scene
const createScene = function () {
    const scene = new BABYLON.Scene(engine);
    scene.clearColor = new BABYLON.Color4(0, 0, 0, 1);  // Set the background color to black
    const axisLabels = showAxis(2, scene);

    engine.runRenderLoop(function () {
        // Make the text always face the camera
        axisLabels.xChar.lookAt(camera.position);
        axisLabels.yChar.lookAt(camera.position);
        axisLabels.zChar.lookAt(camera.position);

        scene.render();
    });

    // Create a camera
    const camera = new BABYLON.ArcRotateCamera("ArcRotateCamera", -Math.PI / 2, Math.PI / 2, 10, new BABYLON.Vector3(0, 0, 0), scene);
    camera.panningSensibility = 0;
    camera.attachControl(canvas, true);


    // Create a light
    const light = new BABYLON.HemisphericLight("light1", new BABYLON.Vector3(1, 1, 0), scene);

    // Load a 3D model
    BABYLON.SceneLoader.ImportMesh("", `https://s3.us-west-2.amazonaws.com/3d-viz-poc-pw-ifodor/turbine/`, "turbine__turbofan_engine.glb", scene, function (meshes) {
        // This callback is executed when the model is loaded
        // You can manipulate the loaded meshes or perform other actions here
        meshes.forEach(mesh => {
            if (mesh.name.startsWith("Плоскость")) {
                mesh.isVisible = false;
            }

        });
        const centroid = computeCentroidOfVisibleMeshes(meshes);

        meshes.forEach(mesh => {
            if (mesh.isVisible) {
                addMeshCentroidClickHandler(mesh, centroid);
            }
        });
        console.log("Model loaded!");
    });

    return scene;
};


// Call the createScene function
const scene = createScene();

const advancedTexture = BABYLON.GUI.AdvancedDynamicTexture.CreateFullscreenUI("UI");

const panel = new BABYLON.GUI.StackPanel();
panel.width = "320px";
panel.horizontalAlignment = BABYLON.GUI.Control.HORIZONTAL_ALIGNMENT_RIGHT;
panel.verticalAlignment = BABYLON.GUI.Control.VERTICAL_ALIGNMENT_CENTER;
advancedTexture.addControl(panel);

const header = new BABYLON.GUI.TextBlock();
header.text = "Box Details";
header.height = "40px";
header.color = "white";
panel.addControl(header);

const description = new BABYLON.GUI.TextBlock();
description.height = "180px";
description.color = "white";
panel.addControl(description);

// Initially hide the panel
panel.isVisible = false;

const { points, sizes, messages } = parseURLParameters();

for (let i = 0; i < points.length; i++) {
    const center = new BABYLON.Vector3(...points[i]);
    const edgeSize = sizes[i];
    const message = messages[i];
    createWireBox(center, edgeSize, message, scene);
}

// Render the scene
engine.runRenderLoop(function () {
    scene.render();
});

// Resize the engine when the window is resized
window.addEventListener("resize", function () {
    engine.resize();
});
--------------------------------------------------------------------
https://a540-2406-7400-63-5b34-9dd5-1c3b-590c-2d8f.ngrok-free.app/

--------------------------------------------------------------
http://www.dmrovisual.com.s3-website.ap-south-1.amazonaws.com/
