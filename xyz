from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType
from pyspark.sql.functions import lit

# Assuming 'spark' is your SparkSession
spark = SparkSession.builder.master("local").appName("mock_data").getOrCreate()

class UnitTestFunctions:
    
    def mock_data_inspection_rawtocurated(self):
        # Input mock data
        data = [
            ('part123', 'serial456', 'Pass', 'StationA', 1, 'Accept', 'Facility1', 101, 'PPI', 1, '20230607-163421_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-07', '12:34:56'),
            ('part456', 'serial789', 'Fail', 'StationB', 2, 'Reject', 'Facility2', 102, 'PPI', 0, '20230608-123456_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-08', '14:45:30'),
            ('part789', 'serial123', 'Pass', 'StationC', 3, 'Accept', 'Facility3', 103, 'PPI', 1, '20230609-045612_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-09', '09:15:00')
        ]

        schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('inspection_date', StringType(), True),
            StructField('inspection_time', StringType(), True),
        ])

        mock_data_input = spark.createDataFrame(data, schema=schema)

        # Expected mock data
        expected_data = [
            ('part123', 'serial456', 'Pass', 'StationA', 1, 'Accept', 'Facility1', 101, 'PPI', 1, '20230607-163421_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-07', '12:34:56', '2023-12-12T00:00:00', False),
            ('part456', 'serial789', 'Fail', 'StationB', 2, 'Reject', 'Facility2', 102, 'PPI', 0, '20230608-123456_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-08', '14:45:30', '2023-12-12T00:00:00', False),
            ('part789', 'serial123', 'Pass', 'StationC', 3, 'Accept', 'Facility3', 103, 'PPI', 1, '20230609-045612_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-09', '09:15:00', '2023-12-12T00:00:00', False)
        ]

        expected_schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('inspection_date', StringType(), True),
            StructField('inspection_time', StringType(), True),
            StructField('load_timestamp', StringType(), True),
            StructField('processed', BooleanType(), True),
        ])

        expected_data_df = spark.createDataFrame(expected_data, schema=expected_schema)

        return mock_data_input, expected_data_df

# Example of using the mock data function
unit_test_instance = UnitTestFunctions()
input_data, expected_output = unit_test_instance.mock_data_inspection_rawtocurated()
input_data.show()
expected_output.show()

###################################################################################
import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col,udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType,FloatType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp

warnings.filterwarnings("ignore")



class UnitTestFunctions(unittest.TestCase):

    def mock_data_uuid(self):
        #input mock data
        dictionary_dt = {'uuid': ['abe85dce-97d3-4565-94f1-692519668b5b',
                               '902dc46c-bf1d-4a05-a396-5ba451d5a3cb',
                               '865e8e5f-a23c-4d85-b03c-6a5456ae7891'],
                      'stream_id': [None,None,'20230607-163421_375fabcd-afd7-4737-be57-3e67218a823f'],
                      'processed':['False','False','True']
                      }
        mock_data_input = spark.createDataFrame([dictionary_dt])
        mock_data_input.createOrReplaceTempView("dt")
        dictionary_df = {'uuid': ['abe85dce-97d3-4565-94f1-692519668b5b',
                               '902dc46c-bf1d-4a05-a396-5ba451d5a3cb',
                               '865e8e5f-a23c-4d85-b03c-6a5456ae7891'],
                      'stream_id': [None,None,'20230607-163421_375fabcd-afd7-4737-be57-3e67218a823f'],
                      'processed':['False','False','True']
                      }
        mock_data_expected=spark.createDataFrame([dictionary_df])
        mock_data_expected.createReplaceTempView("df")

        #expected mock data 
        dictionary_expected = {'uuid': ['abe85dce-97d3-4565-94f1-692519668b5b',
                               '902dc46c-bf1d-4a05-a396-5ba451d5a3cb'],
                      'stream_id': [None,None],
                      'from_table':['detection','defect'],
                      'processed':['False','False'],
                      'reason':['the following 1-1 UUID does not match in defect & detections',
                                'the following 1-1 UUID does not match in defect & detections'],

                      'processed':['False','False']
                      }
        mock_data_expected=spark.createDataFrame([dictionary_expected])
        return mock_data_expected
    
    

    ###inspection
    def inspection_rawtocurated():
            
        #read tag table and get latest timestamp data
        # df_tag = spark.sql("select * from dmro_raw.tag where load_timestamp = (select max(load_timestamp) from dmro_raw.tag)")
        df_tag = spark.sql(f"select * from dmro_raw.tag where processed = False")

        if df_tag.count != 0:
            exception_df=df_tag.filter((df_tag['operation_code'].isNull() == True) | (df_tag['create_time'].isNull() == True))
            if exception_df.count()>0:
                reason_df = exception_df.withColumn("reason", lit("operation code or create time is null"))


            #create new columns to split up time
            df_tag = df_tag.withColumn('inspection_date', F.date_format(F.col('create_time'), 'yyyy-MM-dd'))\
                    .withColumn('inspection_time', F.date_format(F.col('create_time'), 'HH:mm:ss'))

            print("print df with columns split")
            df_tag.display()

            #select the necessary fields
            df_tag = df_tag.select('part_number', 'serial_number', 'automated_disposition', 'create_station', 'inspection_index', 'inspector_disposition', 'inspection_facility', 'operation_code', 'ppi_type', 'shop_visit', 'stream_id', 'inspection_date', 'inspection_time')

            #input new current timestamp 
            df_tag = df_tag.withColumn("load_timestamp", F.current_timestamp()) \
                        .withColumn("processed", lit(False)) \
                        .withColumn("inspection_index",df_tag.inspection_index.cast(IntegerType())) \
                        .withColumn("operation_code",df_tag.operation_code.cast(IntegerType())) \
                        .withColumn("inspection_date",df_tag.inspection_date.cast(DateType())) \
                        .withColumn("shop_visit",df_tag.shop_visit.cast(IntegerType()))
                        # .withColumn("inspection_time",df_tag.inspection_time.cast('timestamp'))

            return df_tag

#######################################################
import pandas as pd
json_data = [
    {"inspection_uuid": "abc123", "obs_uuid": "123"},
    {"inspection_uuid": None, "obs_uuid": "456"},
    {"inspection_uuid": "ghi789", "obs_uuid": None},
    {"inspection_uuid": None, "obs_uuid": "2111"},
    {"inspection_uuid": "ert23", "obs_uuid": None},
    {"inspection_uuid": "gssaa", "obs_uuid": None},
    {"inspection_uuid": None, "obs_uuid": None},
    {"inspection_uuid": "adsd89", "obs_uuid": None}
]

df = spark.createDataFrame(json_data)
#display(df)

def preprocess_validation_pwi(df_validations):
    exclude_files_dict = {"reason": [], "value": []}

    columns_to_check_and_exclude = ["inspection_uuid", "obs_uuid", "inspection_partNumber", "inspection_partserialNumber", "inspection_engineSerialNumber", "observation_areaPath"]

    for column in columns_to_check_and_exclude:
        #print(column)
        null_rows = df_validations.filter(col(column).isNull())
        for row in null_rows.rdd.collect():
            exclude_files_dict["reason"].append(f"{column} value is Null")
            exclude_files_dict["value"].append(row.asDict())

    df_without_nulls = df_validations.filter((col("inspection_uuid").isNotNull()) & (col("obs_uuid").isNotNull()) 
                                             & (col("inspection_partNumber").isNotNull()) & (col("inspection_partserialNumber").isNotNull())
                                             & (col("inspection_engineSerialNumber").isNotNull()) & (col("observation_areaPath").isNotNull()))
    
    columns_to_check_and_include = ["observation_servicableLimit", "attribite_name"]

    for column in columns_to_check_and_include:
        null_rows = df_validations.filter(col(column).isNull())
        for row in null_rows.rdd.collect():
            exclude_files_dict["reason"].append(f"{column} value is Null")
            exclude_files_dict["value"].append(row.asDict())
    print(df_without_nulls)

    return exclude_files_dict#,df_without_nulls

exclude_files_dict, df_without_nulls = preprocess_validation_pwi(df)
#display(spark.createDataFrame(exclude_files_dict))
#print("Exclude Files Dictionary:")
#print(exclude_files_dict)
display(pd.DataFrame(exclude_files_dict))
#after this we can load the dataframe having without null value
display(df_without_nulls)
################################################################################
from pyspark.sql import functions as F
from pprint import pprint
import pandas as pd
from pyspark.sql.functions import monotonically_increasing_id

def preprocess_validations_pwi_api_to_raw(df_validations_api_to_raw):
    exclude_files_dict_api_to_raw = {'reason': [], 'value': []}
    exclude_index_list_api_to_raw = []


    for row in df_validations_api_to_raw.collect():
        if row['inspection.uuid'] is None:
            exclude_files_dict_api_to_raw['reason'].append('inspection.uuid value is null')
            exclude_files_dict_api_to_raw['value'].append(row.asDict())
            exclude_index_list_api_to_raw.append(row['index'])

        if row['obs.uuid'] is None:
            exclude_files_dict_api_to_raw['reason'].append('obs.uuid value is null')
            exclude_files_dict_api_to_raw['value'].append(row.asDict())
            exclude_index_list_api_to_raw.append(row['index'])
        
        if row['inspection.partNumber'] is None:
            exclude_files_dict_api_to_raw['reason'].append('inspection.partNumber value is null')
            exclude_files_dict_api_to_raw['value'].append(row.asDict())
            exclude_index_list_api_to_raw.append(row['index'])
        
        if row['inspection.partSerialNumber'] is None:
            exclude_files_dict_api_to_raw['reason'].append('inspection.partSerialNumber is null')
            exclude_files_dict_api_to_raw['value'].append(row.asDict())
            exclude_index_list_api_to_raw.append(row['index'])
        
        if row['inspection.engineSerialNumber'] is None:
            exclude_files_dict_api_to_raw['reason'].append('inspection.engineSerialNumber is null')
            exclude_files_dict_api_to_raw['value'].append(row.asDict())
            exclude_index_list_api_to_raw.append(row['index'])
        
        if row['obs.servicableLimit'] is None:
            exclude_files_dict_api_to_raw['reason'].append('obs.servicableLimit value is null')
            exclude_files_dict_api_to_raw['value'].append(row.asDict())
            
        if row['attribute.name'] is None:
            exclude_files_dict_api_to_raw['reason'].append('attribute.name value is null')
            exclude_files_dict_api_to_raw['value'].append(row.asDict())

    df_validations_api_to_raw = df_validations_api_to_raw.filter(~df_validations_api_to_raw['index'].isin(exclude_index_list_api_to_raw))

    return exclude_files_dict_api_to_raw, df_validations_api_to_raw

def preprocess_validations_pwi_raw_to_curated(df_validations_raw_to_curated):
    exclude_files_dict_raw_to_curated = {'reason': [], 'value': []}
    exclude_index_list_raw_to_curated = []

    for row in df_validations_raw_to_curated.collect():
        if row['defects.disposition'] is None:
            exclude_files_dict_raw_to_curated['reason'].append('defects.disposition value is null')
            exclude_files_dict_raw_to_curated['value'].append(row.asDict())
            exclude_index_list_raw_to_curated.append(row['index'])

        if row['defects.indication_type'] is None:
            exclude_files_dict_raw_to_curated['reason'].append('defects.indication_type value is null')
            exclude_files_dict_raw_to_curated['value'].append(row.asDict())
            exclude_index_list_raw_to_curated.append(row['index'])
        
        if row['operation_code'] is None:
            exclude_files_dict_raw_to_curated['reason'].append('operation_code value is null')
            exclude_files_dict_raw_to_curated['value'].append(row.asDict())
        
    df_validations_raw_to_curated = df_validations_raw_to_curated.filter(~df_validations_raw_to_curated['index'].isin(exclude_index_list_raw_to_curated))

    return exclude_index_list_raw_to_curated, df_validations_raw_to_curated



json_data = [
    {"inspection.uuid": "abc111", "obs.uuid": "123","inspection.partNumber":"333part","inspection.partSerialNumber":"2222",
     "inspection.engineSerialNumber":"333","obs.servicableLimit":"1235obs","attribute.name":None},
    {"inspection.uuid": "abc222", "obs.uuid": "123","inspection.partNumber":"1111","inspection.partSerialNumber":"2222",
     "inspection.engineSerialNumber":"333","obs.servicableLimit":None,"attribute.name":"123name"},
    {"inspection.uuid": "abc333", "obs.uuid": "123","inspection.partNumber":"1111","inspection.partSerialNumber":"2222",
     "inspection.engineSerialNumber":"333","obs.servicableLimit":"1235obs","attribute.name":"123name"},
    {"inspection.uuid": "xyz444", "obs.uuid": "1233","inspection.partNumber":"prt111","inspection.partSerialNumber":"2222",
     "inspection.engineSerialNumber":None,"obs.servicableLimit":"1235obs","attribute.name":"123name"},
    {"inspection.uuid": "abc555", "obs.uuid": "123","inspection.partNumber":"1111","inspection.partSerialNumber":"2222",
     "inspection.engineSerialNumber":"333","obs.servicableLimit":"1235obs","attribute.name":"123name"},
    {"inspection.uuid": "abc666", "obs.uuid": "123","inspection.partNumber":None,"inspection.partSerialNumber":"2222",
     "inspection.engineSerialNumber":None,"obs.servicableLimit":"1235obs","attribute.name":"123name"},
]
df = spark.createDataFrame(json_data)
df_set_index = df.withColumn('index',monotonically_increasing_id())
display(df_set_index)
exclude_files_dict, df_validations = preprocess_validations_pwi_api_to_raw(df_set_index)

print("Excluded data:")
exclude_files_dict = pd.DataFrame(exclude_files_dict)
exclude_files_dict_final = spark.createDataFrame(exclude_files_dict)
display(exclude_files_dict_final)
print("\nDataFrame after dropping rows:")
display(df_validations)

#################################################################################
import pandas as pd

# Mock data in JSON format
json_data = [
    {
        "inspection.uuid": "abc123",
        "obs.uuid": "123"
    },
    {
        "inspection.uuid": None,
        "obs.uuid": "456"
    },
    {
        "inspection.uuid": "ghi789",
        "obs.uuid": None
    }
]

# Convert JSON data to a DataFrame
df = pd.DataFrame(json_data)

def preprocess_validation_ppi(df_validations):
    excluded_files = []
    
    # Creating Dictionary for storing the data as a table
    exclude_files_dict = {'reason': [], 'value': []}

    for index, row in df_validations.iterrows():
        if pd.isnull(row['inspection.uuid']):
            excluded_files.append(row['inspection.uuid'])
            exclude_files_dict['reason'].append('Null inspection.uuid')
            exclude_files_dict['value'].append('inspection.uuid')

        if pd.isnull(row['obs.uuid']):
            excluded_files.append(row['obs.uuid'])
            exclude_files_dict['reason'].append('Null obs.uuid')
            exclude_files_dict['value'].append('obs.uuid')

    # Additional processing or actions can be performed here

    return excluded_files, exclude_files_dict

# Call the function for the entire DataFrame
excluded_files, exclude_files_dict = preprocess_validation_ppi(df)

# Print the results
print("Excluded Files:", excluded_files)
print("Exclude Files Dictionary:")
display(pd.DataFrame(exclude_files_dict))

#######################
import pandas as pd

# Mock data in JSON format
json_data = [
    {
        "inspection.uuid": "abc123",
        "obs.uuid": "123"
    },
    {
        "inspection.uuid": None,
        "obs.uuid": "456"
    },
    {
        "inspection.uuid": "ghi789",
        "obs.uuid": None
    }
]

# Convert JSON data to a DataFrame
df = pd.DataFrame(json_data)

def preprocess_validation_ppi(df_validations):
    excluded_files = []
    
    # Creating Dictionary for storing the data as a table
    exclude_files_dict = {'reason': [], 'value': []}

    for index, row in df_validations.iterrows():
        if pd.isnull(row['inspection.uuid']):
            excluded_files.append(row['inspection.uuid'])
            exclude_files_dict['reason'].append('Null inspection.uuid')
            exclude_files_dict['value'].append(row['value'])

        if pd.isnull(row['obs.uuid']):
            excluded_files.append(row['obs.uuid'])
            exclude_files_dict['reason'].append('Null obs.uuid')
            exclude_files_dict['value'].append(row['value'])

    # Additional processing or actions can be performed here

    return excluded_files, exclude_files_dict

# Call the function for each row
for index, row in df.iterrows():
    excluded_files, exclude_files_dict = preprocess_validation_ppi(pd.DataFrame([row]))

    # Print the results for each row
    print(f"Row {index + 1}:")
    print("Excluded Files:", excluded_files)
    print("Exclude Files Dictionary:")
    print(exclude_files_dict)
    print("\n")

####################################
import pandas as pd
import json

with open('data_file.json', 'r') as file:
    json_data = json.load(file)

# Convert JSON data to a DataFrame
df = pd.DataFrame(json_data)
preprocess_validation_ppi(df)


def preprocess_validation_ppi(df_validations:DataFrame):
    
    excluded_files = []
    #creating Dictionary for storing the data as table
    exclude_files_dict={'reason':[],
                            'value':[]}
   

    for index,row in df_validations.iterrows():
        if df_validations['inspection.uuid'] is 
######################################################
def function_attribute_raw_to_curated():
    # The following query returns duplicate records without the group by all columns clause
    df_attribute = spark.sql(f"""
        select  data_observation_uuid as observation_uuid,
            data_observation_attributes_name as attribute_name,
            case
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then data_observation_attributes_value
                else null
            end as value,
            case   
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then null
                else data_observation_attributes_value
            end as comment,
            data_observation_attributes_adHoc as attribute_ad_hoc
        from {raw_table_attribute}
        group by observation_uuid,
            attribute_name,
            value,
            comment,
            attribute_ad_hoc
    """)
    return df_attribute


def test_attribute_raw_to_curated():
    dictionary_attribute_input = {
        'data_observation_uuid':'3333_uuid',
        'data_observation_attributes_name':'dummy_attribute_name',
        'data_observation_attributes_value':'10.451 - 12.059 inches (345.534  - 423.454 mm)'
    }
    df_input = spark.createDataframe(dictionary_attribute_input)
    spark.createOrReplaceTempView('raw_table_attribute')
    dictionary_attribute_expected = {
        'data_observation_uuid':'3333_uuid',
        'data_observation_attributes_name':'dummy_attribute_name',
        'data_observation_attributes_value':'10.451'
    }
    expected_df = spark.createDataframe(dictionary_attribute_expected)
    output_df = function_attribute_raw_to_curated(df_input)
    assert output_df.toPandas().equals(expected_df.toPandas())

###########################################################################################
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col

###########################################

@pytest.fixture
def test_data():
    dictionary = {
        'data_event_id': '111111',
        'data_event_name': 'new_event',
        'data_event_engineProgram': 'engine_new',
        'data_inspection_uuid':'111-uuid',
        'data_inspection_createdAt':'11/10/2023',
        'data_inspection_name':'dummy_inspection_name'
    }
    test_data_df = spark.createDataFrame([dictionary])
    test_data_df.createOrReplaceTempView("raw_table")
    print('hello1')
    return test_data_df

###########################################
def function_event_raw_to_curated(raw_table):
    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
    """)
    print('hello2')
    return df_event

###########################################
def test_event_raw_to_curated(test_data):
    output_df = function_event_raw_to_curated("raw_table")

    expected_df = test_data.select(col('data_inspection_uuid').alias('event_id'),
                                   col('data_event_name').alias('event_name'),
                                   col('data_event_engineProgram').alias('engine_program')).distinct()

    # Assuming you want to compare the data, you can convert the DataFrames to Pandas and compare
    print('hello3')
    assert output_df.toPandas().equals(expected_df.toPandas())





=====================================================
@pytest.fixture
def test_data():
    dictionary = {'event_id':'111111',
    'event_name':'new_event',
    'engineProgram':'engine_new',
    'data_inspection_uuid':'111-uuid',
    'data_inspection_createdAt':'11/10/2023',
    'data_inspection_name':'dummy_inspection_name',
    'data_inspection_template_techData_id':'dummmy_description',
    'data_inspection_part_partNumber as insp_part_number':'1122334455',
    'data_inspection_part_serialNumber':'SN1234567',
    'data_inspection_part_batchNumber':'BN12345',
    'data_inspection_part_tac':'tac1234',
    'data_inspection_part_eot':'eot1234',
    'data_inspection_part_efh':'efh1234',
    'data_inspection_part_cso':'cso1234',
    'data_inspection_part_tso':'tso1234',
    'data_inspection_part_engineModel':'dummy_model',
    'data_inspection_part_engineSerialNumber':'ESN12345',
    'data_inspection_disposition_name':'dummy_disposition',
    'data_observation_uuid':'222-uuid',
    'data_observation_areaPath':'dummy_areaPath',
    'data_observation_attributes_type':'dummy',
    'data_observation_adHocArea':'',
    'data_observation_conditionName':'',
    'data_observation_standardConditionName':'',
    'data_observation_disposition':'',
    'data_observation_noneObserved':'',
    'data_observation_servicableLimit':'',
    'data_observation_repairableLimit':'',
    'data_observation_attributes_name':'',
    'data_observation_attributes_value':'',       
    'data_observation_attributes_adHoc':''

    }

    test_data_df = spark.createDataframe(dictionary)
    test_data_df.createOrReplaceTempView("raw_table")
    

###########################################
def generic_timestamp(df:df_event):
    df_timpestamp = df_event.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))
    return df_timestamp

##########################################
def function_event_raw_to_curated():
    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
    """)
    return df_event



def test_event_raw_to_curated(test_data):
    output_df= function_event_raw_to_curated()

    expected_df = test_data_df.select(col('data_event_id').alias('event_id'),col('data_event_name')/
    .alias('event_name'),col('data_event_engineProgram').alias('engine_program')).distinct()

    assert output_df == expected_df
###########################################################


def function_inspection_raw_to_curated(test_data):
    output_df = spark.sql(f"""
        select distinct(data_inspection_uuid) as insp_uuid,
            data_inspection_createdAt as insp_timestamp,
            data_inspection_name as insp_name,
            data_inspection_template_techData_id as insp_description,
            data_inspection_part_partNumber as insp_part_number,
            trim(data_inspection_part_serialNumber) as insp_serial_number,
            data_inspection_part_batchNumber as insp_batch_number,
            data_inspection_part_tac as insp_part_tac,
            data_inspection_part_eot as insp_part_eot,
            data_inspection_part_efh as insp_part_efh,
            data_inspection_part_cso as insp_part_cso,
            data_inspection_part_tso as insp_part_tso,
            data_inspection_part_engineModel as insp_engine_model,
            trim(data_inspection_part_engineSerialNumber) as insp_engine_serial_number,
            data_inspection_disposition_name as insp_disposition_name
        from {raw_table}
    """)

    return output_df



# def function_timestamp_inspection_raw_to_curated(test_data):
#     output_df = df_test_data.withColumn("insp_timestamp",F.to_timestamp("insp_timestamp", "M/d[d]/yyyy H[H]:mm")) \
#         .withColumn("load_timeStamp", F.current_timestamp()) \
#         .withColumn("processed", F.lit(False))

#     expected_df =spark.sql(f"""
#         select *,
#         TO_TIMESTAMP(insp_timestamp,'MM/DD/YYYY HH24:MI') as insp_timestamp,
#         CURRENT_TIMESTAMP() AS load_timstamp,False as processed
#         from raw_table
#     """) 
    
def test_inspection_raw_to_curated(test_data):
    output_df = function_one_of_inspection_raw_to_curated()

    expected_df = test_data_df.select(col('data_inspection_uuid').alias('insp_uuid')/
    ,col('data_inspection_name').alias('insp_name')/
    ,col('data_inspection_template_techData_id').alias('insp_description')/
    ,col('data_inspection_part_partNumber').alias('insp_part_number')/
    ,trim(col('data_inspection_part_serialNumber')).alias('insp_serial_number')/
    ,col('data_inspection_part_batchNumber').alias('insp_batch_number')
    ,col('data_inspection_part_tac').alias('insp_part_tac')/
    ,col('data_inspection_part_eot').alias('insp_part_eot')/
    ,col('data_inspection_part_cso').alias('insp_part_cso')/
    ,col('data_inspection_part_tso').alias('insp_part_tso')/
    ,col('data_inspection_part_engineModel').alias('insp_engine_model')/
    ,trim(col('data_inspection_part_engineSerialNumber')).alias('insp_engine_serial_number')/
    ,col('data_inspection_disposition_name').alias('insp_disposition_name').distinct()
    )
    assert output_df == expected_df
    #function_two_of_inspection_raw_to_curated(test_data)
    return 'Inspection Raw to Curated Function Passed'



############################################    
def function_observation_raw_to_curated(test_data):
    output_df = spark.sql(f"""
        select distinct data_observation_uuid as obs_uuid,
            data_observation_areaPath as obs_distress_name,
            data_observation_attributes_type as obs_type,
            data_observation_adHocArea as obs_adHocArea,
            data_observation_conditionName as obs_condition_name,
            data_observation_standardConditionName as obs_condition_standard_name,
            data_observation_disposition as obs_disposition_type,
            data_observation_noneObserved as obs_none_observed,
            data_observation_servicableLimit as obs_serviceable_limit_description,
            data_observation_repairableLimit as obs_repairable_limit_description
        from {raw_table}
    """)
    return output_df


def test_observation_raw_to_curated(test_data):
    output_df=function_observation_raw_to_curated(test_data)

    expected_df= test_data_df.select(col('data_observation_uuid').alias('obs_uuid')/
    ,col('data_observation_areaPath').alias('obs_distress_name')/
    ,col('data_observation_attributes_type').alias('obs_type')/
    ,col('data_observation_adHocArea').alias('obs_adHocArea')/
    ,col('data_observation_conditionName').alias('obs_condition_name')
    ,col('data_observation_standardConditionName').alias('obs_condition_standard_name')/
    ,col('data_observation_disposition').alias('obs_disposition_type')/
    ,col('data_observation_noneObserved').alias('obs_none_observed')/
    ,col('data_observation_servicableLimit').alias('obs_serviceable_limit_description')/
    ,col('data_inspection_part_engineModel').alias('insp_engine_model')/
    ,col('data_observation_repairableLimit').alias('obs_repairable_limit_description').distinct()
    )
    return 'Observation Raw to Curated Function Passed'
    
#######################################################
####for this we need to have a separate data for testing
def function_one_of_attribute_raw_to_curated(test_data):

    # The following query returns duplicate records without the group by all columns clause
    output_df = spark.sql(f"""
        select  data_observation_uuid as observation_uuid,
            data_observation_attributes_name as attribute_name,
            -- data_observation_areaPath as area_path,
            -- regexp_extract(data_observation_attributes_value, r'^(\d*\.\d+|\d+)') as attribute_value,
            -- regexp_extract(data_observation_attributes_value, r'^(?!(\d*\.\d+|\d+))') as attribute_comment,
            case
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then data_observation_attributes_value
                else null
            end as value,
            case   
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then null
                else data_observation_attributes_value
            end as comment,
            data_observation_attributes_adHoc as attribute_ad_hoc
        from {raw_table}
        group by observation_uuid,
            attribute_name,
            value,
            comment,
            attribute_ad_hoc
    """)

    save_table(
        df=output_df,
        table_name="raw_table",
    )
    assert output_df == expected_df

def function_two_of_attribute_raw_to_curated(test_data):
    output_df = df_attribute.withColumn("load_timeStamp", F.current_timestamp()) \
        .withColumn("processed", F.lit(False))
    expected_df =spark.sql(f"""
        select *,
        CURRENT_TIMESTAMP() AS load_timstamp,False as processed
        from raw_table
    """) 
    #saving table
    save_table(
        df=output_df,
        table_name="raw_table"
    )
    assert output_df == expected_df

def attribute_raw_to_curated(version, raw_table_version=""):
    function_one_of_attribute_raw_to_curated(test_data)
    function_two_of_attribute_raw_to_curated(test_data)
    return 'Attribute Raw to Curated Function Passed'




=======================================================
spark.sql(f"""
        select  data_observation_uuid as observation_uuid,
            data_observation_attributes_name as attribute_name,
            -- data_observation_areaPath as area_path,
            -- regexp_extract(data_observation_attributes_value, r'^(\d*\.\d+|\d+)') as attribute_value,
            -- regexp_extract(data_observation_attributes_value, r'^(?!(\d*\.\d+|\d+))') as attribute_comment,
            case
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then data_observation_attributes_value
                else null
            end as value,
            case   
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then null
                else data_observation_attributes_value
            end as comment,
            data_observation_attributes_adHoc as attribute_ad_hoc
        from {raw_table}
        group by observation_uuid,
            attribute_name,
            value,
            comment,
            attribute_ad_hoc
    """)
===========
df_attribute = df_attribute.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))
============
spark.sql(f"""
        select distinct data_observation_uuid as obs_uuid,
            -- not in current data set
            -- data_observation_inspectionTable as obs_description, 
            data_observation_areaPath as obs_distress_name,
            data_observation_attributes_type as obs_type,
            data_observation_adHocArea as obs_adHocArea,
            data_observation_conditionName as obs_condition_name,
            data_observation_standardConditionName as obs_condition_standard_name,
            data_observation_disposition as obs_disposition_type,
            -- cot in current data set
            -- data_observation_conditionAdHoc as obse_conditionAdHoc,
            data_observation_noneObserved as obs_none_observed,
            -- data_observation_attributes_adHoc as obs_attribute_ad_hoc,
            data_observation_servicableLimit as obs_serviceable_limit_description,
            data_observation_repairableLimit as obs_repairable_limit_description
        from {raw_table}
    """)
=======
df_observation = df_observation.withColumn("load_timeStamp", F.current_timestamp()) \
                        .withColumn("processed", F.lit(False))
================
spark.sql(f"""
        select distinct(data_inspection_uuid) as insp_uuid,
            data_inspection_createdAt as insp_timestamp,
            data_inspection_name as insp_name,
            data_inspection_template_techData_id as insp_description,
            data_inspection_part_partNumber as insp_part_number,
            trim(data_inspection_part_serialNumber) as insp_serial_number,
            data_inspection_part_batchNumber as insp_bath_number,
            data_inspection_part_tac as insp_part_tac,
            data_inspection_part_eot as insp_part_eot,
            data_inspection_part_efh as insp_part_efh,
            data_inspection_part_cso as insp_part_cso,
            data_inspection_part_tso as insp_part_tso,
            data_inspection_part_engineModel as insp_engine_model,
            trim(data_inspection_part_engineSerialNumber) as insp_engine_serial_number,
            data_inspection_disposition_name as insp_disposition_name
        from {raw_table}
    """)
=============
df_test_data.withColumn(
        "insp_timestamp",
        F.to_timestamp("insp_timestamp", "M/d[d]/yyyy H[H]:mm")) \
        .withColumn("load_timeStamp", F.current_timestamp()) \
        .withColumn("processed", F.lit(False))
======================================================================
@pytest.fixture
def test_data():
    mock_data_df = pd.read_csv('dmro_mock_final.csv')
    df_test_data = spark.sql('select * from mock_data_df') 
    return df_test_data

def function_one_event_raw_to_curated(test_data):
    output_df = spark.sql(f"""
        select distinct(part_number) as part_number,
             serial_number,
            automated_disposition
        from {raw_table}
        -- where processed = False
    """)
    expected_df = df_test_data.select(distinct('part_number'),'serial_number','automated_disposition').where(df_test_data.processed='False')
    assert output_df == expected_df

def function_two_of_event_raw_to_curated():
    assert "load_timestamp" in result_df.columns
    assert "processed" in result_df.columns

def event_raw_to_curated(test_data):
    df_test_data.createOrReplaceTempView("raw_table")
    #query one converted to one function
    function_one_event_raw_to_curated(df_test_data)
    function_two_event_raw_to_curated(df_test_data)


def inspection_raw_to_curated(test_data):

    df_inspection = spark.sql(f"""
        select distinct(data_inspection_uuid) as insp_uuid,
            data_inspection_createdAt as insp_timestamp,
            data_inspection_name as insp_name,
            data_inspection_template_techData_id as insp_description,
            data_inspection_part_partNumber as insp_part_number,
            trim(data_inspection_part_serialNumber) as insp_serial_number,
            data_inspection_part_batchNumber as insp_bath_number,
            data_inspection_part_tac as insp_part_tac,
            data_inspection_part_eot as insp_part_eot,
            data_inspection_part_efh as insp_part_efh,
            data_inspection_part_cso as insp_part_cso,
            data_inspection_part_tso as insp_part_tso,
            data_inspection_part_engineModel as insp_engine_model,
            trim(data_inspection_part_engineSerialNumber) as insp_engine_serial_number,
            data_inspection_disposition_name as insp_disposition_name
        from {raw_table}
    """)

    df_inspection = df_inspection.withColumn(
        "insp_timestamp",
        F.to_timestamp("insp_timestamp", "M/d[d]/yyyy H[H]:mm")
    ) \
        .withColumn("load_timeStamp", F.current_timestamp()) \
        .withColumn("processed", F.lit(False))

    # df_inspection.display()
    save_table(
        df=df_inspection,
        table_name="inspection",
        data_layer="curated",
        version=v
    )

========================================================
@pytest.fixture
def test_data():
    df_expected = spark.sql('select top 1 * from dmro_curated.inspection')
    return df

def test_event_raw_to_curated(version, raw_table_version="",test_data):
    result_df = event_raw_to_curated(test_data)
    assert result_df.isnull().any()
    assert "load_timestamp" in result_df.columns
    assert "processed" in result_df.columns
    pass

def test_event_inspection_raw_to_curated(test_data):
    result_df = event_raw_to_curated(test_data)
    assert result_df.isnull().any()
    assert "load_timestamp" in result_df.columns
    assert "processed" in result_df.columns
    pass

 def test_extract_min_max(test_data):
        result_df = extract_min_max(test_data)
        assert result_df.isnull().any()

============================================================================================
@pytest.fixture
def event_df():
    dictionary = {"row1": ["v1", "v2", "v3"], "row2": ["v4", "v5", "v6"]}
    return spark.createDataframe(dictionary)

def test_event_raw_to_curated():
    assert event_raw_to_curated(event_df()) == expected_dataframe

======================================
CombinedImages = 
CONCATENATEX(
    YourTable,
    "<img src='" & YourTable[ImageURL1] & "'/> <img src='" & YourTable[ImageURL2] & "'/>",
    " "
)
=============================================
import requests
from PIL import Image

# URL of the image you want to download
image_url = 'https://images.app.goo.gl/RtAhjjr7YPK7J8mW7'

# Send an HTTP GET request to download the image
response = requests.get(image_url)

if response.status_code == 200:
    # Convert the response content into an image
    image = Image.open(io.BytesIO(response.content))

    # Save the image to your Databricks workspace
    image.save('/dbfs/path_to_save_image.jpg')
=============================


from PIL import Image, ImageDraw

# Load the image
image_path = "/dbfs/path_to_your_image.jpg"
image = Image.open(image_path)

# Create a drawing context
draw = ImageDraw.Draw(image)

# Define the line coordinates (x1, y1, x2, y2)
line_coordinates = (50, 50, 200, 200)

# Draw the line on the image with a curve joint and fill set to black
draw.line(line_coordinates, fill='black', width=3, joint='curve')

# Save the modified image
output_image_path = "/dbfs/path_to_output_image.jpg"
image.save(output_image_path)

# Display the image (optional)
image.show()
==================================================

https://dummy-3lf.pages.dev/
==================================================================
New-SelfSignedCertificate -DnsName "localhost" -CertStoreLocation "cert:\LocalMachine\My" -Subject "localhost" -FriendlyName "localhost" -NotAfter (Get-Date).AddYears(10)
==============================================================

// Get the canvas element
const canvas = document.getElementById("renderCanvas");




// Create the Babylon.js engine
const engine = new BABYLON.Engine(canvas, true);




function parseURLParameters() {
    const params = new URLSearchParams(window.location.search);

    const pointsParam = params.get('points');
    const sizesParam = params.get('sizes');
    const messagesParam = params.get('messages');

    const points = pointsParam ? JSON.parse(pointsParam) : [];
    const sizes = sizesParam ? JSON.parse(sizesParam) : [];
    const messages = messagesParam ? JSON.parse(messagesParam) : [];

    return { points, sizes, messages };
}

function createWireBox(center, edgeSize, message, scene) {
    // Create a box
    const box = BABYLON.MeshBuilder.CreateBox("wireBox", { size: edgeSize }, scene);

    // Set the box position to the provided center
    box.position = center;

    // Create a bright green material
    const wireMaterial = new BABYLON.StandardMaterial("wireMaterial", scene);
    wireMaterial.diffuseColor = new BABYLON.Color3(0, 1, 0); // Bright green
    wireMaterial.emissiveColor = new BABYLON.Color3(0, 1, 0); // Bright green
    wireMaterial.wireframe = true; // Set wireframe mode

    // Apply the material to the box
    box.material = wireMaterial;

    box.actionManager = new BABYLON.ActionManager(scene);
    box.actionManager.registerAction(
        new BABYLON.ExecuteCodeAction(
            BABYLON.ActionManager.OnLeftPickTrigger,
            function () {
                // Update the description text
                description.text = `${message}\nCenter: (${center.x}, ${center.y}, ${center.z})`;

                // Show the panel
                panel.isVisible = true;

                if (scene.activeCamera instanceof BABYLON.ArcRotateCamera) {
                    scene.activeCamera.target = center;
                }
            }
        )
    );

    return box;
}

function showAxis(size, scene, startingPoint = new BABYLON.Vector3(0, 0, 0)) {
    const makeTextPlane = function (text, color, textSize) {
        const dynamicTexture = new BABYLON.DynamicTexture("DynamicTexture", 50, scene, true);
        dynamicTexture.hasAlpha = true;
        dynamicTexture.drawText(text, 5, 40, "bold 36px Arial", color, "transparent", true);

        const plane = new BABYLON.Mesh.CreatePlane("TextPlane", textSize, scene, true);
        plane.material = new BABYLON.StandardMaterial("TextPlaneMaterial", scene);
        plane.material.backFaceCulling = false;
        plane.material.specularColor = new BABYLON.Color3(0, 0, 0);
        plane.material.diffuseTexture = dynamicTexture;

        return plane;
    };

    const axisX = BABYLON.Mesh.CreateLines("axisX", [
        startingPoint,
        startingPoint.add(new BABYLON.Vector3(size, 0, 0)),
        startingPoint.add(new BABYLON.Vector3(size * 0.95, 0.05 * size, 0)),
        startingPoint.add(new BABYLON.Vector3(size, 0, 0)),
        startingPoint.add(new BABYLON.Vector3(size * 0.95, -0.05 * size, 0))
    ], scene);
    axisX.color = new BABYLON.Color3(1, 0, 0);
    const xChar = makeTextPlane("X", "red", size / 10);
    xChar.position = startingPoint.add(new BABYLON.Vector3(0.9 * size, -0.05 * size, 0));

    const axisY = BABYLON.Mesh.CreateLines("axisY", [
        startingPoint,
        startingPoint.add(new BABYLON.Vector3(0, size, 0)),
        startingPoint.add(new BABYLON.Vector3(-0.05 * size, size * 0.95, 0)),
        startingPoint.add(new BABYLON.Vector3(0, size, 0)),
        startingPoint.add(new BABYLON.Vector3(0.05 * size, size * 0.95, 0))
    ], scene);
    axisY.color = new BABYLON.Color3(0, 1, 0);
    const yChar = makeTextPlane("Y", "green", size / 10);
    yChar.position = startingPoint.add(new BABYLON.Vector3(0, 0.9 * size, -0.05 * size));

    const axisZ = BABYLON.Mesh.CreateLines("axisZ", [
        startingPoint,
        startingPoint.add(new BABYLON.Vector3(0, 0, size)),
        startingPoint.add(new BABYLON.Vector3(0, -0.05 * size, size * 0.95)),
        startingPoint.add(new BABYLON.Vector3(0, 0, size)),
        startingPoint.add(new BABYLON.Vector3(0, 0.05 * size, size * 0.95))
    ], scene);
    axisZ.color = new BABYLON.Color3(0, 0, 1);
    const zChar = makeTextPlane("Z", "blue", size / 10);
    zChar.position = startingPoint.add(new BABYLON.Vector3(0, 0.05 * size, 0.9 * size));

    return {
        xChar: xChar,
        yChar: yChar,
        zChar: zChar
    };
}

function computeCentroidOfVisibleMeshes(meshes) {
    let totalCentroid = new BABYLON.Vector3(0, 0, 0);
    let visibleCount = 0;

    meshes.forEach(mesh => {
        // Ensure the bounding info is up-to-date
        mesh.computeWorldMatrix(true);

        // Check if the mesh is visible
        if (mesh.isVisible) {
            const meshCentroid = mesh.getBoundingInfo().boundingBox.centerWorld;
            totalCentroid.addInPlace(meshCentroid);
            visibleCount++;
        }
    });

    // Average out the centroids
    if (visibleCount > 0) {
        totalCentroid.scaleInPlace(1 / visibleCount);
    }

    return totalCentroid;
}


function addMeshCentroidClickHandler(mesh, centroid) {
    mesh.actionManager = new BABYLON.ActionManager(scene);
    mesh.actionManager.registerAction(
        new BABYLON.ExecuteCodeAction(
            BABYLON.ActionManager.OnLeftPickTrigger,
            function () {
                if (scene.activeCamera instanceof BABYLON.ArcRotateCamera) {
                    scene.activeCamera.target = centroid;
                }
            }
        )
    );
}

// Create a basic scene
const createScene = function () {
    const scene = new BABYLON.Scene(engine);
    scene.clearColor = new BABYLON.Color4(0, 0, 0, 1);  // Set the background color to black
    const axisLabels = showAxis(2, scene);

    engine.runRenderLoop(function () {
        // Make the text always face the camera
        axisLabels.xChar.lookAt(camera.position);
        axisLabels.yChar.lookAt(camera.position);
        axisLabels.zChar.lookAt(camera.position);

        scene.render();
    });

    // Create a camera
    const camera = new BABYLON.ArcRotateCamera("ArcRotateCamera", -Math.PI / 2, Math.PI / 2, 10, new BABYLON.Vector3(0, 0, 0), scene);
    camera.panningSensibility = 0;
    camera.attachControl(canvas, true);


    // Create a light
    const light = new BABYLON.HemisphericLight("light1", new BABYLON.Vector3(1, 1, 0), scene);

    // Load a 3D model
    BABYLON.SceneLoader.ImportMesh("", `https://s3.us-west-2.amazonaws.com/3d-viz-poc-pw-ifodor/turbine/`, "turbine__turbofan_engine.glb", scene, function (meshes) {
        // This callback is executed when the model is loaded
        // You can manipulate the loaded meshes or perform other actions here
        meshes.forEach(mesh => {
            if (mesh.name.startsWith("Плоскость")) {
                mesh.isVisible = false;
            }

        });
        const centroid = computeCentroidOfVisibleMeshes(meshes);

        meshes.forEach(mesh => {
            if (mesh.isVisible) {
                addMeshCentroidClickHandler(mesh, centroid);
            }
        });
        console.log("Model loaded!");
    });

    return scene;
};


// Call the createScene function
const scene = createScene();

const advancedTexture = BABYLON.GUI.AdvancedDynamicTexture.CreateFullscreenUI("UI");

const panel = new BABYLON.GUI.StackPanel();
panel.width = "320px";
panel.horizontalAlignment = BABYLON.GUI.Control.HORIZONTAL_ALIGNMENT_RIGHT;
panel.verticalAlignment = BABYLON.GUI.Control.VERTICAL_ALIGNMENT_CENTER;
advancedTexture.addControl(panel);

const header = new BABYLON.GUI.TextBlock();
header.text = "Box Details";
header.height = "40px";
header.color = "white";
panel.addControl(header);

const description = new BABYLON.GUI.TextBlock();
description.height = "180px";
description.color = "white";
panel.addControl(description);

// Initially hide the panel
panel.isVisible = false;

const { points, sizes, messages } = parseURLParameters();

for (let i = 0; i < points.length; i++) {
    const center = new BABYLON.Vector3(...points[i]);
    const edgeSize = sizes[i];
    const message = messages[i];
    createWireBox(center, edgeSize, message, scene);
}

// Render the scene
engine.runRenderLoop(function () {
    scene.render();
});

// Resize the engine when the window is resized
window.addEventListener("resize", function () {
    engine.resize();
});
--------------------------------------------------------------------
https://a540-2406-7400-63-5b34-9dd5-1c3b-590c-2d8f.ngrok-free.app/

--------------------------------------------------------------
http://www.dmrovisual.com.s3-website.ap-south-1.amazonaws.com/
