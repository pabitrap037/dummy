spallation_type = ['Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Other', 'Other', 'Other', 'Other', 'Other', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Erosion', 'Erosion', 'Erosion', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Other', 'Other']

ppi_region_percent = [0.133, 0, 0.041, 0.007, 0.007, 0.5, 0.358, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.013, 0, 0.101, 0.02, 0.08, 0.091, 0, 0, 0, 0, 0, 0, 0, 0, 0]

airfoil_region_percent = [0.045, 0.045, 0.045, 0.046, 0.337, 0.337, 0.337, 0.052, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.046, 0.046, 0.046, 0.052, 0.052, 0.052, 0, 0, 0, 0, 0, 0, 0, 0]
ppi_region = ['x1', 'x2', 'x3', 'y1', 'x1', 'x2', 'x3', 'y1', 'x1', 'x2', 'x3', 'y1', 'x1', 'x2', 'x3', 'y1', 'x1', 'x2', 'x3', 'y1', 'y4', 'y2', 'y3', 'y4', 'y2', 'y3', 'y4', 'y2', 'y3', 'y4', 'y2', 'y3', 'y4', 'y2', 'y3']
'airfoil_region': ['Backing Radius', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side']
****************************************************************
print("Length of 'stream_id':", len(dict_exp['stream_id']))
print("Length of 'part_number':", len(dict_exp['part_number']))
print("Length of 'serial_number':", len(dict_exp['serial_number']))
print("Length of 'shop_visit':", len(dict_exp['shop_visit']))
print("Length of 'create_station':", len(dict_exp['create_station']))
print("Length of 'inspection_index':", len(dict_exp['inspection_index']))
print("Length of 'airfoil_region':", len(dict_exp['airfoil_region']))
print("Length of 'ppi_region':", len(dict_exp['ppi_region']))
print("Length of 'spallation_type':", len(dict_exp['spallation_type']))
print("Length of 'ppi_region_percent':", len(dict_exp['ppi_region_percent']))
print("Length of 'airfoil_region_percent':", len(dict_exp['airfoil_region_percent']))
print("Length of 'ESN':", len(dict_exp['ESN']))
print("Length of 'engine_flight_hours':", len(dict_exp['engine_flight_hours']))
print("Length of 'engine_cycles':", len(dict_exp['engine_cycles']))
print("Length of 'engine_operator':", len(dict_exp['engine_operator']))
print("Length of 'casting_vendor':", len(dict_exp['casting_vendor']))
print("Length of 'coating_vendor':", len(dict_exp['coating_vendor']))
print("Length of 'coating_gain':", len(dict_exp['coating_gain']))
print("Length of 'hole_drill_vendor':", len(dict_exp['hole_drill_vendor']))
print("Length of 'airflow_4800':", len(dict_exp['airflow_4800']))
print("Length of 'airflow_4802':", len(dict_exp['airflow_4802']))

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
dict_exp = {
            'stream_id': ['xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01'],
            'part_number': ['30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501'],
            'serial_number': ['aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122'],
            'shop_visit': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],
            'create_station': ['Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial'],
            'inspection_index': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],
            'airfoil_region': ['Flat Side', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Flat Side'],
            'ppi_region': ['y2', 'y1', 'x1', 'x3', 'y3', 'x2', 'y4', 'x1', 'y1', 'y3', 'y2', 'x2', 'y4', 'x3', 'x2', 'x3', 'x1', 'y3', 'y1', 'y4', 'y2', 'x1', 'y4', 'y2', 'x2', 'y1', 'y3', 'x3', 'y3', 'x2', 'y1', 'x1', 'x3', 'y4', 'y2'],
            'spallation_type': ['Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough'],
            'ppi_region_percent': [0.013, 0.007, 0.133, 0.041, 0.0, 0.0, 0.101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.091, 0.02, 0.5, 0.0, 0.08, 0.358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'airfoil_region_percent': [0.046, 0.046, 0.045, 0.045, 0.046, 0.045, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.337, 0.052, 0.052, 0.337, 0.052, 0.052, 0.337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'ESN': ['ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11'],
            'engine_flight_hours': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500],
            'engine_cycles': [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50],
            'engine_operator': ['Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1'],
            'casting_vendor': ['vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1'],
            'coating_vendor': ['cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2'],
            'coating_gain': [4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3],
            'hole_drill_vendor': ['hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1'],
            'airflow_4800': [78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5],
            'airflow_4802': [-98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5]
        }
*******************************************************
'airfoil_region': ['Flat Side', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius', 'Flat Side', 'Flat Side'],
    'ppi_region': ['y2', 'y1', 'x1', 'x3', 'y3', 'x2', 'y4', 'x1', 'y1', 'y3', 'y2', 'x2', 'y4', 'x3', 'x2', 'x3', 'x1', 'y3', 'y1', 'y4', 'y2', 'x1', 'y4', 'y2', 'x2', 'y1', 'y3', 'x3', 'y3', 'x2', 'y1', 'x1', 'x3', 'y4', 'y2'],
    'spallation_type': ['Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough'],
    'ppi_region_percent': [0.013, 0.007, 0.133, 0.041, 0.0, 0.0, 0.101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.091, 0.02, 0.5, 0.0, 0.08, 0.358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    'airfoil_region_percent': [0.046, 0.046, 0.045, 0.045, 0.046, 0.045, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.337, 0.052, 0.052, 0.337, 0.052, 0.052, 0.337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col,udf
from pyspark.sql.types import StructType, StructField, StringType,BooleanType,DoubleType,IntegerType,DateType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp
from datetime import datetime
warnings.filterwarnings("ignore")

class UnitTestFunctions(unittest.TestCase):
    test=True
    version=''

    def mock_data_spallation(self):
        #spallation table
        schema_spall=StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('ppi_region', StringType(), True),
            StructField('full_spallation', BooleanType(), True),
            StructField('full_spallation_percent', DoubleType(), True),
            StructField('partial_spallation', BooleanType(), True),
            StructField('partial_spallation_percent', DoubleType(), True),
            StructField('erosion', BooleanType(), True),
            StructField('erosion_percent', DoubleType(), True),
            StructField('crack', BooleanType(), True),
            StructField('burnthrough', BooleanType(), True),
            StructField('burnthrough_percent', DoubleType(), True),
            StructField('other', BooleanType(), True),
            StructField('other_percent', DoubleType(), True),
            StructField('stream_id', StringType(), True),
            StructField('processed',BooleanType(),True)
        ])
        dict_spall = {
            'part_number': ['30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501'],
            'serial_number': ['aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122'],
            'shop_visit': [10, 10, 10, 10, 10, 10, 10, 10],
            'create_station': ['Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial'],
            'inspection_index': [3, 3, 3, 3, 3, 3, 3, 3],
            'ppi_region': ['x1', 'x2', 'x3', 'y1', 'y2', 'y3', 'y4', 'x2'],
            'full_spallation': [True, False, True, True, True, False, True, True],
            'full_spallation_percent': [0.133, 0.0, 0.041, 0.007, 0.013, 0.0, 0.101, 0.2],
            'partial_spallation': [True, True, True, False, True, True, True, False],
            'partial_spallation_percent': [0.007, 0.5, 0.358, 0.0, 0.02, 0.08, 0.091, 0.0],
            'erosion': [False, False, False, False, False, False, False, False],
            'erosion_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'crack': [False, False, False, False, False, False, False, False],
            'burnthrough': [False, False, False, False, False, False, False, False],
            'burnthrough_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'other': [False, False, False, False, False, False, False, False],
            'other_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'stream_id': ['xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01'],
            'processed': [False, False, False, False, False, False, False, True]
        }
        rows_inp_spall = [
            {'part_number': pn ,'serial_number': sn, 'shop_visit':sv ,'create_station':cs,'inspection_index':ii,'ppi_region':ppi,'full_spallation':fs,'full_spallation_percent':fp,'partial_spallation':ps,'partial_spallation_percent':pp,'erosion':er,'erosion_percent':ep,'crack':ck,'burnthrough':bn,'burnthrough_percent':bp,'other':ot,'other_percent':op,'stream_id':si,'processed':pr}
            for pn,sn,sv,cs,ii,ppi,fs,fp,ps,pp,er,ep,ck,bn,bp,ot,op,si,pr in zip(
                dict_spall['part_number'],
                dict_spall['serial_number'],
                dict_spall['shop_visit'],
                dict_spall['create_station'],
                dict_spall['inspection_index'],
                dict_spall['ppi_region'],
                dict_spall['full_spallation'],
                dict_spall['full_spallation_percent'],
                dict_spall['partial_spallation'],
                dict_spall['partial_spallation_percent'],
                dict_spall['erosion'],
                dict_spall['erosion_percent'],
                dict_spall['crack'],
                dict_spall['burnthrough'],
                dict_spall['burnthrough_percent'],
                dict_spall['other'],
                dict_spall['other_percent'],
                dict_spall['stream_id'],
                dict_spall['processed']
            )
        ]
        spall_df=spark.createDataFrame(rows_inp_spall,schema=schema_spall)
        spall_df.createOrReplaceTempView('spallation')

        #ppi_airfoil_region table
        schema_ppi=StructType([
            StructField('part_number', StringType(), True),
            StructField('inspection_type', StringType(), True),
            StructField('airfoil_region', StringType(), True),
            StructField('ppi_region', StringType(), True),
            StructField('surface_area', DoubleType(), True),
            StructField('ppi_airfoil_ratio', DoubleType(), True)
        ])
        dict_ppi = {
            'part_number':['30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501'],
            'inspection_type': ['LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall'],
            'airfoil_region': ['Backing Radius','Backing Radius','Backing Radius','Flat Side','Flat Side','Flat Side','Flat Side','Forward Ledge'],
            'ppi_region': ['x1','x2','x3','y1','y2','y3','y4','z1'],
            'surface_area': [0.4,0.7,0.9,0.1,0.3,0.08,0.32,0.33],
            'ppi_airfoil_ratio':[0.2,0.35,0.45,0.125,0.375,0.1,0.4,0.1]
        }
        rows_inp_ppi = [
            {'part_number': pn ,'inspection_type': it, 'airfoil_region':ar ,'ppi_region':pr,'surface_area':sa,'ppi_airfoil_ratio':par}
            for pn,it,ar,pr,sa,par in zip(
                dict_ppi['part_number'],
                dict_ppi['inspection_type'],
                dict_ppi['airfoil_region'],
                dict_ppi['ppi_region'],
                dict_ppi['surface_area'],
                dict_ppi['ppi_airfoil_ratio']
            )
        ]
        ppi_ar_df=spark.createDataFrame(rows_inp_ppi,schema=schema_ppi)
        ppi_ar_df.createOrReplaceTempView('ppi_region')

        #casting table
        schema_cast=StructType([
            StructField('serial_number', StringType(), True),
            StructField('casting_vendor', StringType(), True)
        ])
        dict_cast = {
            'serial_number': ['aabbcc001122', 'aabbcc001133'],
            'casting_vendor': ['vendor1', 'vendor33']
        }
        rows_inp_cast = [
            {'serial_number': sn ,'casting_vendor': cv}
            for sn,cv in zip(
                dict_cast['serial_number'],
                dict_cast['casting_vendor']
            )
        ]
        cast_df=spark.createDataFrame(rows_inp_cast,schema=schema_cast)
        cast_df.createOrReplaceTempView('casting')
        
        #engine table
        schema_eng=StructType([
            StructField('serial_number', StringType(), True),
            StructField('ESN', StringType(), True),
            StructField('engine_flight_hours', IntegerType(), True),
            StructField('engine_cycles', IntegerType(), True),
            StructField('engine_operator', StringType(), True)
        ])
        dict_eng = {
            'serial_number': ['aabbcc001122', 'aabbcc001144'],
            'ESN': ['ESN11', 'ESN44'],
            'engine_flight_hours': [500,400],
            'engine_cycles': [50,40],
            'engine_operator': ['Operator1','Operator4']    
        }
        rows_inp_eng = [
            {'serial_number': sn ,'ESN': esn,'engine_flight_hours':efh,'engine_cycles':ec,'engine_operator':ep}
            for sn,esn,efh,ec,ep in zip(
                dict_eng['serial_number'],
                dict_eng['ESN'],
                dict_eng['engine_flight_hours'],
                dict_eng['engine_cycles'],
                dict_eng['engine_operator']
            )
        ]
        engine_df=spark.createDataFrame(rows_inp_eng,schema=schema_eng)
        engine_df.createOrReplaceTempView('engine')
        
        #coat table
        schema_coat=StructType([
            StructField('serial_number', StringType(), True),
            StructField('coating_vendor', StringType(), True),
            StructField('coating_gain', DoubleType(), True),
            StructField('coating_date',DateType(),True)
        ])
        dict_coat = {
            'serial_number': ['aabbcc001122', 'aabbcc001133'],
            'coating_vendor': ['cvendor2', 'cvendor3'],
            'coating_gain': [4.3,3.5],
            'coating_date':[datetime.strptime('2023-11-11','%Y-%m-%d'),datetime.strptime('2023-11-01','%Y-%m-%d')] 
        }
        rows_inp_coat = [
            {'serial_number': sn ,'coating_vendor': cv,'coating_gain':cg,'coating_date':cd}
            for sn,cv,cg,cd in zip(
                dict_coat['serial_number'],
                dict_coat['coating_vendor'],
                dict_coat['coating_gain'],
                dict_coat['coating_date']
            )
        ]
        coat_df=spark.createDataFrame(rows_inp_coat,schema=schema_coat)
        coat_df.createOrReplaceTempView('coat')
        
        #holedrill table
        schema_hole=StructType([
            StructField('serial_number', StringType(), True),
            StructField('hole_drill_vendor', StringType(), True)
        ])
        dict_hole = {
            'serial_number': ['aabbcc001122', 'aabbcc001132'],
            'hole_drill_vendor': ['hvendor1','hvendor2'] 
        }
        rows_inp_hole = [
            {'serial_number': sn ,'hole_drill_vendor': hv}
            for sn,hv in zip(
                dict_hole['serial_number'],
                dict_hole['hole_drill_vendor']
            )
        ]
        hole_df=spark.createDataFrame(rows_inp_hole,schema=schema_hole)
        hole_df.createOrReplaceTempView('hole')

        #airflow table
        schema_airflow=StructType([
            StructField('serial_number', StringType(), True),
            StructField('4800', DoubleType(), True),
            StructField('4802', DoubleType(), True)
        ])
        dict_airflow = {
            'serial_number': ['aabbcc001122', 'aabbcc001132'],
            '4800': [78.5,58.5],
            '4802': [-98.5,-28.5] 
        }
        rows_inp_airflow = [
            {'serial_number': sn ,'4800': n1,'4802':n2}
            for sn,n1,n2 in zip(
                dict_airflow['serial_number'],
                dict_airflow['4800'],
                dict_airflow['4802']
            )
        ]
        airflow_df=spark.createDataFrame(rows_inp_airflow,schema=schema_airflow)
        airflow_df.createOrReplaceTempView('airflow_group')

        #expected dtaframe
        schema_exp = StructType([
            StructField("system_def_id", StringType(), True),
            StructField("stream_id", StringType(), True),
            StructField("part_number", StringType(), True),
            StructField("serial_number", StringType(), True),
            StructField("shop_visit", IntegerType(), True),
            StructField("create_station", StringType(), True),
            StructField("inspection_index", IntegerType(), True),
            StructField("airfoil_region", StringType(), True),
            StructField("ppi_region", StringType(), True),
            StructField("spallation_type", StringType(), False),
            StructField("ppi_region_percent", DoubleType(), True),
            StructField("airfoil_region_percent", DoubleType(), True),
            StructField("ESN", StringType(), True),
            StructField("engine_flight_hours", IntegerType(), True),
            StructField("engine_cycles", IntegerType(), True),
            StructField("engine_operator", StringType(), True),
            StructField("casting_vendor", StringType(), True),
            StructField("coating_vendor", StringType(), True),
            StructField("coating_gain", DoubleType(), True),
            StructField("hole_drill_vendor", StringType(), True),
            StructField("airflow_4800", DoubleType(), True),
            StructField("airflow_4802", DoubleType(), True)
        ])
        dict_exp = {
            'stream_id': ['xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01'],
            'part_number': ['30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501'],
            'serial_number': ['aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122'],
            'shop_visit': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],
            'create_station': ['Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial'],
            'inspection_index': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],
            'airfoil_region': ['Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side','Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius','Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius','Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side','Backing Radius', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius','Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius','Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side'],
            'ppi_region': ['y1', 'y3', 'y4', 'x1', 'y4', 'y2', 'x2', 'y1', 'y3', 'x3', 'x1', 'y1', 'y3', 'y2', 'x2', 'y4', 'x3', 'y3', 'x2', 'x1', 'y1', 'y4', 'y2', 'x3', 'x1', 'x2', 'x2', 'x3', 'x1', 'y3', 'y1', 'y4', 'x1', 'x2', 'x3'],
            'spallation_type': ['Full Spallation', 'Full Spallation', 'Full Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other'],
            'ppi_region_percent': [0.007, 0.0, 0.101, 0.007, 0.091, 0.02, 0.5, 0.0, 0.08, 0.358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.133, 0, 0.041, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'airfoil_region_percent': [0.046, 0.046, 0.046, 0.337, 0.052, 0.052, 0.337, 0.052, 0.052, 0.337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045, 0.045, 0.045, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'ESN': ['ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11'],
            'engine_flight_hours': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500],
            'engine_cycles': [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50],
            'engine_operator': ['Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1'],
            'casting_vendor': ['vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1'],
            'coating_vendor': ['cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2'],
            'coating_gain': [4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3],
            'hole_drill_vendor': ['hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1'],
            'airflow_4800': [78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5],
            'airflow_4802': [-98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5]
        }
        rows_inp_exp = [
            {'stream_id': si,'part_number': pn, 'serial_number': sn, 'shop_visit': sv, 'create_station': cs, 'inspection_index': ii, 'airfoil_region':ar,'ppi_region': ppi,'spallation_type':st,'ppi_region_percent':prp,'airfoil_region_percent': arp, 'ESN': esn, 'engine_flight_hours': efh,'engine_cycles': ec, 'engine_operator': eo, 'casting_vendor': cv, 'coating_vendor': cov, 'coating_gain': cg, 'hole_drill_vendor': hv,'airflow_4800':ar1, 'airflow_4802': ar2}
            for si, pn, sn, sv, cs, ii, ar, ppi, st, prp, arp, esn, efh, ec, eo, cv, cov, cg, hv,ar1,ar2 in zip(
                dict_exp['stream_id'],
                dict_exp['part_number'],
                dict_exp['serial_number'],
                dict_exp['shop_visit'],
                dict_exp['create_station'],
                dict_exp['inspection_index'],
                dict_exp['airfoil_region'],
                dict_exp['ppi_region'],
                dict_exp['spallation_type'],
                dict_exp['ppi_region_percent'],
                dict_exp['airfoil_region_percent'],
                dict_exp['ESN'],
                dict_exp['engine_flight_hours'],
                dict_exp['engine_cycles'],
                dict_exp['engine_operator'],
                dict_exp['casting_vendor'],
                dict_exp['coating_vendor'],
                dict_exp['coating_gain'],
                dict_exp['hole_drill_vendor'],
                dict_exp['airflow_4800'],
                dict_exp['airflow_4802']
            )
        ]
        expected_df=spark.createDataFrame(rows_inp_exp,schema=schema_exp)
        return expected_df

    def ppi_spall_df(self):
        sp_df = spark.sql(f"""
            select 
                sp.part_number,
                sp.serial_number,
                sp.shop_visit,
                sp.create_station,
                sp.inspection_index,
                sp.ppi_region,
                ppi.airfoil_region,
                sp.full_spallation,
                sp.full_spallation_percent,
                sp.partial_spallation,
                sp.partial_spallation_percent,
                sp.erosion,
                sp.erosion_percent,
                sp.crack,
                sp.burnthrough,
                sp.burnthrough_percent,
                sp.other,
                sp.other_percent,
                eng.ESN,
                eng.engine_flight_hours,
                eng.engine_cycles,
                eng.engine_operator,
                cast.casting_vendor,
                ct.coating_vendor,
                ct.coating_gain,
                ct.coating_date,
                hol.hole_drill_vendor,
                ar.`4800` as airflow_4800,
                ar.`4802` as airflow_4802,
                current_timestamp() as load_timestamp,
                sp.stream_id
            from spallation as sp
            left join casting as cast on sp.serial_number = cast.serial_number
            left join engine as eng on eng.serial_number = cast.serial_number
            left join coat as ct on ct.serial_number = eng.serial_number
            left join hole as hol on hol.serial_number = ct.serial_number
            left join airflow_group as ar on ct.serial_number = ar.serial_number
            left join ppi_region as ppi
            on sp.ppi_region = ppi.PPI_region
            and sp.part_number = ppi.part_number
            where sp.processed = False
            and ppi.inspection_type = 'LPPI_Spall'
        """)
        # print('distinct')
        # sp_df = sp_df.drop('airflow_4800','airflow_4802','engine_flight_hours','engine_cycles')
        # sp_df=sp_df.distinct()
        # display(sp_df)       
        return sp_df

    def airfoil_spal_df(self):
        ar_df = spark.sql(f"""
            with weighted_percentages as (
                select 
                    sp.stream_id,
                    sp.part_number,
                    sp.ppi_region,
                    ppi.airfoil_region,
                    sp.serial_number,
                    sp.shop_visit,
                    sp.create_station,
                    sp.inspection_index,
                    round(sp.full_spallation_percent * ppi.ppi_airfoil_ratio, 3) as weighted_full_spallation_percent,
                    round(sp.partial_spallation_percent * ppi.ppi_airfoil_ratio, 3) as weighted_partial_spallation_percent,
                    round(sp.erosion_percent * ppi.ppi_airfoil_ratio, 3) as weighted_erosion_percent,
                    round(sp.burnthrough_percent * ppi.ppi_airfoil_ratio, 3) as weighted_burnthrough_percent,
                    round(sp.other_percent * ppi.ppi_airfoil_ratio, 3) as weighted_other_percent
                from spallation as sp
                left join ppi_region as ppi
                on sp.ppi_region = ppi.PPI_region
                and sp.part_number = ppi.part_number
                where sp.processed = False
                and ppi.inspection_type = 'LPPI_Spall'
            )

            select 
                stream_id,
          shop_visit,
                create_station,
                inspection_index,
                part_number,
                serial_number,
                airfoil_region,
                round(sum(weighted_full_spallation_percent), 3) as full_spallation_percent,
                round(sum(weighted_partial_spallation_percent), 3) as partial_spallation_percent,
                round(sum(weighted_erosion_percent), 3) as erosion_percent,--
                round(sum(weighted_burnthrough_percent), 3) as burnthrough_percent,
                round(sum(weighted_other_percent), 3) as other_percent
            from weighted_percentages
            group by 
                stream_id,
                shop_visit,
                create_station,
                inspection_index,
                part_number,
                serial_number,
                airfoil_region
        """)
        print('ar_df')
        display(ar_df)
        return ar_df

    def spal_pivot_combined_df(self):

        df_ppi = self.ppi_spall_df()
        df_airfoil = self.airfoil_spal_df()
        print('df_ppi')
        display(df_ppi)
        print('df_airfoil')
        display(df_airfoil)
        df_ppi.createOrReplaceTempView("ppi_spal")
        df_airfoil.createOrReplaceTempView("airfoil_spal")

        spall_types = ["full_spallation", "partial_spallation", "erosion", "burnthrough", "other"]

        query_airfoil_percent = ""

        for i, spall_type in enumerate(spall_types):
            if i > 0:
                query_airfoil_percent += "union"
            query_airfoil_percent += f"""
                    select stream_id,
                        part_number,
                        serial_number,
                        shop_visit,
                        create_station,
                        inspection_index,
                        airfoil_region,
                        '{spall_type.replace("_", " ").title()}' as spallation_type,
                        {spall_type}_percent as airfoil_region_percent
                    from airfoil_spal
                """

        query_airfoil_percent += "order by serial_number, stream_id"

        df_airfoil_percent = spark.sql(query_airfoil_percent)

        print("airfoil df count: ", df_airfoil_percent.count())

        #this for loop and query is for extracting ppi percent

        query_ppi_percent = ""

        for i, spall_type in enumerate(spall_types):
            if i > 0:
                query_ppi_percent += "union"
            query_ppi_percent += f"""
                select serial_number,
                    stream_id,
                    airfoil_region,
                    ppi_region,
                    '{spall_type.replace("_", " ").title()}' as spallation_type,
                    {spall_type}_percent as ppi_region_percent
                from ppi_spal
            """

        query_ppi_percent += "order by serial_number, stream_id, ppi_region"

        df_ppi_percent = spark.sql(query_ppi_percent)

        df_airfoil_percent.display()
        print("df_airfoil_percent", df_airfoil_percent.select("serial_number").distinct().count())
        df_ppi_percent.display()
        print("df_ppi_percent", df_ppi_percent.select("serial_number").distinct().count())

        df_joined = df_airfoil_percent.join(
            df_ppi_percent,
            ["serial_number", "stream_id", "airfoil_region", "spallation_type"]
        )

        if df_joined.count() > 0:

            df_id = df_joined.withColumn("system_def_id", F.monotonically_increasing_id())

            df_id.createOrReplaceTempView("spallation_combined")
            df_id.display()

            df_combined = spark.sql("""
                select
                sp.system_def_id,
                sp.stream_id,
                sp.part_number,
                sp.serial_number,
                sp.shop_visit,
                sp.create_station,
                sp.inspection_index,
                sp.airfoil_region,
                sp.ppi_region,
                sp.spallation_type,
                sp.ppi_region_percent,
                sp.airfoil_region_percent,
                eng.ESN,
                eng.engine_flight_hours,
                eng.engine_cycles,
                eng.engine_operator,
                cast.casting_vendor,
                ct.coating_vendor,
                ct.coating_gain,
                ct.coating_date,
                hol.hole_drill_vendor,
                ar.`4800` as airflow_4800,
                ar.`4802` as airflow_4802,
                current_timestamp() as load_timestamp
                from spallation_combined as sp
                left join casting as cast on sp.serial_number = cast.serial_number
                left join engine as eng on eng.serial_number = cast.serial_number
                left join coat as ct on ct.serial_number = eng.serial_number
                left join hole as hol on hol.serial_number = ct.serial_number
                left join airflow_group as ar on ct.serial_number = ar.serial_number
            """)

            print("final scatterplot df:")
            df_combined.display()

            df_dict = {
                "df": df_combined,
                "table_name": "spallation_publish", 
                "data_layer": "published",
                "merge_schema": True
            }

            return df_dict
        
    def test_spallation_curatedtopublish(self):
        # expected_df = 
        self.mock_data_spallation()
        #display(inp_data)
        temp_df = self.spal_pivot_combined_df()
        output_df=temp_df['df']
        output_df.printSchema()
        # self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)


++++++++++++++++++++++++
schema = StructType([
    StructField("system_def_id", StringType(), nullable=True),
    StructField("stream_id", StringType(), nullable=True),
    StructField("part_number", StringType(), nullable=True),
    StructField("serial_number", StringType(), nullable=True),
    StructField("shop_visit", IntegerType(), nullable=True),
    StructField("create_station", StringType(), nullable=True),
    StructField("inspection_index", IntegerType(), nullable=True),
    StructField("airfoil_region", StringType(), nullable=True),
    StructField("ppi_region", StringType(), nullable=True),
    StructField("spallation_type", StringType(), nullable=False),
    StructField("ppi_region_percent", DoubleType(), nullable=True),
    StructField("airfoil_region_percent", DoubleType(), nullable=True),
    StructField("ESN", StringType(), nullable=True),
    StructField("engine_flight_hours", IntegerType(), nullable=True),
    StructField("engine_cycles", IntegerType(), nullable=True),
    StructField("engine_operator", StringType(), nullable=True),
    StructField("casting_vendor", StringType(), nullable=True),
    StructField("coating_vendor", StringType(), nullable=True),
    StructField("coating_gain", DoubleType(), nullable=True),
    StructField("coating_date", DateType(), nullable=True),
    StructField("hole_drill_vendor", StringType(), nullable=True),
    StructField("airflow_4800", DoubleType(), nullable=True),
    StructField("airflow_4802", DoubleType(), nullable=True)
])
dict_spall = {
    'stream_id': ['xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01'],
    'part_number': ['30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501'],
    'serial_number': ['aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122'],
    'shop_visit': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],
    'create_station': ['Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial'],
    'inspection_index': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],
    'airfoil_region': ['Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius', 'Flat Side','Flat Side', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius','Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Backing Radius','Flat Side', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Flat Side','Backing Radius', 'Backing Radius', 'Flat Side', 'Flat Side', 'Backing Radius','Backing Radius', 'Backing Radius', 'Flat Side', 'Backing Radius', 'Backing Radius','Backing Radius', 'Flat Side', 'Flat Side', 'Flat Side', 'Flat Side'],
    'ppi_region': ['y1', 'y3', 'y4', 'x1', 'y4', 'y2', 'x2', 'y1', 'y3', 'x3', 'x1', 'y1', 'y3', 'y2', 'x2', 'y4', 'x3', 'y3', 'x2', 'x1', 'y1', 'y4', 'y2', 'x3', 'x1', 'x2', 'x2', 'x3', 'x1', 'y3', 'y1', 'y4', 'x1', 'x2', 'x3'],
    'spallation_type': ['Full Spallation', 'Full Spallation', 'Full Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Partial Spallation', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Erosion', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Burnthrough', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Full Spallation', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other'],
    'ppi_region_percent': [0.007, 0, 0.101, 0.007, 0.091, 0.02, 0.5, 0, 0.08, 0.358, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.133, 0, 0.041, 0.013, 0, 0, 0, 0, 0, 0, 0],
    'airfoil_region_percent': [0.046, 0.046, 0.046, 0.337, 0.052, 0.052, 0.337, 0.052, 0.052, 0.337, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.045, 0.045, 0.045, 0.046, 0, 0, 0, 0, 0, 0, 0],
    'ESN': ['ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11', 'ESN11'],
    'engine_flight_hours': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500],
    'engine_cycles': [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50],
    'engine_operator': ['Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1', 'Operator1'],
    'casting_vendor': ['vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1', 'vendor1'],
    'coating_vendor': ['cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2', 'cvendor2'],
    'coating_gain': [4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3, 4.3],
    'coating_date': ['11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023', '11/11/2023'],
    'hole_drill_vendor': ['hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1', 'hvendor1'],
    'airflow_4800': [78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5, 78.5],
    'airflow_4802': [-98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5, -98.5]
}


++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 |-- stream_id: string (nullable = true)
 |-- part_number: string (nullable = true)
 |-- serial_number: string (nullable = true)
 |-- shop_visit: integer (nullable = true)
 |-- create_station: string (nullable = true)
 |-- inspection_index: integer (nullable = true)
 |-- airfoil_region: string (nullable = true)
 |-- ppi_region: string (nullable = true)
 |-- spallation_type: string (nullable = false)
 |-- ppi_region_percent: double (nullable = true)
 |-- airfoil_region_percent: double (nullable = true)
 |-- ESN: string (nullable = true)
 |-- engine_flight_hours: integer (nullable = true)
 |-- engine_cycles: integer (nullable = true)
 |-- engine_operator: string (nullable = true)
 |-- casting_vendor: string (nullable = true)
 |-- coating_vendor: string (nullable = true)
 |-- coating_gain: double (nullable = true)
 |-- coating_date: date (nullable = true)
 |-- hole_drill_vendor: string (nullable = true)
 |-- airflow_4800: double (nullable = true)
 |-- airflow_4802: double (nullable = true)


# Extended data in the format of a dictionary
dict_spall = {
    'part_number': ['30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501', '30G5501'],
    'serial_number': ['aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122'],
    'shop_visit': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],
    'create_station': ['Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial'],
    'inspection_index': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],
    'ppi_region': ['x1', 'x2', 'x3', 'y1', 'y2', 'y3', 'y4', 'x2', 'x1', 'x2', 'x3', 'y1', 'y2', 'y3', 'y4', 'x2', 'x1', 'x2', 'x3', 'y1', 'y2', 'y3', 'y4', 'x2', 'x1', 'x2', 'x3', 'y1', 'y2', 'y3', 'y4', 'x2', 'x1', 'x2', 'x3', 'y1', 'y2'],
    'full_spallation': [True, False, True, True, True, False, True, True, True, False, True, True, False, True, True, True, True, False, True, True, True, False, True, True, True, True, False, True, True, True, False, True, True, True, True, False],
    'full_spallation_percent': [0.133, 0.0, 0.041, 0.007, 0.013, 0.0, 0.101, 0.2, 0.133, 0.0, 0.041, 0.007, 0.013, 0.0, 0.101, 0.2, 0.133, 0.0, 0.041, 0.007, 0.013, 0.0, 0.101, 0.2, 0.133, 0.0, 0.041, 0.007, 0.013, 0.0, 0.101, 0.2, 0.133, 0.0, 0.041, 0.007, 0.013, 0.0],
    'partial_spallation': [True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, False],
    'partial_spallation_percent': [0.007, 0.5, 0.358, 0.0, 0.02, 0.08, 0.091, 0.0, 0.007, 0.5, 0.358, 0.0, 0.02, 0.08, 0.091, 0.0, 0.007, 0.5, 0.358, 0.0, 0.02, 0.08, 0.091, 0.0, 0.007, 0.5, 0.358, 0.0, 0.02, 0.08, 0.091, 0.0, 0.007, 0.5, 0.358, 0.0, 0.02, 0.08],
    'erosion': [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],
    'erosion_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    'crack': [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],
    'burnthrough': [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],
    'burnthrough_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    'other': [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False],
    'other_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    'stream_id': ['xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01'],
    'processed': [False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False]
}

# List of dictionaries
rows_inp_spall = [
    {'part_number': pn, 'serial_number': sn, 'shop_visit': sv, 'create_station': cs, 'inspection_index': ii, 'ppi_region': ppi,
     'full_spallation': fs, 'full_spallation_percent': fp, 'partial_spallation': ps, 'partial_spallation_percent': pp,
     'erosion': er, 'erosion_percent': ep, 'crack': ck, 'burnthrough': bn, 'burnthrough_percent': bp, 'other': ot, 'other_percent': op,
     'stream_id': si, 'processed': pr}
    for pn, sn, sv, cs, ii, ppi, fs, fp, ps, pp, er, ep, ck, bn, bp, ot, op, si, pr in zip(
        dict_spall['part_number'], dict_spall['serial_number'], dict_spall['shop_visit'], dict_spall['create_station'],
        dict_spall['inspection_index'], dict_spall['ppi_region'], dict_spall['full_spallation'], dict_spall['full_spallation_percent'],
        dict_spall['partial_spallation'], dict_spall['partial_spallation_percent'], dict_spall['erosion'], dict_spall['erosion_percent'],
        dict_spall['crack'], dict_spall['burnthrough'], dict_spall['burnthrough_percent'], dict_spall['other'], dict_spall['other_percent'],
        dict_spall['stream_id'], dict_spall['processed']
    )
]

# Displaying the list of dictionaries
for row in rows_inp_spall:
    print(row)



____________________________________________________________________________
system_def_id	stream_id	part_number	serial_number	shop_visit	create_station	inspection_index	airfoil_region	ppi_region	spallation_type	ppi_region_percent	airfoil_region_percent	ESN	engine_flight_hours	engine_cycles	engine_operator	casting_vendor	coating_vendor	coating_gain	coating_date	hole_drill_vendor	airflow_4800	airflow_4802	load_timestamp
0	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y1	Full Spallation	0.007	0.046	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
1	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y3	Full Spallation	0	0.046	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
2	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y4	Full Spallation	0.101	0.046	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
3	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x1	Partial Spallation	0.007	0.337	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
4	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y4	Partial Spallation	0.091	0.052	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
5	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y2	Partial Spallation	0.02	0.052	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
6	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x2	Partial Spallation	0.5	0.337	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
7	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y1	Partial Spallation	0	0.052	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
8	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y3	Partial Spallation	0.08	0.052	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
9	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x3	Partial Spallation	0.358	0.337	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
10	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x1	Erosion	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
11	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y1	Erosion	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
12	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y3	Erosion	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
13	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y2	Erosion	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
14	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x2	Erosion	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
15	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y4	Erosion	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
16	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x3	Erosion	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
17	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y3	Burnthrough	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
18	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x2	Burnthrough	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
19	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y1	Burnthrough	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
20	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x1	Burnthrough	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
21	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x3	Burnthrough	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
22	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y4	Burnthrough	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
23	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y2	Burnthrough	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
24	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x1	Full Spallation	0.133	0.045	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
25	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x2	Full Spallation	0	0.045	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
26	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x3	Full Spallation	0.041	0.045	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
27	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y2	Full Spallation	0.013	0.046	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
28	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x2	Other	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
29	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x3	Other	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
30	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Backing Radius	x1	Other	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
31	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y3	Other	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
32	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y1	Other	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
33	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y4	Other	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z
34	xxxx-01	30G5501	aabbcc001122	10	Initial	3	Flat Side	y2	Other	0	0	ESN11	500	50	Operator1	vendor1	cvendor2	4.3	11/11/2023	hvendor1	78.5	-98.5	2024-04-10T09:31:34.845Z




import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col,udf
from pyspark.sql.types import StructType, StructField, StringType,BooleanType,DoubleType,IntegerType,DateType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp
from datetime import datetime
warnings.filterwarnings("ignore")



class UnitTestFunctions(unittest.TestCase):
    test=True
    version=''

    def mock_data_spallation(self):
        #spallation table
        schema_spall=StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('ppi_region', StringType(), True),
            StructField('full_spallation', BooleanType(), True),
            StructField('full_spallation_percent', DoubleType(), True),
            StructField('partial_spallation', BooleanType(), True),
            StructField('partial_spallation_percent', DoubleType(), True),
            StructField('erosion', BooleanType(), True),
            StructField('erosion_percent', DoubleType(), True),
            StructField('crack', BooleanType(), True),
            StructField('burnthrough', BooleanType(), True),
            StructField('burnthrough_percent', DoubleType(), True),
            StructField('other', BooleanType(), True),
            StructField('other_percent', DoubleType(), True),
            StructField('stream_id', StringType(), True),
            StructField('processed',BooleanType(),True)
        ])
        dict_spall = {
            'part_number': ['30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501'],
            'serial_number': ['aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122', 'aabbcc001122'],
            'shop_visit': [10, 10, 10, 10, 10, 10, 10, 10],
            'create_station': ['Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial', 'Initial'],
            'inspection_index': [3, 3, 3, 3, 3, 3, 3, 3],
            'ppi_region': ['x1', 'x2', 'x3', 'y1', 'y2', 'y3', 'y4', 'x2'],
            'full_spallation': [True, False, True, True, True, False, True, True],
            'full_spallation_percent': [0.133, 0.0, 0.041, 0.007, 0.013, 0.0, 0.101, 0.2],
            'partial_spallation': [True, True, True, False, True, True, True, False],
            'partial_spallation_percent': [0.007, 0.5, 0.358, 0.0, 0.02, 0.08, 0.091, 0.0],
            'erosion': [False, False, False, False, False, False, False, False],
            'erosion_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'crack': [False, False, False, False, False, False, False, False],
            'burnthrough': [False, False, False, False, False, False, False, False],
            'burnthrough_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'other': [False, False, False, False, False, False, False, False],
            'other_percent': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'stream_id': ['xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01', 'xxxx-01'],
            'processed': [False, False, False, False, False, False, False, True]
        }
        rows_inp_spall = [
            {'part_number': pn ,'serial_number': sn, 'shop_visit':sv ,'create_station':cs,'inspection_index':ii,'ppi_region':ppi,'full_spallation':fs,'full_spallation_percent':fp,'partial_spallation':ps,'partial_spallation_percent':pp,'erosion':er,'erosion_percent':ep,'crack':ck,'burnthrough':bn,'burnthrough_percent':bp,'other':ot,'other_percent':op,'stream_id':si,'processed':pr}
            for pn,sn,sv,cs,ii,ppi,fs,fp,ps,pp,er,ep,ck,bn,bp,ot,op,si,pr in zip(
                dict_spall['part_number'],
                dict_spall['serial_number'],
                dict_spall['shop_visit'],
                dict_spall['create_station'],
                dict_spall['inspection_index'],
                dict_spall['ppi_region'],
                dict_spall['full_spallation'],
                dict_spall['full_spallation_percent'],
                dict_spall['partial_spallation'],
                dict_spall['partial_spallation_percent'],
                dict_spall['erosion'],
                dict_spall['erosion_percent'],
                dict_spall['crack'],
                dict_spall['burnthrough'],
                dict_spall['burnthrough_percent'],
                dict_spall['other'],
                dict_spall['other_percent'],
                dict_spall['stream_id'],
                dict_spall['processed']
            )
        ]
        spall_df=spark.createDataFrame(rows_inp_spall,schema=schema_spall)
        spall_df.createOrReplaceTempView('spallation')

        #ppi_airfoil_region table
        schema_ppi=StructType([
            StructField('part_number', StringType(), True),
            StructField('inspection_type', StringType(), True),
            StructField('airfoil_region', StringType(), True),
            StructField('ppi_region', StringType(), True),
            StructField('surface_area', DoubleType(), True),
            StructField('ppi_airfoil_ratio', DoubleType(), True)
        ])
        dict_ppi = {
            'part_number':['30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501','30G5501'],
            'inspection_type': ['LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall','LPPI_Spall'],
            'airfoil_region': ['Backing Radius','Backing Radius','Backing Radius','Flat Side','Flat Side','Flat Side','Flat Side','Forward Ledge'],
            'ppi_region': ['x1','x2','x3','y1','y2','y3','y4','z1'],
            'surface_area': [0.4,0.7,0.9,0.1,0.3,0.08,0.32,0.33],
            'ppi_airfoil_ratio':[0.2,0.35,0.45,0.125,0.375,0.1,0.4,0.1]
        }
        rows_inp_ppi = [
            {'part_number': pn ,'inspection_type': it, 'airfoil_region':ar ,'ppi_region':pr,'surface_area':sa,'ppi_airfoil_ratio':par}
            for pn,it,ar,pr,sa,par in zip(
                dict_ppi['part_number'],
                dict_ppi['inspection_type'],
                dict_ppi['airfoil_region'],
                dict_ppi['ppi_region'],
                dict_ppi['surface_area'],
                dict_ppi['ppi_airfoil_ratio']
            )
        ]
        ppi_ar_df=spark.createDataFrame(rows_inp_ppi,schema=schema_ppi)
        ppi_ar_df.createOrReplaceTempView('ppi_region')

        #casting table
        schema_cast=StructType([
            StructField('serial_number', StringType(), True),
            StructField('casting_vendor', StringType(), True)
        ])
        dict_cast = {
            'serial_number': ['aabbcc001122', 'aabbcc001133'],
            'casting_vendor': ['vendor1', 'vendor33']
        }
        rows_inp_cast = [
            {'serial_number': sn ,'casting_vendor': cv}
            for sn,cv in zip(
                dict_cast['serial_number'],
                dict_cast['casting_vendor']
            )
        ]
        cast_df=spark.createDataFrame(rows_inp_cast,schema=schema_cast)
        cast_df.createOrReplaceTempView('casting')
        
        #engine table
        schema_eng=StructType([
            StructField('serial_number', StringType(), True),
            StructField('ESN', StringType(), True),
            StructField('engine_flight_hours', IntegerType(), True),
            StructField('engine_cycles', IntegerType(), True),
            StructField('engine_operator', StringType(), True)
        ])
        dict_eng = {
            'serial_number': ['aabbcc001122', 'aabbcc001144'],
            'ESN': ['ESN11', 'ESN44'],
            'engine_flight_hours': [500,400],
            'engine_cycles': [50,40],
            'engine_operator': ['Operator1','Operator4']    
        }
        rows_inp_eng = [
            {'serial_number': sn ,'ESN': esn,'engine_flight_hours':efh,'engine_cycles':ec,'engine_operator':ep}
            for sn,esn,efh,ec,ep in zip(
                dict_eng['serial_number'],
                dict_eng['ESN'],
                dict_eng['engine_flight_hours'],
                dict_eng['engine_cycles'],
                dict_eng['engine_operator']
            )
        ]
        engine_df=spark.createDataFrame(rows_inp_eng,schema=schema_eng)
        engine_df.createOrReplaceTempView('engine')
        
        #coat table
        schema_coat=StructType([
            StructField('serial_number', StringType(), True),
            StructField('coating_vendor', StringType(), True),
            StructField('coating_gain', DoubleType(), True),
            StructField('coating_date',DateType(),True)
        ])
        dict_coat = {
            'serial_number': ['aabbcc001122', 'aabbcc001133'],
            'coating_vendor': ['cvendor2', 'cvendor3'],
            'coating_gain': [4.3,3.5],
            'coating_date':[datetime.strptime('2023-11-11','%Y-%m-%d'),datetime.strptime('2023-11-01','%Y-%m-%d')] 
        }
        rows_inp_coat = [
            {'serial_number': sn ,'coating_vendor': cv,'coating_gain':cg,'coating_date':cd}
            for sn,cv,cg,cd in zip(
                dict_coat['serial_number'],
                dict_coat['coating_vendor'],
                dict_coat['coating_gain'],
                dict_coat['coating_date']
            )
        ]
        coat_df=spark.createDataFrame(rows_inp_coat,schema=schema_coat)
        coat_df.createOrReplaceTempView('coat')
        
        #holedrill table
        schema_hole=StructType([
            StructField('serial_number', StringType(), True),
            StructField('hole_drill_vendor', StringType(), True)
        ])
        dict_hole = {
            'serial_number': ['aabbcc001122', 'aabbcc001132'],
            'hole_drill_vendor': ['hvendor1','hvendor2'] 
        }
        rows_inp_hole = [
            {'serial_number': sn ,'hole_drill_vendor': hv}
            for sn,hv in zip(
                dict_hole['serial_number'],
                dict_hole['hole_drill_vendor']
            )
        ]
        hole_df=spark.createDataFrame(rows_inp_hole,schema=schema_hole)
        hole_df.createOrReplaceTempView('hole')

        #airflow table
        schema_airflow=StructType([
            StructField('serial_number', StringType(), True),
            StructField('4800', DoubleType(), True),
            StructField('4802', DoubleType(), True)
        ])
        dict_airflow = {
            'serial_number': ['aabbcc001122', 'aabbcc001132'],
            '4800': [78.5,58.5],
            '4802': [-98.5,-28.5] 
        }
        rows_inp_airflow = [
            {'serial_number': sn ,'4800': n1,'4802':n2}
            for sn,n1,n2 in zip(
                dict_airflow['serial_number'],
                dict_airflow['4800'],
                dict_airflow['4802']
            )
        ]
        airflow_df=spark.createDataFrame(rows_inp_airflow,schema=schema_airflow)
        airflow_df.createOrReplaceTempView('airflow_group')

        return True

    def ppi_spall_df(self):
        sp_df = spark.sql(f"""
            select 
                sp.part_number,
                sp.serial_number,
                sp.shop_visit,
                sp.create_station,
                sp.inspection_index,
                sp.ppi_region,
                ppi.airfoil_region,
                sp.full_spallation,
                sp.full_spallation_percent,
                sp.partial_spallation,
                sp.partial_spallation_percent,
                sp.erosion,
                sp.erosion_percent,
                sp.crack,
                sp.burnthrough,
                sp.burnthrough_percent,
                sp.other,
                sp.other_percent,
                eng.ESN,
                eng.engine_flight_hours,
                eng.engine_cycles,
                eng.engine_operator,
                cast.casting_vendor,
                ct.coating_vendor,
                ct.coating_gain,
                ct.coating_date,
                hol.hole_drill_vendor,
                ar.`4800` as airflow_4800,
                ar.`4802` as airflow_4802,
                current_timestamp() as load_timestamp,
                sp.stream_id
            from spallation as sp
            left join casting as cast on sp.serial_number = cast.serial_number
            left join engine as eng on eng.serial_number = cast.serial_number
            left join coat as ct on ct.serial_number = eng.serial_number
            left join hole as hol on hol.serial_number = ct.serial_number
            left join airflow_group as ar on ct.serial_number = ar.serial_number
            left join ppi_region as ppi
            on sp.ppi_region = ppi.PPI_region
            and sp.part_number = ppi.part_number
            where sp.processed = False
            and ppi.inspection_type = 'LPPI_Spall'
        """)
        # print('distinct')
        # sp_df = sp_df.drop('airflow_4800','airflow_4802','engine_flight_hours','engine_cycles')
        # sp_df=sp_df.distinct()
        # display(sp_df)       
        return sp_df

    def airfoil_spal_df(self):
        ar_df = spark.sql(f"""
            with weighted_percentages as (
                select 
                    sp.stream_id,
                    sp.part_number,
                    sp.ppi_region,
                    ppi.airfoil_region,
                    sp.serial_number,
                    sp.shop_visit,
                    sp.create_station,
                    sp.inspection_index,
                    round(sp.full_spallation_percent * ppi.ppi_airfoil_ratio, 3) as weighted_full_spallation_percent,
                    round(sp.partial_spallation_percent * ppi.ppi_airfoil_ratio, 3) as weighted_partial_spallation_percent,
                    round(sp.erosion_percent * ppi.ppi_airfoil_ratio, 3) as weighted_erosion_percent,
                    round(sp.burnthrough_percent * ppi.ppi_airfoil_ratio, 3) as weighted_burnthrough_percent,
                    round(sp.other_percent * ppi.ppi_airfoil_ratio, 3) as weighted_other_percent
                from spallation as sp
                left join ppi_region as ppi
                on sp.ppi_region = ppi.PPI_region
                and sp.part_number = ppi.part_number
                where sp.processed = False
                and ppi.inspection_type = 'LPPI_Spall'
            )

            select 
                stream_id,
          shop_visit,
                create_station,
                inspection_index,
                part_number,
                serial_number,
                airfoil_region,
                round(sum(weighted_full_spallation_percent), 3) as full_spallation_percent,
                round(sum(weighted_partial_spallation_percent), 3) as partial_spallation_percent,
                round(sum(weighted_erosion_percent), 3) as erosion_percent,--
                round(sum(weighted_burnthrough_percent), 3) as burnthrough_percent,
                round(sum(weighted_other_percent), 3) as other_percent
            from weighted_percentages
            group by 
                stream_id,
                shop_visit,
                create_station,
                inspection_index,
                part_number,
                serial_number,
                airfoil_region
        """)
        print('ar_df')
        display(ar_df)
        return ar_df

    def spal_pivot_combined_df(self):

        df_ppi = self.ppi_spall_df()
        df_airfoil = self.airfoil_spal_df()
        print('df_ppi')
        display(df_ppi)
        print('df_airfoil')
        display(df_airfoil)
        df_ppi.createOrReplaceTempView("ppi_spal")
        df_airfoil.createOrReplaceTempView("airfoil_spal")

        spall_types = ["full_spallation", "partial_spallation", "erosion", "burnthrough", "other"]

        query_airfoil_percent = ""

        for i, spall_type in enumerate(spall_types):
            if i > 0:
                query_airfoil_percent += "union"
            query_airfoil_percent += f"""
                    select stream_id,
                        part_number,
                        serial_number,
                        shop_visit,
                        create_station,
                        inspection_index,
                        airfoil_region,
                        '{spall_type.replace("_", " ").title()}' as spallation_type,
                        {spall_type}_percent as airfoil_region_percent
                    from airfoil_spal
                """

        query_airfoil_percent += "order by serial_number, stream_id"

        df_airfoil_percent = spark.sql(query_airfoil_percent)

        print("airfoil df count: ", df_airfoil_percent.count())

        #this for loop and query is for extracting ppi percent

        query_ppi_percent = ""

        for i, spall_type in enumerate(spall_types):
            if i > 0:
                query_ppi_percent += "union"
            query_ppi_percent += f"""
                select serial_number,
                    stream_id,
                    airfoil_region,
                    ppi_region,
                    '{spall_type.replace("_", " ").title()}' as spallation_type,
                    {spall_type}_percent as ppi_region_percent
                from ppi_spal
            """

        query_ppi_percent += "order by serial_number, stream_id, ppi_region"

        df_ppi_percent = spark.sql(query_ppi_percent)

        df_airfoil_percent.display()
        print("df_airfoil_percent", df_airfoil_percent.select("serial_number").distinct().count())
        df_ppi_percent.display()
        print("df_ppi_percent", df_ppi_percent.select("serial_number").distinct().count())

        df_joined = df_airfoil_percent.join(
            df_ppi_percent,
            ["serial_number", "stream_id", "airfoil_region", "spallation_type"]
        )

        if df_joined.count() > 0:

            df_id = df_joined.withColumn("system_def_id", F.monotonically_increasing_id())

            df_id.createOrReplaceTempView("spallation_combined")
            df_id.display()

            df_combined = spark.sql("""
                select
                sp.system_def_id,
                sp.stream_id,
                sp.part_number,
                sp.serial_number,
                sp.shop_visit,
                sp.create_station,
                sp.inspection_index,
                sp.airfoil_region,
                sp.ppi_region,
                sp.spallation_type,
                sp.ppi_region_percent,
                sp.airfoil_region_percent,
                eng.ESN,
                eng.engine_flight_hours,
                eng.engine_cycles,
                eng.engine_operator,
                cast.casting_vendor,
                ct.coating_vendor,
                ct.coating_gain,
                ct.coating_date,
                hol.hole_drill_vendor,
                ar.`4800` as airflow_4800,
                ar.`4802` as airflow_4802,
                current_timestamp() as load_timestamp
                from spallation_combined as sp
                left join casting as cast on sp.serial_number = cast.serial_number
                left join engine as eng on eng.serial_number = cast.serial_number
                left join coat as ct on ct.serial_number = eng.serial_number
                left join hole as hol on hol.serial_number = ct.serial_number
                left join airflow_group as ar on ct.serial_number = ar.serial_number
            """)

            print("final scatterplot df:")
            df_combined.display()

            df_dict = {
                "df": df_combined,
                "table_name": "spallation_publish", 
                "data_layer": "published",
                "merge_schema": True
            }

            return df_dict
        
    def test_spallation_curatedtopublish(self):
        # expected_df = 
        self.mock_data_spallation()
        #display(inp_data)
        temp_df = self.spal_pivot_combined_df()
        output_df=temp_df['df']
        display(output_df)
        # self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)
===================================================================
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, DoubleType

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("example") \
    .getOrCreate()

# Your data and schema definition remain unchanged

# Empty list to store dictionaries
result_list = []

# Iterate over distinct stream_id and stream_label combinations
for row in df.select("stream_id", "stream_label").distinct().collect():
    stream_id = row.stream_id
    stream_label = row.stream_label
    
    # Filter data for current combination of stream_id and stream_label
    filtered_df = df.filter((col("stream_id") == stream_id) & (col("stream_label") == stream_label))
    
    # Full spallation data for current combination
    region_data = filtered_df.select("ppi_region", "full_spallation_percent", "partial_spallation_percent",
                                      "erosion_percent", "burnthrough_percent", "other_percent").collect()
    
    # List to hold dictionaries for each region
    regions_list = []
    
    # Iterate over regions in the filtered data
    for r in region_data:
        # List to hold damage dictionaries for each damage type
        damages_list = []
        
        # Append damage dictionaries to damages_list
        damages_list.append({"damage_type": "full_spallation", "damage_percentage": float(r.full_spallation_percent)})
        damages_list.append({"damage_type": "partial_spallation", "damage_percentage": float(r.partial_spallation_percent)})
        damages_list.append({"damage_type": "erosion", "damage_percentage": float(r.erosion_percent)})
        damages_list.append({"damage_type": "burnthrough", "damage_percentage": float(r.burnthrough_percent)})
        damages_list.append({"damage_type": "other", "damage_percentage": float(r.other_percent)})
        
        # Append region dictionary to the list
        regions_list.append({"region_name": r.ppi_region, "damages": damages_list})
    
    # Append stream information with regions list to the result list
    result_list.append({"stream_id": stream_id, "stream_label": stream_label, "regions": regions_list})

# Define schema for DataFrame
schema = StructType([
    StructField("stream_id", StringType(), True),
    StructField("stream_label", StringType(), True),
    StructField("regions", ArrayType(
        StructType([
            StructField("region_name", StringType(), True),
            StructField("damages", ArrayType(
                StructType([
                    StructField("damage_type", StringType(), True),
                    StructField("damage_percentage", DoubleType(), True)
                ])
            ), True)
        ])
    ), True)
])

# Create DataFrame
result_df = spark.createDataFrame(result_list, schema)

# Show DataFrame
result_df.show(truncate=False)

##########################################
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, DoubleType

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("example") \
    .getOrCreate()

# Your data and schema definition remain unchanged

# Empty list to store dictionaries
result_list = []

# Iterate over distinct stream_id and stream_label combinations
for row in df.select("stream_id", "stream_label").distinct().collect():
    stream_id = row.stream_id
    stream_label = row.stream_label
    
    # Filter data for current combination of stream_id and stream_label
    filtered_df = df.filter((col("stream_id") == stream_id) & (col("stream_label") == stream_label))
    
    # Full spallation data for current combination
    region_data = filtered_df.select("ppi_region", "full_spallation_percent", "partial_spallation_percent",
                                      "erosion_percent", "burnthrough_percent", "other_percent").collect()
    
    # List to hold region dictionaries
    region_list = []
    
    # Iterate over regions in the filtered data
    for r in region_data:
        # Dictionary to hold damage information for each region
        damage_dict = {
            "full_spallation": float(r.full_spallation_percent),
            "partial_spallation": float(r.partial_spallation_percent),
            "erosion": float(r.erosion_percent),
            "burnthrough": float(r.burnthrough_percent),
            "other": float(r.other_percent)
        }
        
        # Dictionary to hold region information
        region_dict = {"region_name": r.ppi_region, "damages": damage_dict}
        
        # Append region dictionary to the list
        region_list.append(region_dict)
    
    # Dictionary to hold stream information
    stream_dict = {
        "stream_id": stream_id,
        "stream_label": stream_label,
        "regions": region_list
    }
    
    # Append stream dictionary to the result list
    result_list.append(stream_dict)

# Define schema for DataFrame
schema = StructType([
    StructField("stream_id", StringType(), True),
    StructField("stream_label", StringType(), True),
    StructField("regions", ArrayType(
        StructType([
            StructField("region_name", StringType(), True),
            StructField("damages", MapType(StringType(), DoubleType()), True)
        ])
    ), True)
])

# Create DataFrame
result_df = spark.createDataFrame(result_list, schema)

# Show DataFrame
result_df.show(truncate=False)

++++++++++++++++++++++++++++++++++++++++++++++++++++++
schema = StructType([
    StructField("region_name", StringType(), True),
    StructField("damages", ArrayType(
        StructType([
            StructField("damage_type", StringType(), True),
            StructField("damage_percentage", FloatType(), True)
        ])
    ))
])

# Create DataFrame from list of dictionaries
result_df = spark.createDataFrame(result_list, schema)

# Show DataFrame
result_df.show(truncate=False)
=========================================================
from pyspark.sql import Row
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
import json

# Your data and schema definition remain unchanged

# Empty list to store dictionaries
result_list = []

# Iterate over distinct stream_id and stream_label combinations
for row in df.select("stream_id", "stream_label").distinct().collect():
    stream_id = row.stream_id
    stream_label = row.stream_label
    
    # Filter data for current combination of stream_id and stream_label
    filtered_df = df.filter((col("stream_id") == stream_id) & (col("stream_label") == stream_label))
    
    # Full spallation data for current combination
    region_data = filtered_df.select("ppi_region", "full_spallation_percent", "partial_spallation_percent",
                                      "erosion_percent", "burnthrough_percent", "other_percent").collect()
    
    # Iterate over regions in the filtered data
    for r in region_data:
        # Dictionary to hold region information
        region_dict = {"region_name": r.ppi_region, "damages": []}
        
        # Append damage type and percentage to the damages list
        region_dict["damages"].append({"damage_type": "full_spallation", "damage_percentage": float(r.full_spallation_percent)})
        region_dict["damages"].append({"damage_type": "partial_spallation", "damage_percentage": float(r.partial_spallation_percent)})
        region_dict["damages"].append({"damage_type": "erosion", "damage_percentage": float(r.erosion_percent)})
        region_dict["damages"].append({"damage_type": "burnthrough", "damage_percentage": float(r.burnthrough_percent)})
        region_dict["damages"].append({"damage_type": "other", "damage_percentage": float(r.other_percent)})
        
        # Append region dictionary to the result list
        result_list.append(region_dict)

# Convert result list to JSON format
json_result = json.dumps(result_list, indent=4)

# Print JSON result
print(json_result)

=======================================================
[
      {
        "region_name": "x1",
        "damages": [
          {
            "damage_type": "full_spallation",
            "damage_percentage": 0.1
          },
          {
            "damage_type": "partial_spallation",
            "damage_percentage": 0.2
          }
        ]
      },
      {
        "region_name": "x2",
        "damages: [
          {
            "damage_type": "full_spallation",
            "damage_percentage": 0.05
          },
          {
            "damage_type": "partial_spallation",
            "damage_percentage": 0.32
          }
        ]
     }
]

from pyspark.sql.functions import col
from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
import json

data = [
    ("x1", 0.1, 0.2, 0.03, 0.09, 0.2, "xxxx-01", "xxxx-01"),
    ("x2", 0.7, 0.13, 0.02, 0.09, 0.5, "xxxx-01", "xxxx-01"),
    ("x3", 0.3, 0.8, 0.2, 0.8, 0.02, "xxxx-02", "xxxx-02"),
    ("x4", 0.8, 0.05, 0.7, 0.01, 0.04, "xxxx-02", "xxxx-02")
]
schema = StructType([
    StructField('ppi_region', StringType(), True),
    StructField('full_spallation_percent', DoubleType(), True),
    StructField('partial_spallation_percent', DoubleType(), True),
    StructField('erosion_percent', DoubleType(), True),
    StructField('burnthrough_percent', DoubleType(), True),
    StructField('other_percent', DoubleType(), True),
    StructField('stream_id', StringType(), True),
    StructField('stream_label', StringType(), True)
])
df = spark.createDataFrame(data, schema)

#empty list to store Row objects
new_data = []
#empty list to store results
result_list = []
for row in df.select("stream_id", "stream_label").distinct().collect():
    stream_id = row.stream_id
    stream_label = row.stream_label
    
    # Filter data for current combination of stream_id and stream_label
    filtered_df = df.filter((col("stream_id") == stream_id) & (col("stream_label") == stream_label))
    # display(filtered_df)
    
    #full spallation data
    region_data = filtered_df.select("ppi_region", "full_spallation_percent","partial_spallation_percent","erosion_percent", "burnthrough_percent", "other_percent","stream_id", "stream_label").collect()
    
    region_list = [
        {"region_name": r.ppi_region, "damages":
            [
            {"damage_type":"full_spallation","damage_percentage":float(r.full_spallation_percent)},
            {"damage_type":"partial_spallation","damage_percentage":float(r.partial_spallation_percent)},
            {"damage_type":"erosion","damage_percentage":float(r.erosion_percent)},
            {"damage_type":"burnthrough","damage_percentage":float(r.burnthrough_percent)},
            {"damage_type":"other","damage_percentage":float(r.other_percent)}
            ]
        }
        for r in region_data
    ]

    
    # print(region_list)
    new_row = Row(stream_id=stream_id, stream_label=stream_label, regions=region_list)
    # Add the new Row to the list
    new_data.append(new_row)

# Create a new DataFrame from the list of Rows
# Create DataFrame from result list
schema_res = StructType([
    StructField('stream_id', StringType(), True),
    StructField('stream_label', StringType(), True),
    StructField('regions', StructType([
        StructField('damages', StringType(), True),
        StructField('partial_spallation', StringType(), True),
        StructField('erosion', StringType(), True),
        StructField('burnthrough', StringType(), True),
        StructField('other', StringType(), True)
    ]), True)
])
result_df = spark.createDataFrame(new_data,schema_res)


result_df.show(truncate=False)
result_df.printSchema()

# # Convert DataFrame to JSON format
json_result = result_df.toJSON().collect()

# Print JSON result
# for json_string in json_result:
#     print(json_string)
json_object = json.dumps(json_result, indent = 4) 
 
# Print JSON object
print('json')
print(json_object)
++++++++++++++++++++++++++++++++++
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
import json

data = [
    ("x1", 0.1, 0.2, 0.03, 0.09, 0.2, "xxxx-01", "xxxx-01"),
    ("x2", 0.7, 0.13, 0.02, 0.09, 0.5, "xxxx-01", "xxxx-01"),
    ("x3", 0.3, 0.8, 0.2, 0.8, 0.02, "xxxx-02", "xxxx-02"),
    ("x4", 0.8, 0.05, 0.7, 0.01, 0.04, "xxxx-02", "xxxx-02")
]
columns = ["ppi_region", "full_spallation_percent", "partial_spallation_percent",
           "erosion_percent", "burnthrough_percent", "other_percent",
           "stream_id", "stream_label"]
df = spark.createDataFrame(data, columns)

#empty list to store results
result_list = []
for row in df.select("stream_id", "stream_label").distinct().collect():
    stream_id = row.stream_id
    stream_label = row.stream_label
    
    # Filter data for current combination of stream_id and stream_label
    filtered_df = df.filter((col("stream_id") == stream_id) & (col("stream_label") == stream_label))
    # display(filtered_df)
    
    #full spallation data
    full_spallation_data = filtered_df.select("ppi_region", "full_spallation_percent").collect()
    full_spallation_list = [{"ppi_region": r.ppi_region, "spallation_value": r.full_spallation_percent} 
                            for r in full_spallation_data]
    # partial spallation data
    partial_spallation_data = filtered_df.select("ppi_region", "partial_spallation_percent").collect()
    partial_spallation_list = [{"ppi_region": r.ppi_region, "spallation_value": r.partial_spallation_percent} 
                               for r in partial_spallation_data]
    
    # erosion percent
    erosion_percent_data = filtered_df.select("ppi_region", "erosion_percent").collect()
    erosion_percent_list = [{"ppi_region": r.ppi_region, "erosion_percent": r.erosion_percent} 
                               for r in erosion_percent_data]
    
    # burnthrough percent
    burnthrough_percent_data = filtered_df.select("ppi_region", "burnthrough_percent").collect()
    burnthrough_percent_list = [{"ppi_region": r.ppi_region, "burnthrough_percent": r.burnthrough_percent} 
                               for r in burnthrough_percent_data]
    
    # other percent
    other_percent_data = filtered_df.select("ppi_region", "other_percent").collect()
    other_percent_list = [{"ppi_region": r.ppi_region, "other_percent": r.other_percent} 
                               for r in other_percent_data]
    
    #JSON like format
    spallation_dict = {
        "full_spallation": full_spallation_list,  
        "partial_spallation": partial_spallation_list,
        "erosion": erosion_percent_list,
        "burnthrough": burnthrough_percent_list,
        "other": other_percent_list, 
    }
    
    # Appending to result list
    result_list.append((stream_id, stream_label, spallation_dict))

print(result_list)

# Create DataFrame from result list
# schema=StructType([
#     StructField('stream_id', StringType(),True),
#     StructField('stream_label', StringType(),True),
#     StructField('spallation',StructType([
#         StructField('full_spallation',StructType([
#             StructField('ppi_region',StringType(),True),
#             StructField('spallation_value',DoubleType(),True)
#         ]),True),
#         StructField('partial_spallation',StructType([
#             StructField('ppi_region',StringType(),True),
#             StructField('spallation_value',DoubleType(),True)
#         ]),True),
#         StructField('erosion',StructType([
#             StructField('ppi_region',StringType(),True),
#             StructField('erosion_percent',DoubleType(),True)
#         ]),True),
#         StructField('burnthrough',StructType([
#             StructField('ppi_region',StringType(),True),
#             StructField('burnthrough_percent',DoubleType(),True)
#         ]),True),
#         StructField('other',StructType([
#             StructField('ppi_region',StringType(),True),
#             StructField('other_percent',DoubleType(),True)
#         ]),True)
#     ]),True)
# ])
result_df = spark.createDataFrame(result_list, schema)
display(result_df)

# Convert DataFrame to JSON format
json_result = result_df.toJSON().collect()

# Print JSON result
for json_string in json_result:
    print(json_string)
json_object = json.dumps(result_list, indent = 4) 
 
# Print JSON object
print(json_object)
+++++++++++++++++++++++++++++++++++++++++++++++++++++
from pyspark.sql import SparkSession
from pyspark.sql.functions import collect_list, struct, col, lit

# Assuming SparkSession is already initialized
spark = SparkSession.builder \
    .appName("Data Transformation") \
    .getOrCreate()

# Create DataFrame from provided data
data = [
    ("x1", 0.1, 0.2, 0.03, 0, 0, "xxxx-01", "xxxx-01"),
    ("x2", 0.7, 0.13, 0.02, 0.09, 0, "xxxx-01", "xxxx-01"),
    ("x3", 0.3, 0.8, 0.2, 0, 0.02, "xxxx-02", "xxxx-02"),
    ("x4", 0, 0.05, 0, 0.01, 0, "xxxx-02", "xxxx-02")
]
columns = ["ppi_region", "full_spallation_percent", "partial_spallation_percent",
           "erosion_percent", "burnthrough_percent", "other_percent",
           "stream_id", "stream_label"]
df = spark.createDataFrame(data, columns)

# Initialize an empty list to store results
result_list = []

# Iterate over distinct combinations of stream_id and stream_label
for row in df.select("stream_id", "stream_label").distinct().collect():
    stream_id = row.stream_id
    stream_label = row.stream_label
    
    # Filter data for current combination of stream_id and stream_label
    filtered_df = df.filter((col("stream_id") == stream_id) & (col("stream_label") == stream_label))
    
    # Collect full spallation data
    full_spallation_data = filtered_df.select("ppi_region", "full_spallation_percent").collect()
    full_spallation_list = [{"ppi_region": r.ppi_region, "spallation_value": r.full_spallation_percent} for r in full_spallation_data]
    
    # Collect partial spallation data
    partial_spallation_data = filtered_df.select("ppi_region", "partial_spallation_percent").collect()
    partial_spallation_list = [{"ppi_region": r.ppi_region, "spallation_value": r.partial_spallation_percent} for r in partial_spallation_data]
    
    # Construct JSON-like format
    spallation_dict = {
        "full_spallation": full_spallation_list,
        "partial_spallation": partial_spallation_list
    }
    
    # Append to result list
    result_list.append((stream_id, stream_label, spallation_dict))

# Create DataFrame from result list
result_df = spark.createDataFrame(result_list, ["stream_id", "stream_label", "spallation"])

# Convert DataFrame to JSON format
json_result = result_df.toJSON().collect()

# Print JSON result
for json_string in json_result:
    print(json_string)

****************************************************************************
from pyspark.sql import SparkSession
from pyspark.sql.functions import collect_list, struct, col, lit, concat_ws

# Assuming SparkSession is already initialized
spark = SparkSession.builder \
    .appName("Data Transformation") \
    .getOrCreate()

# Create DataFrame from provided data
data = [
    ("x1", 0.1, 0.2, 0.03, 0, 0, "xxxx-01", "xxxx-01"),
    ("x2", 0.7, 0.13, 0.02, 0.09, 0, "xxxx-01", "xxxx-01"),
    ("x3", 0.3, 0.8, 0.2, 0, 0.02, "xxxx-02", "xxxx-02"),
    ("x4", 0, 0.05, 0, 0.01, 0, "xxxx-02", "xxxx-02")
]
columns = ["ppi_region", "full_spallation_percent", "partial_spallation_percent",
           "erosion_percent", "burnthrough_percent", "other_percent",
           "stream_id", "stream_label"]
df = spark.createDataFrame(data, columns)

# Group by stream_id and stream_label and collect list of structs for spallation
result_df = df.groupby("stream_id", "stream_label").agg(
    collect_list(
        struct(
            col("ppi_region").alias("ppi_region"),
            col("full_spallation_percent").alias("spallation_value")
        )
    ).alias("full_spallation"),
    collect_list(
        struct(
            col("ppi_region").alias("ppi_region"),
            col("partial_spallation_percent").alias("spallation_value")
        )
    ).alias("partial_spallation")
)

# Concatenate the lists of structs into a single string with desired format
result_df = result_df.withColumn(
    "full_spallation",
    to_json(struct(lit("full_spallation").alias("type"), col("full_spallation")))
)

result_df = result_df.withColumn(
    "partial_spallation",
    to_json(struct(lit("partial_spallation").alias("type"), col("partial_spallation")))
)

result_df = result_df.withColumn(
    "spallation",
    concat_ws(", ", 
              lit('{"full_spallation": ['),
              col("full_spallation"),
              lit('], "partial_spallation": ['),
              col("partial_spallation"),
              lit(']}')
             )
)

# Drop the full_spallation and partial_spallation columns
result_df = result_df.drop("full_spallation", "partial_spallation")

# Show the transformed DataFrame
result_df.show(truncate=False)

=========================================
from pyspark.sql import SparkSession
from pyspark.sql.functions import collect_list, struct, col, create_map, lit, concat_ws

# Assuming SparkSession is already initialized
spark = SparkSession.builder \
    .appName("Data Transformation") \
    .getOrCreate()

# Create DataFrame from provided data
data = [
    ("x1", 0.1, 0.2, 0.03, 0, 0, "xxxx-01", "xxxx-01"),
    ("x2", 0.7, 0.13, 0.02, 0.09, 0, "xxxx-01", "xxxx-01"),
    ("x3", 0.3, 0.8, 0.2, 0, 0.02, "xxxx-02", "xxxx-02"),
    ("x4", 0, 0.05, 0, 0.01, 0, "xxxx-02", "xxxx-02")
]
columns = ["ppi_region", "full_spallation_percent", "partial_spallation_percent",
           "erosion_percent", "burnthrough_percent", "other_percent",
           "stream_id", "stream_label"]
df = spark.createDataFrame(data, columns)

# Group by stream_id and stream_label and collect list of structs for spallation
result_df = df.groupby("stream_id", "stream_label").agg(
    collect_list(
        struct(
            col("ppi_region").alias("ppi_region"),
            col("full_spallation_percent").alias("spallation_value")
        )
    ).alias("full_spallation"),
    collect_list(
        struct(
            col("ppi_region").alias("ppi_region"),
            col("partial_spallation_percent").alias("spallation_value")
        )
    ).alias("partial_spallation")
)

# Concatenate the lists of structs into a single string with desired format
result_df = result_df.withColumn(
    "spallation",
    concat_ws(", ", 
              lit('{"full_spallation": '), 
              to_json(col("full_spallation")), 
              lit(', "partial_spallation": '), 
              to_json(col("partial_spallation")), 
              lit('}')
             )
)

# Show the transformed DataFrame
result_df.show(truncate=False)

++++++++++++++++++++++++++++++++++++++++++++++++
from pyspark.sql import SparkSession
from pyspark.sql.functions import collect_list, struct, col, create_map

# Assuming SparkSession is already initialized
spark = SparkSession.builder \
    .appName("Data Transformation") \
    .getOrCreate()

# Create DataFrame from provided data
data = [
    ("x1", 0.1, 0.2, 0.03, 0, 0, "xxxx-01", "xxxx-01"),
    ("x2", 0.7, 0.13, 0.02, 0.09, 0, "xxxx-01", "xxxx-01"),
    ("x3", 0.3, 0.8, 0.2, 0, 0.02, "xxxx-02", "xxxx-02"),
    ("x4", 0, 0.05, 0, 0.01, 0, "xxxx-02", "xxxx-02")
]
columns = ["ppi_region", "full_spallation_percent", "partial_spallation_percent",
           "erosion_percent", "burnthrough_percent", "other_percent",
           "stream_id", "stream_label"]
df = spark.createDataFrame(data, columns)

# Group by stream_id and stream_label and collect list of structs for spallation
result_df = df.groupby("stream_id", "stream_label").agg(
    create_map(
        lit("full_spallation"),
        collect_list(
            struct(
                col("ppi_region").alias("ppi_region"),
                col("full_spallation_percent").alias("spallation_value")
            )
        ),
        lit("partial_spallation"),
        collect_list(
            struct(
                col("ppi_region").alias("ppi_region"),
                col("partial_spallation_percent").alias("spallation_value")
            )
        )
    ).alias("spallation")
)

# Show the transformed DataFrame
result_df.show(truncate=False)

===========================
from pyspark.sql import SparkSession
from pyspark.sql.functions import collect_list, struct, col

# Assuming SparkSession is already initialized
spark = SparkSession.builder \
    .appName("Data Transformation") \
    .getOrCreate()

# Create DataFrame from provided data
data = [
    ("x1", 0.1, 0.2, 0.03, 0, 0, "xxxx-01", "xxxx-01"),
    ("x2", 0.7, 0.13, 0.02, 0.09, 0, "xxxx-01", "xxxx-01"),
    ("x3", 0.3, 0.8, 0.2, 0, 0.02, "xxxx-02", "xxxx-02"),
    ("x4", 0, 0.05, 0, 0.01, 0, "xxxx-02", "xxxx-02")
]
columns = ["ppi_region", "full_spallation_percent", "partial_spallation_percent",
           "erosion_percent", "burnthrough_percent", "other_percent",
           "stream_id", "stream_label"]
df = spark.createDataFrame(data, columns)

# Group by stream_id and stream_label and collect list of structs for spallation
result_df = df.groupby("stream_id", "stream_label").agg(
    collect_list(
        struct(
            col("ppi_region").alias("ppi_region"),
            col("full_spallation_percent").alias("spallation_value")
        )
    ).alias("full_spallation"),
    collect_list(
        struct(
            col("ppi_region").alias("ppi_region"),
            col("partial_spallation_percent").alias("spallation_value")
        )
    ).alias("partial_spallation")
)

# Show the transformed DataFrame
result_df.show(truncate=False)

==================================================================
ppi_region	full_spallation_percent	partial_spallation_percent	erosion_percent	burnthrough_percent	other_percent	stream_id	stream_label
x1	0.1	0.2	0.03	0	0	xxxx-01	xxxx-01
x2	0.7	0.13	0.02	0.09	0	xxxx-01	xxxx-01
x3	0.3	0.8	0.2	0	0.02	xxxx-02	xxxx-02
x4	0	0.05	0	0.01	0	xxxx-02	xxxx-02
							
							
stream_id	stream_label	spallation					
xxxx-01	xxxx-01	{full_spallation: [ {"ppi_region":"x1", "spallation_value":0.1}, {"ppi_region":"x2", "spallation_value":0.7}, {"ppi_region":"x3", "spallation_value":0.3}, {"ppi_region":"x4", "spallation_value":0.0} ], "partial_spallation": [ {"ppi_region":"x1", "spallation_value":0.2}, {"ppi_region":"x2", "spallation_value":0.13}, ... ] ... }					
							
							
							
xxxx-02	xxxx-02	{full_spallation: [ {"ppi_region":"x1", "spallation_value":0.1}, {"ppi_region":"x2", "spallation_value":0.7}, {"ppi_region":"x3", "spallation_value":0.3}, {"ppi_region":"x4", "spallation_value":0.0} ], "partial_spallation": [ {"ppi_region":"x1", "spallation_value":0.2}, {"ppi_region":"x2", "spallation_value":0.13}, ... ] ... }					
							
							
							
----------------------------------------------------------------
import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp

warnings.filterwarnings("ignore")


class UnitTestFunctions(unittest.TestCase):
    test = True
    version = ''

    def spallation_curated_dataframe(self):
        schema_inp=StructType([
            StructField('detections',StringType(),True),
            StructField('file_path',StringType(),True)
        ])
        mock_data_input = {'team': 'A',
                           'points': 1,
                           'assists': 5,
                           'all_star': True}
        df_input = spark.createDataFrame([mock_data_input], schema=schema_inp)
        dtypes_df = df_input.dtypes
        return df_input, dtypes_df

    def test_spallation_curated_dataframe(self):
        input_df, dtypes_inp = self.spallation_curated_dataframe()
        dtypes_exp = input_df.dtypes
        self.assertTrue(dtypes_inp.equals(dtypes_exp))


if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)
--------------------------------
class InspectionRawToCurated:
    def __init__(self, version: str, test: bool = False):
        self.version = version
        self.test = test

    def inspection_validation(self):
        if self.test:
            df_tag = spark.sql(f"select * from dmro_raw_tag{self.version} where processed = False")
        else:
            df_tag = spark.sql(f"select * from dmro_raw.tag{self.version} where processed = False")

        if df_tag.count != 0:
            exception_df = df_tag.filter(
                (df_tag['operation_code'].isNull() == True) | (df_tag['create_time'].isNull() == True)
            )
            if exception_df.count() > 0:
                reason_df = exception_df.withColumn("reason", lit("operation code or create time is null"))
                return [exception_df, df_tag]
            else:
                return [df_tag]
        else:
            print("No New Records")
            return

    def inspection_curated_dataframe(self, validated_df: DataFrame):
        df_tag = validated_df.withColumn('inspection_date', F.date_format(F.col('create_time'), 'yyyy-MM-dd')) \
            .withColumn('inspection_time', F.date_format(F.col('create_time'), 'HH:mm:ss'))

        print("print df with columns split")
        df_tag.display()

        # select the necessary fields
        df_tag = df_tag.select(
            'part_number',
            'serial_number',
            'automated_disposition',
            'create_station',
            'inspection_index',
            'inspector_disposition',
            'inspection_facility',
            'operation_code',
            'ppi_type',
            'shop_visit',
            'stream_id',
            'inspection_date',
            'inspection_time'
        )

        # input new current timestamp
        df_tag = df_tag.withColumn("inspection_index", df_tag.inspection_index.cast(IntegerType())) \
            .withColumn("operation_code", df_tag.operation_code.cast(IntegerType())) \
            .withColumn("inspection_date", df_tag.inspection_date.cast(DateType())) \
            .withColumn("shop_visit", df_tag.shop_visit.cast(IntegerType()))
        # .withColumn("inspection_time",df_tag.inspection_time.cast('timestamp'))

        df_tag.schema
        df_tag.display()

        df_tag_dict = {
            "df": df_tag,
            "table_name": "inspection",
            "data_layer": "curated",
            "merge_schema": False
        }

        return df_tag_dict

    def process_inspection(self):
        insp_validation_dataframes = self.inspection_validation()

        if len(insp_validation_dataframes) > 0:

            if len(insp_validation_dataframes) > 1:
                exception_df_dict, validated_df_dict = insp_validation_dataframes
            else:
                validated_df_dict = insp_validation_dataframes[0]

            curated_df_dict = self.inspection_curated_dataframe(validated_df_dict)

            # if exception_df_dict doesn't exist, this line will throw NameError
            try:
                return [exception_df_dict, curated_df_dict]

            # if there's a NameError, return only curated_df_dict
            except NameError:
                return [curated_df_dict]
        else:
            pass


# Example usage:
inspection_processor = InspectionRawToCurated("your_version", test=True)
result = inspection_processor.process_inspection()
class DefectRawToCurated:
    def __init__(self, version: str, test: bool = False):
        self.version = version
        self.test = test

    def defect_validation(self):
        if self.test:
            df_tag = spark.read.table("dmro_raw_tag")    
            print("tag from raw")
            df_tag.display()

            df_def = spark.read.table(f"dmro_raw_defect{self.version}")
            print("defect from raw")
            df_def.display()

            df_ppi = spark.read.table("dmro_curated_ppi_airfoil_region")
        else:
            df_tag = spark.read.table(f"dmro_raw.tag{self.version}")
            df_def = spark.read.table(f"dmro_raw.defect{self.version}")
            df_ppi = spark.read.table("dmro_curated.ppi_airfoil_region")
            
        df_tag.createOrReplaceTempView("tag")
        df_ppi.createOrReplaceTempView("ppi")

        df_ppi.display()

        # check for rows with null disposition or indication_type values and return as dataframe in a dictionary
        exception_df = df_def.filter(
            (df_def['disposition'].isNull() == True) | (df_def['`indication_type`'].isNull() == True)
        )

        if exception_df.count() > 0:
            reason_df = exception_df.withColumn("reason", lit("disposition or indication type is null"))
            df_def = df_def.filter(
                (df_def['disposition'].isNotNull()) & (df_def['`indication_type`'].isNotNull())
            )

            return [reason_df, df_def]
        else:
            return [df_def]
            
    def defect_curated_dataframe(self, validated_df: DataFrame):
        validated_df.createOrReplaceTempView("defect")
        print("joining")

        # sql statement with joins from tag to defect and ppi based on max timestamp
        df = spark.sql("""
            select tg.part_number,
                tg.serial_number,
                df.`Defect_Type` as defect_type,
                df.size,
                df.disposition as defect_disposition,
                df.`Indication_Type` as ppi_region,
                df.`Short_text` as short_text,
                df.x,
                df.y,
                df.z,
                df.`Analytics_Confidence` as analytics_confidence,
                df.angle,
                df.elongation,
                df.uuid,
                df.stream_id,
                pp.airfoil_region
            from tag as tg
            left outer join defect as df
            on df.stream_id = tg.stream_id 
            left outer join ppi as pp
            on pp.PPI_region = rtrim(df.`Indication_Type`) and pp.part_number = tg.part_number
            where tg.processed = False
        """)

        print("defect joined table")
        df.schema
        df.display()

        # input new current timestamp
        if df.count != 0:
            df = df.withColumn("size", df.size.cast(DoubleType()))\
                   .withColumn("x", df.x.cast(DoubleType()))\
                   .withColumn("y", df.y.cast(DoubleType()))\
                   .withColumn("z", df.z.cast(DoubleType()))\
                   .withColumn("analytics_confidence", df.analytics_confidence.cast(DoubleType()))\
                   .withColumn("angle", df.angle.cast(DoubleType()))\
                   .withColumn("elongation", df.elongation.cast(DoubleType()))

            print("writing defect curated table")
            df.display()

            df_dict = {
                "df": df,
                "table_name": "defect", 
                "data_layer": "curated", 
                "merge_schema": False
            }

            return df_dict
                
        else: 
            print("No New Records")

    def process_defect(self):
        def_validation_dataframes = self.defect_validation()

        if len(def_validation_dataframes) > 0:
            if len(def_validation_dataframes) > 1:
                exception_df_dict, validated_df_dict = def_validation_dataframes
            else:
                validated_df_dict = def_validation_dataframes[0]

            curated_df_dict = self.defect_curated_dataframe(validated_df_dict)

            # if exception_df_dict doesn't exist, this line will throw NameError
            try:
                return [exception_df_dict, curated_df_dict]
            # if there's a NameError, return only curated_df_dict
            except NameError:
                return [curated_df_dict]
        else:
            pass

# Example usage:
defect_processor = DefectRawToCurated("your_version", test=True)
result = defect_processor.process_defect()


''''''''''''''''''''''''''''''''''''''
class DefectDetectionMatcher:
    def __init__(self, version: str, test: bool = False):
        self.version = version
        self.test = test

    def read_tables(self):
        if self.test:
            self.dt = spark.read.table("dmro_raw_detection")
            self.df = spark.read.table("dmro_raw_defect")
        else:
            self.dt = spark.read.table(f"dmro_raw.detection{self.version}")
            self.df = spark.read.table(f"dmro_raw.defect{self.version}")

    def create_temp_views(self):
        self.dt.createOrReplaceTempView("dt")
        self.df.createOrReplaceTempView("df")

    def find_mismatches(self):
        df_not_match = spark.sql("""
            SELECT dt.uuid, dt.stream_id, 'detection' AS from_table
            FROM dt
            LEFT OUTER JOIN df ON dt.uuid = df.uuid
            WHERE df.stream_id IS NULL AND dt.processed IS FALSE
            UNION ALL
            SELECT dt.uuid, dt.stream_id, 'defect' AS from_table
            FROM df
            LEFT OUTER JOIN dt ON dt.uuid = df.uuid
            WHERE dt.stream_id IS NULL AND df.processed IS FALSE
        """)
        return df_not_match

    def check_and_return_result(self):
        self.read_tables()
        self.create_temp_views()
        df_not_match = self.find_mismatches()

        if df_not_match.count() != 0:
            print("Defect/detection uuid mismatch found.")
            df_not_match = df_not_match.withColumn("reason", lit("the following 1-1 UUID does not match in defect & detections"))

            df_final = {
                "df": df_not_match,
                "table_name": "defect_detection_uuid_match",
                "data_layer": "exception",
                "merge_schema": False
            }
            return df_final
        else:
            print("Defect/detection uuid match validation passed.")


class InspectionRawToCurated:
    def __init__(self, version: str, test: bool = False):
        self.version = version
        self.test = test

    def inspection_validation(self):
        if self.test:
            df_tag = spark.sql(f"SELECT * FROM dmro_raw_tag{self.version} WHERE processed = False")
        else:
            df_tag = spark.sql(f"SELECT * FROM dmro_raw.tag{self.version} WHERE processed = False")

        if df_tag.count != 0:
            exception_df = df_tag.filter(
                (df_tag['operation_code'].isNull() == True) | (df_tag['create_time'].isNull() == True)
            )
            if exception_df.count() > 0:
                reason_df = exception_df.withColumn("reason", lit("operation code or create time is null"))
                return [exception_df, df_tag]
            else:
                return [df_tag]
        else:
            print("No New Records")
            return None


# Example usage:
defect_detection_matcher = DefectDetectionMatcher("your_version", test=True)
result = defect_detection_matcher.check_and_return_result()
inspection_raw_to_curated = InspectionRawToCurated("your_version", test=True)
inspection_result = inspection_raw_to_curated.inspection_validation()

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
def uuid_defect_detection_match(version: str, test: bool=False):

    if test:
        dt = spark.read.table(f"dmro_raw_detection")
        df = spark.read.table(f"dmro_raw_defect")
    else:
        dt = spark.read.table(f"dmro_raw.detection{version}")
        df = spark.read.table(f"dmro_raw.defect{version}") 

    dt.createOrReplaceTempView("dt")
    df.createOrReplaceTempView("df")

    ##to find all non-matching 1:1 uuid in detections and defects file
    df_not_match = spark.sql("""
        select dt.uuid, dt.stream_id, 'detection' as from_table
        from dt
        left outer join df
        on dt.uuid = df.uuid
        where df.stream_id is null and dt.processed is false
        union all
        select dt.uuid, dt.stream_id, 'defect' as from_table
        from df
        left outer join dt 
        on dt.uuid = df.uuid 
        where dt.stream_id is null and df.processed is false
    """)

    # df_not_match.display()

    if df_not_match.count() != 0:
        print("Defect/detection uuid mismatch found.")
        df_not_match = df_not_match.withColumn("reason", lit("the following 1-1 UUID does not match in defect & detections"))

        df_final = {
            "df": df_not_match,
            "table_name": "defect_detection_uuid_match", 
            "data_layer": "exception",
            "merge_schema": False
        }
        return df_final
    
    else:
        print("Defect/detection uuid match validation passed.")


def inspection_rawtocurated(version: str, test: bool=False):

    ###inspection
    def inspection_validation(version, test):
        # v = add_underscore(version)
        
        if test:
            df_tag = spark.sql(f"select * from dmro_raw_tag{version} where processed = False")
        #read tag table and get latest timestamp data
        else:
            df_tag = spark.sql(f"select * from dmro_raw.tag{version} where processed = False")

        if df_tag.count != 0:
            exception_df=df_tag.filter(
                (df_tag['operation_code'].isNull() == True) | (df_tag['create_time'].isNull() == True)
            )
            if exception_df.count() > 0:
                reason_df = exception_df.withColumn("reason", lit("operation code or create time is null"))

                return [exception_df, df_tag]
            
            else:
                return [df_tag]
            
        else:
            print("No New Records")
            return

    def inspection_curated_dataframe(validated_df: DataFrame):

        df_tag = validated_df.withColumn('inspection_date', F.date_format(F.col('create_time'), 'yyyy-MM-dd'))\
                    .withColumn('inspection_time', F.date_format(F.col('create_time'), 'HH:mm:ss'))

        print("print df with columns split")
        df_tag.display()

        #select the necessary fields
        df_tag = df_tag.select(
            'part_number', 
            'serial_number', 
            'automated_disposition', 
            'create_station', 
            'inspection_index', 
            'inspector_disposition', 
            'inspection_facility', 
            'operation_code', 
            'ppi_type', 
            'shop_visit', 
            'stream_id', 
            'inspection_date', 
            'inspection_time'
        )

        #input new current timestamp 
        df_tag = df_tag.withColumn("inspection_index",df_tag.inspection_index.cast(IntegerType())) \
                    .withColumn("operation_code",df_tag.operation_code.cast(IntegerType())) \
                    .withColumn("inspection_date",df_tag.inspection_date.cast(DateType())) \
                    .withColumn("shop_visit",df_tag.shop_visit.cast(IntegerType()))
                    # .withColumn("inspection_time",df_tag.inspection_time.cast('timestamp'))

        df_tag.schema
        df_tag.display()

        df_tag_dict = {
            "df": df_tag,
            "table_name": "inspection", 
            "data_layer": "curated",
            "merge_schema": False
        }

        return df_tag_dict

    insp_validation_dataframes = inspection_validation(version, test)

    if len(insp_validation_dataframes) > 0:
    
        if len(insp_validation_dataframes) > 1:
            exception_df_dict, validated_df_dict = insp_validation_dataframes

        else:
            validated_df_dict = insp_validation_dataframes[0]

        curated_df_dict = inspection_curated_dataframe(validated_df_dict)

        # if exception_df_dict doesn't exist, this line will throw NameError
        try:
            return [exception_df_dict, curated_df_dict]
        
        # if there's a NameError, return only curated_df_dict
        except NameError:
            return [curated_df_dict]
        
    else:
        pass


def defect_rawtocurated(version: str, test: bool=False):

    def defect_validation(version, test):

        #read tables from raw
        if test:
            df_tag = spark.read.table(f"dmro_raw_tag")    
            print("tag from raw")
            df_tag.display()

            df_def = spark.read.table(f"dmro_raw_defect{version}")
            print("defect from raw")
            df_def.display()

            df_ppi = spark.read.table("dmro_curated_ppi_airfoil_region")

        else:
            df_tag = spark.read.table(f"dmro_raw.tag{version}")
            df_def = spark.read.table(f"dmro_raw.defect{version}")
            df_ppi = spark.read.table("dmro_curated.ppi_airfoil_region")

            
        df_tag.createOrReplaceTempView("tag")
        # df_def.createOrReplaceTempView("defect")
        df_ppi.createOrReplaceTempView("ppi")

        df_ppi.display()

        # check for rows with null disposition or indication_type values and return as dataframe in a dictionary
        exception_df = df_def.filter(
            (df_def['disposition'].isNull() == True) | (df_def['`indication_type`'].isNull() == True)
        )

        if exception_df.count() > 0:
            reason_df = exception_df.withColumn("reason", lit("disposition or indication type is null"))
            df_def = df_def.filter(
                (df_def['disposition'].isNotNull()) & (df_def['`indication_type`'].isNotNull())
            )

            return [reason_df, df_def]
        
        else:
            return [df_def]
            
    def defect_curated_dataframe(validated_df: DataFrame):

        validated_df.createOrReplaceTempView("defect")
        # validated_df.display()

        print("joining")

        # sql statement with joins from tag to defect and ppi based on max timestamp
        df = spark.sql("""
            select tg.part_number,
                tg.serial_number,
                df.`Defect_Type` as defect_type,
                df.size,
                df.disposition as defect_disposition,
                df.`Indication_Type` as ppi_region,
                df.`Short_text` as short_text,
                df.x,
                df.y,
                df.z,
                df.`Analytics_Confidence` as analytics_confidence,
                df.angle,
                df.elongation,
                df.uuid,
                df.stream_id,
                pp.airfoil_region
            from tag as tg
            left outer join defect as df
            on df.stream_id = tg.stream_id 
            left outer join ppi as pp
            on pp.PPI_region = rtrim(df.`Indication_Type`) and pp.part_number = tg.part_number
            where tg.processed = False
        """)

        print("defect joined table")
        df.schema
        df.display()

        #input new current timestamp
        if df.count != 0:
            df = df.withColumn("size",df.size.cast(DoubleType()))\
                    .withColumn("x",df.x.cast(DoubleType()))\
                    .withColumn("y",df.y.cast(DoubleType()))\
                    .withColumn("z",df.z.cast(DoubleType()))\
                    .withColumn("analytics_confidence",df.analytics_confidence.cast(DoubleType()))\
                    .withColumn("angle",df.angle.cast(DoubleType()))\
                    .withColumn("elongation",df.elongation.cast(DoubleType()))\

            print("writing defect curated table")
            df.display()

            df_dict = {
                "df": df,
                "table_name": "defect", 
                "data_layer": "curated", 
                "merge_schema": False
            }

            return df_dict
                
        else: 
            print("No New Records")

    def_validation_dataframes = defect_validation(version, test)

    if len(def_validation_dataframes) > 0:
    
        if len(def_validation_dataframes) > 1:
            exception_df_dict, validated_df_dict = def_validation_dataframes

        else:
            validated_df_dict = def_validation_dataframes[0]

        curated_df_dict = defect_curated_dataframe(validated_df_dict)

        # if exception_df_dict doesn't exist, this line will throw NameError
        try:
            return [exception_df_dict, curated_df_dict]
        
        # if there's a NameError, return only curated_df_dict
        except NameError:
            return [curated_df_dict]
        
    else:
        pass

###defect

def defect_rawtocurated(version):
    v = version
    #read tables from raw
    df_tag = spark.read.table(f"dmro_raw.tag{v}")
    # df_tag = spark.sql(f"select * from dmro_raw.tag{v}")
    print("tag from raw")
    df_tag.display()
    df_tag.createOrReplaceTempView("tag")
    df_def = spark.read.table(f"dmro_raw.defect{v}")
    print("defect from raw")
    df_def.display()
    df_ppi = spark.read.table("dmro_curated.ppi_airfoil_region")
    # df_def.createOrReplaceTempView("defect")
    df_ppi.createOrReplaceTempView("ppi")

    
    
    df_ppi.display()

    exception_df=df_def.filter((df_def['disposition'].isNull() == True) | (df_def['`indication type`'].isNull() == True))
    if exception_df.count()>0:
        reason_df = exception_df.withColumn("reason", lit("disposition or indication type is null"))
        reason_df.write.format('delta').mode('append').option('header','true').option("delta.columnMapping.mode", "name").saveAsTable(f"dmro.defect_exception{v}")

    df_def = df_def.filter((df_def['disposition'].isNotNull()) & (df_def['`indication type`'].isNotNull()))
    df_def.createOrReplaceTempView("defect")
    print("here")
    df_def.display()
    # df_def.display()

    print("joining")
    # sql statement with joins from tag to defect and ppi based on max timestamp
    df = spark.sql("""
        select tg.part_number,
            tg.serial_number,
            df.`Defect Type` as defect_type,
            df.size,
            df.disposition as defect_disposition,
            df.`Indication Type` as ppi_region,
            df.`Short text` as short_text,
            df.x,
            df.y,
            df.z,
            df.`Analytics Confidence` as analytics_confidence,
            df.angle,
            df.elongation,
            df.uuid,
            df.stream_id,
            pp.airfoil_region
        from tag as tg
        left outer join defect as df
        on df.stream_id = tg.stream_id 
        left outer join ppi as pp
        on pp.PPI_region = rtrim(df.`Indication Type`) and pp.part_number = tg.part_number
        where tg.processed = False
    """)

    # df = spark.sql("""select tg.part_number, df.`Indication Type` as ppi_region, pp.airfoil_region from tag as tg left outer join defect as df on df.stream_id = tg.stream_id left outer join ppi as pp on pp.PPI_region = df.`Indication Type`""")

    print("defect joined table")
    df.schema
    df.display()

    #input new current timestamp
    if df.count != 0:
        df = df.withColumn("load_timestamp", F.current_timestamp())\
                .withColumn("processed", lit(False))\
                .withColumn("size",df.size.cast(DoubleType()))\
                .withColumn("x",df.x.cast(DoubleType()))\
                .withColumn("y",df.y.cast(DoubleType()))\
                .withColumn("z",df.z.cast(DoubleType()))\
                .withColumn("analytics_confidence",df.analytics_confidence.cast(DoubleType()))\
                .withColumn("angle",df.angle.cast(DoubleType()))\
                .withColumn("elongation",df.elongation.cast(DoubleType()))\

        print("writing defect curated table")
        df.display()

        #write table to curated layer
        def table_exists(path):
            try:
                dbutils.fs.ls(path)
                return True
            except:
                return False
            
        if table_exists(f"dbfs:/mnt/dmro/dmro_curated{v}/defect{v}"):
            df.write.format("delta").mode("append").option("appendSchema", "true").option("delta.columnMapping.mode", "name").saveAsTable(f"dmro_curated.defect{v}")
        else:
            df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(f"dbfs:/mnt/dmro/dmro_curated{v}/defect{v}")
            spark.sql(f"create table if not exists dmro_curated.defect{v} using delta location 'dbfs:/mnt/dmro/dmro_curated{v}/defect{v}'")
            
    else: 
         print("No New Records")   

    return True

def processed_true_raw(version):
    v = version
    ###processed true for raw layer tables/new function
    df_tag_raw = spark.sql(f"select * from dmro_raw.tag{v}")
    df_defect_raw = spark.sql(f"select * from dmro_raw.defect{v}")

    df_updated_tag = df_tag_raw.withColumn("processed", F.when(F.col("processed") == False, True).otherwise(F.col("processed")))
    df_updated_defect = df_defect_raw.withColumn("processed", F.when(F.col("processed") == False, True).otherwise(F.col("processed")))

    # df_updated_tag.display()
    # df_updated_defect.display()

    df_updated_tag.write.format("delta").mode("overwrite").option("overwriteSchema", "true").option("delta.columnMapping.mode", "name").saveAsTable(f"dmro_raw.tag{v}")
    df_updated_defect.write.format("delta").mode("overwrite").option("overwriteSchema", "true").option("delta.columnMapping.mode", "name").saveAsTable(f"dmro_raw.defect{v}")

    return True

#for changing the data type of inspection table of selected columns in curated layer
# def datatype_change_inspection(version):
#     v = version
#     df_ins = spark.read.table(f"dmro_curated.inspection{v}")
#     df_ins.withColumn("inspection_index",df_ins.inspection_index.cast('int'))
#     df_ins.withColumn("operation_code",df_ins.operation_code.cast('int'))
#     df_ins.withColumn("inspection_date",df_ins.inspection_date.cast('date'))
#     df_ins.withColumn("inspection_time",df_ins.inspection_time.cast('time'))

# #for changing the data type of defect table of selected columns in curated layer
# def datatype_change_defect(version):
#     v = version
#     df_df = spark.read.table(f"dmro_curated.defect{v}")
#     df_df.withColumn("x",df_df.x.cast('double'))
#     df_df.withColumn("y",df_df.y.cast('double'))
#     df_df.withColumn("z",df_df.z.cast('double'))
#     df_df.withColumn("analytics_confidence",df_df.analytics_confidence.cast('double'))
#     df_df.withColumn("angle",df_df.angle.cast('double'))
#     df_df.withColumn("elongation",df_df.elongation.cast('double'))


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Documentation for UnitTestFunctions

This documentation provides an overview of the code contained in `UnitTestFunctions` class, explaining its purpose, methods, and usage.

## Purpose
The `UnitTestFunctions` class is designed to perform unit tests for various functions related to processing file paths and data in a Spark environment. The tests cover functions such as fixing image file paths, constructing full file paths, extracting parent directories, and processing boundary data.

## Class Structure
The `UnitTestFunctions` class inherits from `unittest.TestCase`, providing the framework for defining individual test cases. The class contains multiple methods, each representing a test case for a specific function. Below are the key methods:

### `mock_fix_img_file_path`
- Purpose: Mocks input and output dataframes for testing the `fix_img_file_path` function.
- Returns: Input and expected output dataframes.

### `test_fix_img_file_path`
- Purpose: Tests the `fix_img_file_path` function.
- Steps:
  - Mocks input and expected output dataframes.
  - Calls `fix_img_file_path` with input dataframe.
  - Compares the resulting dataframe with the expected output.
- Assertion: Asserts that the resulting dataframe matches the expected output dataframe.

### `test_full_file_path`
- Purpose: Tests the `full_file_path` function.
- Steps:
  - Defines input image and JSON file paths.
  - Calls `full_file_path` with the input paths.
- Assertion: Asserts that the resulting full file path matches the expected output.

### `mock_path_boundary_string_df`
- Purpose: Mocks input and output dataframes for testing the `path_boundary_string_df` function.
- Returns: Input dataframe.

### `test_path_boundary_string_df`
- Purpose: Tests the `path_boundary_string_df` function.
- Steps:
  - Mocks input dataframe.
  - Calls `path_boundary_string_df` with the input dataframe.
- Assertion: Asserts that the resulting dataframe matches the expected output.

### `mock_path_boundary_array_df`
- Purpose: Mocks input and output dataframes for testing the `path_boundary_array_df` function.
- Returns: Input and expected output dataframes.

### `test_path_boundary_array_df`
- Purpose: Tests the `path_boundary_array_df` function.
- Steps:
  - Mocks input and expected output dataframes.
  - Calls `path_boundary_array_df` with the input dataframe.
- Assertion: Asserts that the resulting dataframe matches the expected output.

### `test_extract_parent_directory`
- Purpose: Tests the `extract_parent_directory` function.
- Steps:
  - Defines an input file path.
  - Calls `extract_parent_directory` with the input path.
- Assertion: Asserts that the resulting parent directory matches the expected output.

### `mock_path_image_boundary_df`
- Purpose: Mocks input dataframes for testing the `path_image_boundary_df` function.
- Returns: Dataframes containing boundary, image, and mapping data.

### `test_path_image_boundary_df`
- Purpose: Tests the `path_image_boundary_df` function.
- Steps:
  - Mocks input dataframes.
  - Calls `path_image_boundary_df` with the input dataframes.
- Assertion: Asserts that the resulting dataframe matches the expected output.

## Running Tests
To execute the tests, run the script containing the `UnitTestFunctions` class with the `unittest.main()` function. The script will run all test cases and provide detailed output regarding the success or failure of each test.

Example:
```bash
python <script_name>.py

----------------------------------------------------------------------------
Documentation for UnitTestFunctions

This documentation provides an overview of the code contained in the UnitTestFunctions class, explaining its purpose, methods, and usage.

Purpose
The UnitTestFunctions class is designed to perform unit tests for various functions related to processing file paths and data in a Spark environment. The tests cover functions such as fixing image file paths, constructing full file paths, extracting parent directories, and processing boundary data.

Class Structure
The UnitTestFunctions class inherits from unittest.TestCase, providing the framework for defining individual test cases. The class contains multiple methods, each representing a test case for a specific function. Below are the key methods:

mock_fix_img_file_path
Purpose: Mocks input and output dataframes for testing the fix_img_file_path function.
Returns: Input and expected output dataframes.

test_fix_img_file_path
Purpose: Tests the fix_img_file_path function.
Steps:
Mocks input and expected output dataframes.
Calls fix_img_file_path with input dataframe.
Compares the resulting dataframe with the expected output.
Assertion: Asserts that the resulting dataframe matches the expected output dataframe.

test_full_file_path
Purpose: Tests the full_file_path function.
Steps:
Defines input image and JSON file paths.
Calls full_file_path with the input paths.
Assertion: Asserts that the resulting full file path matches the expected output.

mock_path_boundary_string_df
Purpose: Mocks input and output dataframes for testing the path_boundary_string_df function.
Returns: Input dataframe.

test_path_boundary_string_df
Purpose: Tests the path_boundary_string_df function.
Steps:
Mocks input dataframe.
Calls path_boundary_string_df with the input dataframe.
Assertion: Asserts that the resulting dataframe matches the expected output.

mock_path_boundary_array_df
Purpose: Mocks input and output dataframes for testing the path_boundary_array_df function.
Returns: Input and expected output dataframes.

test_path_boundary_array_df
Purpose: Tests the path_boundary_array_df function.
Steps:
Mocks input and expected output dataframes.
Calls path_boundary_array_df with the input dataframe.
Assertion: Asserts that the resulting dataframe matches the expected output.

test_extract_parent_directory
Purpose: Tests the extract_parent_directory function.
Steps:
Defines an input file path.
Calls extract_parent_directory with the input path.
Assertion: Asserts that the resulting parent directory matches the expected output.

mock_path_image_boundary_df
Purpose: Mocks input dataframes for testing the path_image_boundary_df function.
Returns: Dataframes containing boundary, image, and mapping data.

test_path_image_boundary_df
Purpose: Tests the path_image_boundary_df function.
Steps:
Mocks input dataframes.
Calls path_image_boundary_df with the input dataframes.
Assertion: Asserts that the resulting dataframe matches the expected output.

Running Tests
To execute the tests, run the script containing the UnitTestFunctions class with the unittest.main() function. The script will run all test cases and provide detailed output regarding the success or failure of each test.
-----------------------------------------------------------------------------
import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col,udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType,FloatType,DoubleType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp
class UnitTestFunctions(unittest.TestCase):
    test=True
    version=''
    def mock_fix_img_file_path(self):
        schema_input=StructType([
                StructField('path',StringType(),True)
            ])
        mock_data_input={'path':r'["Pose_811_FrontForm\\instant_11\\11.png"]'}
        df_input=spark.createDataFrame([mock_data_input],schema=schema_input)
        schema_output=StructType([
               StructField('img_file_path',StringType(),True)
            ])
        mock_data_output={'img_file_path':'Pose_811_FrontForm/instant_11/11.png'}
        df_output=spark.createDataFrame([mock_data_output],schema=schema_output)
        return df_input,df_output

    def test_fix_img_file_path(self):
        input_df,expected_df=self.mock_fix_img_file_path()
        # expected_df.display()
        output_df=fix_img_file_path(input_df)
        # output_df.display()
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

    #####################################

    def test_full_file_path(self):
        inp_img_path = "Pose_811_FrontForm/instant_11/11.png"
        inp_json_path = "/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/RIERX72599_mapping.json"
        output_path = "/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_811_FrontForm/instant_11/11.png"
        expected_path=full_file_path(inp_img_path,inp_json_path)
        self.assertEqual(output_path,expected_path)

    #########################################
    #path_boundary_string_df
    def mock_path_boundary_string_df(self):
        schema_inp=StructType([
            StructField('detections',StringType(),True),
            StructField('file_path',StringType(),True)
        ])
        mock_data_input={
            'detections':'[{"boundary":[[123.0,210.0],[101.0,392.0],[152.0,283.0]],"confidence":0.73223,"uuid":"f123g1h0-1234-5679-i1il-123dc4b5678","damage_type":0}, {"boundary":[[103.0,215.0],[125.0,202.0],[130.0,231.0]],"confidence":0.50231,"uuid":"b890f8g9-1234-5678-p9pn-097mn4oo830","damage_type":0}]',
            'file_path':'/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/instant_19.json'
        }
        df_input=spark.createDataFrame([mock_data_input],schema=schema_inp)
        #i need the datatypes of the dataframe which is given as input and as well as output
        schema_output=StructType([
            StructField('file_path',StringType(),True),
            StructField('boundary',StringType(),True)
        ])
        mock_data_output={
            'file_path':'/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/instant_19.json',
            'boundary': '[[123.0,210.0],[101.0,392.0],[152.0,283.0]]'
        }
        df_output=spark.createDataFrame([mock_data_output],schema=schema_output)

        df_input.createOrReplaceTempView("dmro_raw_instant")
        return df_output


    def test_path_boundary_string_df(self):
        expected_df=self.mock_path_boundary_string_df()
        # expected_df.display()
        output_df=path_boundary_string_df("", test=True)
        # output_df.display()
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

    #################################
    #path_boundary_array_df
    def mock_path_boundary_array_df(self):
        schema_inp=StructType([
            StructField('boundary',StringType(),True),
            StructField('file_path',StringType(),True)
        ])
        mock_data_input={
            'boundary':'[[123.0,210.0],[101.0,392.0],[152.0,283.0]]',
            'file_path':'/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/instant_19.json'
        }
        df_input=spark.createDataFrame([mock_data_input],schema=schema_inp)
        #i need the datatypes of the dataframe which is given as input and as well as output
        schema_output=StructType([
            StructField('boundary',ArrayType(ArrayType(DoubleType())),True),
            StructField('file_path',StringType(),True)
        ])
        mock_data_output={
            'boundary':[[123.0,210.0],[101.0,392.0],[152.0,283.0]],
            'file_path':'/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/instant_19.json'
        }
        df_output=spark.createDataFrame([mock_data_output],schema=schema_output)

        return df_input,df_output

    
    def test_path_boundary_array_df(self):
        input_df,expected_df=self.mock_path_boundary_array_df()
        # expected_df.display()
        output_df=path_boundary_array_df(input_df)
        # output_df.display()
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

    ############################################
    ##extract parent directory

    def test_extract_parent_directory(self):
        input_str='/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/instant_19.json'
        output_str='/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/'
        expected_str=extract_parent_directory(input_str)
        self.assertEqual(output_str,expected_str)

    ###############################
    #path_image_boundary
    def mock_path_image_boundary_df(self):
        schema_boundary=StructType([
            StructField('filepath',StringType(),True),
            StructField('boundary',StringType(),True)
        ])
        mock_df_boundary={
            'file_path':'/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/instant_19.json',
            'boundary':'[[123.0,210.0],[101.0,392.0],[152.0,283.0]]'
        }
        df_boundary=spark.createDataFrame([mock_df_boundary],schema=schema_boundary)

        schema_mapping=StructType([
            StructField('uuid',StringType(),True),
            StructField('img_file_path',StringType(),True)
        ])
        mock_df_mapping={
            'uuid':'asdf878m-df87-743h-a5ea-5nd8cds89du3',
            'img_file_path':'/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/19.png'
        }
        df_mapping=spark.createDataFrame([mock_df_mapping],schema=schema_mapping)

        schema_img=StructType([
            StructField('content',StringType(),True),
            StructField('path',StringType(),True)
        ])
        mock_df_img={
            'content':'image1',
            'path':'/mnt/dmro/ppi_scene_files/RIERX72599-2-Initial-1/Pose_611_FrontEdge/instant_19/19.png'
        }
        df_image=spark.createDataFrame([mock_df_img],schema=schema_img)

        schema_dmro=StructType([
            StructField('uuid',StringType(),True),
            StructField('stream_id',StringType(),True)
        ])
        mock_df_dmro={
            'uuid':'asdf878m-df87-743h-a5ea-5nd8cds89du3',
            'stream_id':'20230704-140841_arvv9809-17f0-4392-o89i-dnu854jdu7uhh'
        }
        df_dmro_raw_defect=spark.createDataFrame([mock_df_dmro],schema=schema_dmro)
        df_dmro_raw_defect.createOrReplaceTempView('dmro_raw_defect')

        return df_boundary,df_image,df_mapping


    def test_path_image_boundary_df(self):
        input_df,output_df=self.mock_path_boundary_array_df()
        output_df.display()
        expected_df=path_boundary_array_df(input_df)
        expected_df.display()
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)



-----------------------------------------------------------------------------
##inspection raw to curated
    def mock_data_inspection_rawtocurated(self):
        #input data
        input_data={
            'part_number':['part1111','part2222','part3333'],
            'serial_number':['serial111','serial222','serial333'],
            'automated_disposition':['Pass','Fail','Pass'],
            'create_station':['StationA','StationB','StationC'],
            'inspection_index':[1,2,3],
            'inspector_disposition':['Accept','Reject','Accept'],
            'inspection_facility':['Facility1','Facility2','Facility3'],
            'operation_code':[101,None,103],
            'ppi_type':['PPI','PPI','PPI'],
            'shop_visit':[1,0,1],
            'stream_id':['streamid_11','streamid_22','streamid_33'],
            'create_time':['2023-06-06T01:01:01.581+0000','2023-07-07T02:02:02.581+0000','2023-08-08T03:03:03.581+0000'],
            'processed':[False,False,False]
            }
        schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('create_time', StringType(), True),
            StructField('processed',BooleanType(),True)
        ])
        rows_inp = [
            {'part_number': pn, 'serial_number':sn ,'automated_disposition':ad, 'create_station': cs,'inspection_index':ii,
             'inspector_disposition':ind,'inspection_facility':insf,'operation_code':oc,'ppi_type':pp,'shop_visit':sv,'stream_id':si,
             'create_time':ct,'processed':pr}
            for pn, sn, ad, cs, ii,ind,insf ,oc,pp,sv,si,ct,pr in zip(
                input_data['part_number'],
                input_data['serial_number'],
                input_data['automated_disposition'],
                input_data['create_station'],
                input_data['inspection_index'],
                input_data['inspector_disposition'],
                input_data['inspection_facility'],
                input_data['operation_code'],
                input_data['ppi_type'],
                input_data['shop_visit'],
                input_data['stream_id'],
                input_data['create_time'],
                input_data['processed']
            )
        ]
        mock_data_input = spark.createDataFrame(rows_inp, schema=schema)
        mock_data_input.createOrReplaceTempView("dmro_raw_tag")
        expected_data={
            'part_number':['part1111','part2222','part3333'],
            'serial_number':['serial111','serial222','serial333'],
            'automated_disposition':['Pass','Fail','Pass'],
            'create_station':['StationA','StationB','StationC'],
            'inspection_index':[1,2,3],
            'inspector_disposition':['Accept','Reject','Accept'],
            'inspection_facility':['Facility1','Facility2','Facility3'],
            'operation_code':[101,None,103],
            'ppi_type':['PPI','PPI','PPI'],
            'shop_visit':[1,0,1],
            'stream_id':['streamid_11','streamid_22','streamid_33'],
            'inspection_date':['2023-06-06','2023-07-07','2023-08-08'],
            'inspection_time':['01:01:01','02:02:02','03:03:03'],
            'processed':[False,False,False]
            }

        expected_schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('inspection_date', StringType(), True),
            StructField('inspection_time', StringType(), True),
            StructField('processed', BooleanType(), False)
        ])

        rows_exp=[
            {'part_number': pn, 'serial_number':sn ,'automated_disposition':ad, 'create_station': cs,'inspection_index':ii,'inspector_disposition':ind,'inspection_facility':insf,'operation_code':oc,'ppi_type':pp,'shop_visit':sv,'stream_id':si,'inspection_date':idt,'inspection_time':it,'processed':pr}
            for pn, sn, ad, cs, ii,ind,insf ,oc,pp,sv,si,idt,it,pr in zip(
                expected_data['part_number'],
                expected_data['serial_number'],
                expected_data['automated_disposition'],
                expected_data['create_station'],
                expected_data['inspection_index'],
                expected_data['inspector_disposition'],
                expected_data['inspection_facility'],
                expected_data['operation_code'],
                expected_data['ppi_type'],
                expected_data['shop_visit'],
                expected_data['stream_id'],
                expected_data['inspection_date'],
                expected_data['inspection_time'],
                expected_data['processed']
            )
        ]
        expected_data_df = spark.createDataFrame(rows_exp, schema=expected_schema)
        expected_data_df = expected_data_df.withColumn('inspection_date',F.to_date('inspection_date','yyyy-MM-dd'))
        return expected_data_df

    def inspection_rawtocurated(version):
        # v = add_underscore(version)
            
        #read tag table and get latest timestamp data
        # df_tag = spark.sql("select * from dmro_raw.tag where load_timestamp = (select max(load_timestamp) from dmro_raw.tag)")
        df_tag = spark.sql(f"select * from dmro_raw.tag{version} where processed = False")

        if df_tag.count != 0:
            exception_df=df_tag.filter((df_tag['operation_code'].isNull() == True) | (df_tag['create_time'].isNull() == True))
            if exception_df.count()>0:
                reason_df = exception_df.withColumn("reason", lit("operation code or create time is null"))
                # reason_df.write.format('delta') \
                #     .mode('append') \
                #     .option('header','true') \
                #     .option("delta.columnMapping.mode", "name") \
                #     .saveAsTable(f"dmro_exception.tag_exception{version}")

                #### ORIGINALLY INCLUDED
                # save_table(
                #     df=reason_df,
                #     table_name="tag_exception",
                #     data_layer="exception",
                #     version=v,
                #     header=True
                # )

                yield {
                    "df": reason_df,
                    "table_name": "tag_exception", 
                    "data_layer": "exception",
                    # "header": True
                    "merge_schema": False
                }

            #create new columns to split up time
            df_tag = df_tag.withColumn('inspection_date', F.date_format(F.col('create_time'), 'yyyy-MM-dd'))\
                    .withColumn('inspection_time', F.date_format(F.col('create_time'), 'HH:mm:ss'))

            print("print df with columns split")
            df_tag.display()

            #select the necessary fields
            df_tag = df_tag.select(
                'part_number', 
                'serial_number', 
                'automated_disposition', 
                'create_station', 
                'inspection_index', 
                'inspector_disposition', 
                'inspection_facility', 
                'operation_code', 
                'ppi_type', 
                'shop_visit', 
                'stream_id', 
                'inspection_date', 
                'inspection_time'
            )

            #input new current timestamp 
            df_tag = df_tag.withColumn("inspection_index",df_tag.inspection_index.cast(IntegerType())) \
                        .withColumn("operation_code",df_tag.operation_code.cast(IntegerType())) \
                        .withColumn("inspection_date",df_tag.inspection_date.cast(DateType())) \
                        .withColumn("shop_visit",df_tag.shop_visit.cast(IntegerType()))
                        # .withColumn("inspection_time",df_tag.inspection_time.cast('timestamp'))

            df_tag.schema
            df_tag.display()

            #### ORIGINALLY INCLUDED
            # save_table(
            #     df=df_tag,
            #     table_name="inspection",
            #     data_layer="curated",
            #     version=v
            # )

            yield {
                "df": df_tag,
                "table_name": "inspection", 
                "data_layer": "curated",
                "merge_schema": False
            }

        else:
            print("No New Records")
        
        #### return True

    def test_inspection_rawtocurated(self):
        expected_df = self.mock_data_inspection_rawtocurated()
        # output_df= self.inspection_rawtocurated()
        temp_df = self.inspection_rawtocurated()
        temp1_df=next(temp_df)
        output_df=final_df['df']
        #display(output_df)
        # print('exp')
        # display(expected_df)
        # print(expected_df.schema)
        #assertion statement for load_timestamp
        load_timestamp_datatype= output_df.schema['load_timestamp'].dataType
        # print('output')
        
        # print(output_df['load_timestamp'].schema)
        self.assertIsInstance(load_timestamp_datatype,(TimestampType,))
        #drop load time stamp
        output_df= output_df.drop('load_timestamp')
        # display(output_df)
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))
    

###inspection
def inspection_validation(version):
    # v = add_underscore(version)
        
    #read tag table and get latest timestamp data
    if test_inspection_rawtocrated()
    df_tag = spark.sql(f"select * from dmro_raw.tag{version} where processed = False")

    if df_tag.count != 0:
        exception_df=df_tag.filter(
            (df_tag['operation_code'].isNull() == True) | (df_tag['create_time'].isNull() == True)
        )
        if exception_df.count()>0:
            reason_df = exception_df.withColumn("reason", lit("operation code or create time is null"))

            return [exception_df, df_tag]
        
        else:
            return [df_tag]
        
    else:
        print("No New Records")
        return

def inspection_curated_dataframe(validated_df: DataFrame):

    df_tag = validated_df.withColumn('inspection_date', F.date_format(F.col('create_time'), 'yyyy-MM-dd'))\
                .withColumn('inspection_time', F.date_format(F.col('create_time'), 'HH:mm:ss'))

    print("print df with columns split")
    df_tag.display()

    #select the necessary fields
    df_tag = df_tag.select(
        'part_number', 
        'serial_number', 
        'automated_disposition', 
        'create_station', 
        'inspection_index', 
        'inspector_disposition', 
        'inspection_facility', 
        'operation_code', 
        'ppi_type', 
        'shop_visit', 
        'stream_id', 
        'inspection_date', 
        'inspection_time'
    )

    #input new current timestamp 
    df_tag = df_tag.withColumn("inspection_index",df_tag.inspection_index.cast(IntegerType())) \
                .withColumn("operation_code",df_tag.operation_code.cast(IntegerType())) \
                .withColumn("inspection_date",df_tag.inspection_date.cast(DateType())) \
                .withColumn("shop_visit",df_tag.shop_visit.cast(IntegerType()))
                # .withColumn("inspection_time",df_tag.inspection_time.cast('timestamp'))

    df_tag.schema
    df_tag.display()

    df_tag_dict = {
        "df": df_tag,
        "table_name": "inspection", 
        "data_layer": "curated",
        "merge_schema": False
    }

    return df_tag_dict


def inspection_rawtocurated(version):
    insp_validation_dataframes = inspection_validation(version)
    
    if len(insp_validation_dataframes) > 1:
        exception_df_dict, validated_df_dict = insp_validation_dataframes

    else:
        validated_df_dict = insp_validation_dataframes[0]

    curated_df_dict = inspection_curated_dataframe(validated_df_dict)

    try:
        return [exception_df_dict, curated_df_dict]
    
    except NameError:
        return [curated_df_dict]
##################################################
import warnings
import unittest
from datetime import datetime

from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DateType

warnings.filterwarnings("ignore")

class UnitTestFunctions(unittest.TestCase):

    def mock_weight_gain_rawtocurated(self):
        wt_schema = StructType([
            StructField('serial_number', StringType(), True),
            StructField('coating_date', StringType(), True),
            StructField('coater', StringType(), True),
            StructField('Location', StringType(), True),
            StructField('Gain', StringType(), True),
            StructField('load_timestamp', TimestampType(), True)
        ])

        dictionary_wt = {
            'serial_number': ['SN111', 'SN222', 'SN333'],
            'coating_date': ['01/01/2021', '02/02/2022', '03/03/2023'],
            'coater': ['Coater1', 'Coater2', 'Coater3'],
            'Location': ['Location1', 'Location2', 'LocationEH'],
            'Gain': ['Gain1', 'Gain2', 'Gain3'],
            'load_timestamp': ['2020-01-01 12:01:19.000', '2020-02-02 12:01:19.000', '2020-03-03 12:01:19.000']
        }

        input_df = spark.createDataFrame(dictionary_wt, schema=wt_schema).withColumn('load_timestamp', F.to_timestamp('load_timestamp'))

        input_df.createOrReplaceTempView('dmro_raw.weight_gain')

        exp_schema = StructType([
            StructField('serial_number', StringType(), True),
            StructField('coating_date', DateType(), True),
            StructField('coater', StringType(), True),
            StructField('Gain', StringType(), True)
        ])

        dictionary_exp = {
            'serial_number': 'SN333',
            'coating_date': datetime.strptime('03/03/2023', '%d/%m/%Y').date(),
            'coater': 'Coater3',
            'Gain': 'Gain3'
        }

        mock_data_expected = spark.createDataFrame([dictionary_exp], schema=exp_schema).withColumn('load_timestamp', F.to_timestamp('load_timestamp'))

        return mock_data_expected

    def weight_gain_rawtocurated(self):
        df_wg = spark.sql("""
            SELECT *
            FROM dmro_raw.weight_gain
            WHERE load_timestamp = (
                SELECT MAX(load_timestamp)
                FROM dmro_raw.weight_gain
            )
        """)

        df_wg = df_wg.withColumn('coating_date', F.to_date('coating_date', 'M/d/yyyy')) \
            .withColumn(
                'coating_vendor',
                F.when(
                    F.col('Location') == 'EH', 'PWEH'
                ).otherwise(F.col('Location'))) \
            .withColumnRenamed('Gain', 'coating_gain') \
            .drop('load_timestamp') \
            .drop('Location')

        df_wg = df_wg.withColumn('load_timestamp', F.to_timestamp('load_timestamp'))

        return df_wg

    def test_weight_gain_rawtocurated(self):
        expected_df = self.mock_weight_gain_rawtocurated()
        output_df = self.weight_gain_rawtocurated()
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)
...

[Message clipped]  View entire message
##############

import unittest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType, DateType, TimestampType

class UnitTestFunctions(unittest.TestCase):

    @staticmethod
    def mock_inspection_curatedtopublish():
        mock_data_dict = {
            'system_insp_id': 1,
            'part_number': 'ABC123',
            'serial_number': 'SN123',
            'automated_disposition': 'Pass',
            'inspection_date': '2022-01-01',
            'shop_visit': 2,
            'create_station': 'Station1',
            'inspection_index': 42,
            'ESN': 'ESN123',
            'engine_flight_hours': 500,
            'engine_cycles': 50,
            'engine_operator': 'Operator1',
            'casting_vendor': 'Vendor1',
            'coating_vendor': 'Vendor2',
            'coating_gain': 'High',
            'hole_drill_vendor': 'Vendor3',
            'airflow_4800': 78.5,
            'airflow_4802': 82.3,
            'stream_id': 'Stream123',
            'load_timestamp': '2022-01-01 12:34:56'
        }

        return Row(**mock_data_dict)

    def inspection_curatedtopublish():
        # Your existing code here

    def test_inspection_curatedtopublish(self):
        spark = SparkSession.builder.getOrCreate()

        mock_row = self.mock_inspection_curatedtopublish()
        schema = StructType([
            StructField('system_insp_id', LongType(), True),
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('inspection_date', DateType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('ESN', StringType(), True),
            StructField('engine_flight_hours', IntegerType(), True),
            StructField('engine_cycles', IntegerType(), True),
            StructField('engine_operator', StringType(), True),
            StructField('casting_vendor', StringType(), True),
            StructField('coating_vendor', StringType(), True),
            StructField('coating_gain', StringType(), True),
            StructField('hole_drill_vendor', StringType(), True),
            StructField('airflow_4800', DoubleType(), True),
            StructField('airflow_4802', DoubleType(), True),
            StructField('stream_id', StringType(), True),
            StructField('load_timestamp', TimestampType(), True)
        ])

        mock_df = spark.createDataFrame([mock_row], schema=schema)

        # Continue with your testing logic using mock_df
        # The rest of your test code

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)

#########################
import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col,udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType,FloatType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp

warnings.filterwarnings("ignore")



class UnitTestFunctions(unittest.TestCase):
    def mock_inspection_curatedtopublish():
        mock_data_dictionary=
        return expected_df
    def inspection_curatedtopublish():
        #print("writing inspection publish layer")
        #v = add_underscore(version)

        df_inspection = spark.sql("""select 
            ins.part_number,
            ins.serial_number,
            ins.automated_disposition,
            ins.inspection_date,
            ins.create_station,
            ins.shop_visit,
            ins.stream_id,
            ins.inspection_index,
            current_timestamp() as load_timestamp,
            cast.casting_vendor,
            eng.ESN,
            eng.engine_flight_hours,
            eng.engine_cycles,
            eng.engine_operator,
            ct.coating_vendor,
            ct.coating_gain,
            hol.hole_drill_vendor,
            ar.`4800` as airflow_4800,
            ar.`4802` as airflow_4802
        from inspection as ins
        left join 
        casting as cast 
        on ins.serial_number=cast.serial_number
        left join
        engine as eng
        on eng.serial_number=cast.serial_number
        left join 
        coat as ct
        on ct.serial_number=eng.serial_number
        left join
        hole as hol
        on hol.serial_number =ct.serial_number
        left join
        airflow_group as ar
        on ct.serial_number=ar.serial_number
        where ins.processed = False
        """)

        # df = spark.sql("""SELECT * FROM inspection""")
        if df_inspection.count() != 0:
            df_inspect = df_inspection.withColumn("system_insp_id", F.monotonically_increasing_id())

            #reorder table
            df_insp = df_inspect.select('system_insp_id', 'part_number', 'serial_number', 'automated_disposition', 'inspection_date','shop_visit', 'create_station', 'inspection_index', 'ESN', 'engine_flight_hours', 'engine_cycles', 'engine_operator','casting_vendor', 'coating_vendor', 'coating_gain', 'hole_drill_vendor', 'airflow_4800', 'airflow_4802', 'stream_id', 'load_timestamp')

            df_insp.schema
            df_insp.display()
            
        return df_inspection

    def test_inspection_curatedtopublish(self):
        inp_data = self.inspection_curatedtopublish()
        #display(inp_data)
        #output_df = self.reorder_columns(inp_data)
        #display(output_df)
        #self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))
    
    
    
if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)

################
col_name	data_type
system_insp_id	bigint
part_number	string
serial_number	string
automated_disposition	string
inspection_date	date
shop_visit	int
create_station	string
inspection_index	int
ESN	string
engine_flight_hours	int
engine_cycles	int
engine_operator	string
casting_vendor	string
coating_vendor	string
coating_gain	string
hole_drill_vendor	string
airflow_4800	double
airflow_4802	double
stream_id	string
load_timestamp	timestamp

################
import warnings
import unittest
from pyspark.sql.types import StructType, StringType
from pyspark.sql.functions import col

warnings.filterwarnings("ignore")

class UnitTestFunctions(unittest.TestCase):

    def mock_reorder_columns(self):
        schema = StructType([StructField("reason", StringType()), StructField("value", StringType())])
        mock_data_dictionary = [{'reason': 'abc', 'value': 'def'}]
        exp_data_dictionary = [{'reason': 'abc', 'value1': 'def'}]
        mock_data_df = spark.createDataFrame(mock_data_dictionary, schema=schema)
        exp_data_df = spark.createDataFrame(exp_data_dictionary)
        return mock_data_df, exp_data_df

    def reorder_columns(self, my_df):
        """reorders the columns in the exception tables so that the exception reason comes first"""
        columns = my_df.columns
        columns.remove("reason")

        # Some column names contain a period, which is a reserved character.
        # Adding back ticks prevents an error.
        columns = [f"`{col}`" for col in columns]
        df = my_df.select("reason", *columns)
        return df

    def test_reorder_columns(self):
        inp_data, expected_df = self.mock_reorder_columns()
        output_df = self.reorder_columns(inp_data)
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)

###################
import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col,udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType,FloatType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp

warnings.filterwarnings("ignore")
class UnitTestFunctions(unittest.TestCase):
    
    def mock_reorder_columns(self):
        schema=StructType
        mock_data_dictionary=[{'reason':'abc','value':'def'}]
        exp_data_dictionary=[{'value1':'def','reason':'abc'}]
        mock_data_df=spark.createDataFrame(mock_data_dictionary)
        #display(mock_data_df)
        exp_data_df=spark.createDataFrame(exp_data_dictionary)
        display(exp_data_df)
        return mock_data_df,exp_data_df
    
    def reorder_columns(my_df):
        """reorders the columns in the exception tables so that the exception reason comes first"""
        columns = my_df.columns
        columns.remove("reason")

        # Some column names contain a period, which is a reserved character. Adding back ticks prevents an error.
        columns = [f"`{col}`" for col in columns]
        df = my_df.select("reason", *columns)
        return df
    
    def test_reorder_columns(self):
        inp_data,expected_df = self.mock_reorder_columns()
        output_df = self.reorder_columns(inp_data)
        #display(output_df)
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))
        
if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)
################################
import warnings
import unittest
import re
from pyspark.sql import functions as F
from pyspark.sql.functions import col,udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType,FloatType
from pyspark.sql.window import Window
from pyspark.sql.functions import to_timestamp

warnings.filterwarnings("ignore")



class UnitTestFunctions(unittest.TestCase):
    def mock_data_extract_stream_id(self):
        #need to know the input and output pattern
        file_path_str_inp=r'C:\Users\pabitrakumar.pradhan\OneDrive - HCL Technologies Ltd\DMRO\Productivity'
        file_path_str_exp=r'C:\Users\pabitrakumar.p'
        return file_path_str_inp,file_path_str_exp
    
    def extract_stream_id(file_path: str):
        """extract stream id from file path by finding 52-character string made of digits, letters, underscores, or hyphens"""
        regex = r'/(?P<stream_id>[\w\-]{52})_'
        match = re.search(regex, str(file_path))
        return match.group('stream_id')
    # to use extract_stream_id on the DataFrames, it has to be in a udf
    # stream_id_udf = F.udf(lambda x: extract_stream_id(x),StringType())
        
    def test_extract_stream_id(self):
        inp_data,expected_df = self.mock_data_extract_stream_id()
        output_df = self.extract_stream_id(inp_data)
        display(output_df)
        #self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)
#################################3333333333333333333
import warnings
import unittest
import re
import pandas
from pyspark.sql import functions  as F
from pyspark.sql.functions import col,udf,lit,to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType, DoubleType, BooleanType, TimestampType, DateType
from pyspark.sql.window import Window

warnings.filterwarnings("ignore")

class UnitTestFunctions(unittest.TestCase):
    
    ###defect
    def mock_data_defect_rawtocurated(self):
        #read tables from raw
        ##mock data for tag table
        #columns needed from tag table will create mock data according to that
        tag_dictionary={
            'part_number':['part1111','part2222','part3333'],
            'serial_number':['serial111','serial222','serial333'],
            'stream_id':['stream1111','stream2222','stream3333'],
            'processed':[True,False,False]
        }
        tag_schema=StructType([
            StructField('part_number',StringType(),True),
            StructField('serial_number',StringType(),True),
            StructField('stream_id',StringType(),True),
            StructField('processed',BooleanType(),True)
        ])
        rows_tag=[
            {'part_number':pn,'serial_number':sn,'stream_id':si,'processed':pr}
            for pn,sn,si,pr in zip(
                tag_dictionary['part_number'],
                tag_dictionary['serial_number'],
                tag_dictionary['stream_id'],
                tag_dictionary['processed']
            )
        ]
        mock_data_tag = spark.createDataFrame(rows_tag,tag_schema)
        mock_data_tag.createOrReplaceTempView("dmro_raw_tag")

        ##mock data for defect table
        defect_dictionary={
            'Defect_Type':['defect1','defect2','defect3'],
            'size':[1.0,2.0,3.0],
            'disposition':['disp1','disp2','disp3','disp4'],
            'Indication_Type':['ind1','ind2','ind3'],
            'Short_text':['stext_1','stext_2','stext_3'],
            'x':[1.1,1.2,1.3],
            'y':[2.1,2.2,2.3],
            'z':[3.1,3.2,3.3],
            'Analytics_Confidence':[4.1,4.2,4.3],
            'angle':[9.1,9.2,9.3],
            'elongation':[5.1,5.2,5.3],
            'uuid':['uuid1111','uuid2222','uuid3333'],
            'stream_id':['stream_11','stream_22','stream_33']
            }
        defect_schema=StructType([
            StructField('Defect_Type',StringType(),True),
            StructField('size',DoubleType(),True),
            StructField('disposition',StringType(),True),
            StructField('Indication_Type',StringType(),True),
            StructField('Short_text',StringType(),True),
            StructField('x',DoubleType(),True),
            StructField('y',DoubleType(),True),
            StructField('z',DoubleType(),True),
            StructField('Analytics_Confidence',DoubleType(),True),
            StructField('angle',StringType(),True),
            StructField('elongation',DoubleType(),True),
            StructField('uuid',StringType(),True),
            StructField('stream_id',StringType(),True)
        ])
        rows_defect = [
            {'Defect_Type': dt, 'size':s ,'disposition':dis, 'Indication_Type': it,'Short_text':st,'x':x,'y':y,'z':z,'Analytics_Confidence':ac,
             'angle':an,'elongation':si,'uuid':ct,'stream_id':si}
            for dt, s, dis,it,st,x,y,z,ac,an,si,ct,si in zip(
                defect_dictionary['Defect_Type'],
                defect_dictionary['size'],
                defect_dictionary['disposition'],
                defect_dictionary['Indication_Type'],
                defect_dictionary['Short_text'],
                defect_dictionary['x'],
                defect_dictionary['y'],
                defect_dictionary['z'],
                defect_dictionary['Analytics_Confidence'],
                defect_dictionary['angle'],
                defect_dictionary['elongation'],
                defect_dictionary['uuid'],
                defect_dictionary['stream_id']
            )
        ]
        mock_data_defect = spark.createDataFrame(rows_defect,defect_schema)
        mock_data_defect.createOrReplaceTempView("dmro_raw_defect")

        ##mock data for PPI table
        ppi_dictionary ={
            'part_number':['part1111','part2222','part3333'],
            'airfoil_region':['airfoil1','airfoil2','airfoil3'],
            'PPI_region':['ind1','ind2','ind3']
        }
        ppi_schema=StructType([
            StructField('part_number',StringType(),True),
            StructField('airfoil_region',StringType(),True),
            StructField('PPI_region',StringType(),True)
        ])
        rows_ppi=[
            {'part_number':pn,'airfoil_region':ar,'PPI_region':pp}
            for pn,ar,pp in zip(
                ppi_dictionary['part_number'],
                ppi_dictionary['airfoil_region'],
                ppi_dictionary['PPI_region']
            )
        ]
        mock_data_ppi = spark.createDataFrame(rows_ppi,ppi_schema)
        mock_data_ppi.createOrReplaceTempView("dmro_curated_ppi_airfoil_region")
        #mock data for expected data
        expected_dictionary={
            'part_number':['part1111','part2222','part3333'],
            'serial_number':['serial111','serial222','serial333'],
            'Defect_Type':['defect1','defect2','defect3'],
            'size':[1.1,2.1,3.1],
            'disposition':['disp1','disp2','disp3','disp4'],
            'Indication_Type':['ind1','ind2','ind3'],
            'Short_text':['stext1','stext2','stext3'],
            'x':[1.1,1.2,1.3],
            'y':[2.1,2.2,2.3],
            'z':[3.1,3.2,3.3],
            'Analytics_Confidence':[4.1,4.2,4.3],
            'angle':[9.1,9.2,9.3],
            'elongation':[5.1,5.2,5.3],
            'uuid':['uuid1111','uuid2222','uuid3333'],
            'stream_id':['stream1111','stream2222','stream3333'],
            'airfoil_region':['airfoil1','airfoil2','airfoil3'],
            'processed':[False,False,False]
            }
        expected_schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('Defect_Type', StringType(), True),
            StructField('size', DoubleType(), True),
            StructField('disposition', StringType(), True),
            StructField('Indication_Type', StringType(), True),
            StructField('Short_text', StringType(), True),
            StructField('x', DoubleType(), True),
            StructField('y', DoubleType(), True),
            StructField('z', DoubleType(), True),
            StructField('Analytics_Confidence', DoubleType(), True),
            StructField('uuid', StringType(), True),
            StructField('stream_id', StringType(), True),
            StructField('airfoil_region', StringType(), True),
            StructField('processed', BooleanType(), True)
        ])
        rows_exp = [
            {'part_number':pn,'serial_number':sn,'Defect_Type': dt, 'size':s ,'disposition':dis, 'Indication_Type': it,'Short_text':st,'x':x,'y':y,'z':z,'Analytics_Confidence':ac,'angle':an,'elongation':el,'uuid':uu,'stream_id':si,'airfoil_region':ar,'processed':pr}
            for pn,sn,dt, s, dis, it, st, x, y, z, ac, an, el, uu, si, ar, pr in zip(
                expected_dictionary['part_number'],
                expected_dictionary['serial_number'],
                expected_dictionary['Defect_Type'],
                expected_dictionary['size'],
                expected_dictionary['disposition'],
                expected_dictionary['Indication_Type'],
                expected_dictionary['Short_text'],
                expected_dictionary['x'],
                expected_dictionary['y'],
                expected_dictionary['z'],
                expected_dictionary['Analytics_Confidence'],
                expected_dictionary['angle'],
                expected_dictionary['elongation'],
                expected_dictionary['uuid'],
                expected_dictionary['stream_id'],
                expected_dictionary['airfoil_region'],
                expected_dictionary['processed']
            )
        ]
        expected_data_df = spark.createDataFrame(rows_exp,expected_schema)
        return expected_data_df

    def defect_rawtocurated(self):
        df_tag = spark.read.table(f"dmro_raw_tag")
        # df_tag = spark.sql(f"select * from dmro_raw.tag{v}")
        # print("tag from raw")
        # df_tag.display()
        df_tag.createOrReplaceTempView("tag")
        df_def = spark.read.table(f"dmro_raw_defect")
        # print("defect from raw")
        # df_def.display()
        df_def.createOrTeplaceTempView('defect')
        df_ppi = spark.read.table(f"dmro_curated_ppi_airfoil_region")
        # df_def.createOrReplaceTempView("defect")
        df_ppi.createOrReplaceTempView("ppi")

        exception_df=df_def.filter((df_def['disposition'].isNull() == True) | (df_def['indication_type'].isNull() == True))
        if exception_df.count()>0:
            reason_df = exception_df.withColumn("reason", lit("disposition or indication type is null"))

        df_def = df_def.filter((df_def['disposition'].isNotNull()) & (df_def['indication_type'].isNotNull()))
        df_def.createOrReplaceTempView("defect")
        # print("here")
        # df_def.display()
        # df_def.display()

        # print("joining")
        # sql statement with joins from tag to defect and ppi based on max timestamp
        df = spark.sql("""
            select tg.part_number,
                tg.serial_number,
                df.Defect_Type as defect_type,
                df.size,
                df.disposition as defect_disposition,
                df.Indication_Type as ppi_region,
                df.Short_text as short_text,
                df.x,
                df.y,
                df.z,
                df.Analytics_Confidence as analytics_confidence,
                df.angle,
                df.elongation,
                df.uuid,
                df.stream_id,
                pp.airfoil_region
            from tag as tg
            left outer join defect as df
            on df.stream_id = tg.stream_id 
            left outer join ppi as pp
            on pp.PPI_region = rtrim(df.Indication_Type) and pp.part_number = tg.part_number
            where tg.processed = False
        """)

        # df = spark.sql("""select tg.part_number, df.`Indication_Type` as ppi_region, pp.airfoil_region from tag as tg left outer join defect as df on df.stream_id = tg.stream_id left outer join ppi as pp on pp.PPI_region = df.`Indication Type`""")

        # print("defect joined table")
        # df.schema
        # df.display()

        #input new current timestamp
        if df.count != 0:
            df = df.withColumn("load_timestamp", F.current_timestamp())\
                    .withColumn("processed", lit(False))\
                    .withColumn("size",df.size.cast(DoubleType()))\
                    .withColumn("x",df.x.cast(DoubleType()))\
                    .withColumn("y",df.y.cast(DoubleType()))\
                    .withColumn("z",df.z.cast(DoubleType()))\
                    .withColumn("analytics_confidence",df.analytics_confidence.cast(DoubleType()))\
                    .withColumn("angle",df.angle.cast(DoubleType()))\
                    .withColumn("elongation",df.elongation.cast(DoubleType()))

            # print("writing defect curated table")
            # df.display()
        return df

    def test_defect_rawtocurated(self):
        expected_df = self.mock_data_defect_rawtocurated()
        output_df= self.defect_rawtocurated()
        print('expected')
        display(expected_df)
        #assertion statement for load_timestamp
        load_timestamp_datatype= output_df.schema['load_timestamp'].dataType
        self.assertIsInstance(load_timestamp_datatype,TimestampType())
        #drop load time stamp
        output_df= output_df.drop('load_timestamp')
        print('output')
        display(output_df)
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

 
if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)


######################################
import warnings
import unittest
import re
import pandas
from pyspark.sql import functions  as F
from pyspark.sql.functions import col,udf,lit,to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType, DoubleType, BooleanType, TimestampType, DateType
from pyspark.sql.window import Window

warnings.filterwarnings("ignore")

class UnitTestFunctions(unittest.TestCase):

    ##inspection raw to curated
    def mock_data_inspection_rawtocurated(self):
        #input data
        input_data={
            'part_number':['part1111','part2222','part3333'],
            'serial_number':['serial111','serial222','serial333'],
            'automated_disposition':['Pass','Fail','Pass'],
            'create_station':['StationA','StationB','StationC'],
            'inspection_index':[1,2,3],
            'inspector_disposition':['Accept','Reject','Accept'],
            'inspection_facility':['Facility1','Facility2','Facility3'],
            'operation_code':[101,None,103],
            'ppi_type':['PPI','PPI','PPI'],
            'shop_visit':[1,0,1],
            'stream_id':['streamid_11','streamid_22','streamid_33'],
            'create_time':['2023-06-06T01:01:01.581+0000','2023-07-07T02:02:02.581+0000','2023-08-08T03:03:03.581+0000'],
            'processed':[False,False,False]
            }
        schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('create_time', StringType(), True),
            StructField('processed',BooleanType(),True)
        ])
        rows_inp = [
            {'part_number': pn, 'serial_number':sn ,'automated_disposition':ad, 'create_station': cs,'inspection_index':ii,
             'inspector_disposition':id,'inspection_facility':insf,'operation_code':oc,'ppi_type':pp,'shop_visit':sv,'stream_id':si,
             'create_time':ct,'processed':pr}
            for pn, sn, ad, cs, ii,id,insf ,oc,pp,sv,si,ct,pr in zip(
                input_data['part_number'],
                input_data['serial_number'],
                input_data['automated_disposition'],
                input_data['create_station'],
                input_data['inspection_index'],
                input_data['inspector_disposition'],
                input_data['inspection_facility'],
                input_data['operation_code'],
                input_data['ppi_type'],
                input_data['shop_visit'],
                input_data['stream_id'],
                input_data['create_time'],
                input_data['processed']
            )
        ]
        mock_data_input = spark.createDataFrame(rows_inp, schema=schema)
        mock_data_input.createOrReplaceTempView("dmro_raw_tag")
        expected_data={
            'part_number':['part1111','part2222','part3333'],
            'serial_number':['serial111','serial222','serial333'],
            'automated_disposition':['Pass','Fail','Pass'],
            'create_station':['StationA','StationB','StationC'],
            'inspection_index':[1,2,3],
            'inspector_disposition':['Accept','Reject','Accept'],
            'inspection_facility':['Facility1','Facility2','Facility3'],
            'operation_code':[101,None,103],
            'ppi_type':['PPI','PPI','PPI'],
            'shop_visit':[1,0,1],
            'stream_id':['streamid_11','streamid_22','streamid_33'],
            'inspection_date':['6/6/2023','7/7/2023','8/8/2023'],
            'inspection_time':['01:01:01','02:02:02','03:03:03'],
            'processed':[False,False,False]
            }

        expected_schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('inspection_date', StringType(), True),
            StructField('inspection_time', StringType(), True),
            StructField('processed', BooleanType(), True)
        ])

        rows_exp=[
            {'part_number': pn, 'serial_number':sn ,'automated_disposition':ad, 'create_station': cs,'inspection_index':ii,'inspector_disposition':ind,'inspection_facility':insf,'operation_code':oc,'ppi_type':pp,'shop_visit':sv,'stream_id':si,'inspection_date':idt,'inspection_time':it,'processed':pr}
            for pn, sn, ad, cs, ii,ind,insf ,oc,pp,sv,si,idt,it,pr in zip(
                expected_data['part_number'],
                expected_data['serial_number'],
                expected_data['automated_disposition'],
                expected_data['create_station'],
                expected_data['inspection_index'],
                expected_data['inspector_disposition'],
                expected_data['inspection_facility'],
                expected_data['operation_code'],
                expected_data['ppi_type'],
                expected_data['shop_visit'],
                expected_data['stream_id'],
                expected_data['inspection_date'],
                expected_data['inspection_time'],
                expected_data['processed']
            )
        ]
        expected_data_df = spark.createDataFrame(rows_exp, schema=expected_schema)

        return expected_data_df

    def inspection_rawtocurated(self):
            
        df_tag = spark.sql(f"select * from dmro_raw_tag where processed = False")

        if df_tag.count != 0:
            exception_df=df_tag.filter((df_tag['operation_code'].isNull() == True) | (df_tag['create_time'].isNull() == True))
            if exception_df.count()>0:
                reason_df = exception_df.withColumn("reason", lit("operation code or create time is null"))

            #create new columns to split up time
            df_tag = df_tag.withColumn('inspection_date', F.date_format(F.col('create_time'), 'yyyy-MM-dd'))\
                    .withColumn('inspection_time', F.date_format(F.col('create_time'), 'HH:mm:ss'))

            # print("print df with columns split")
            # df_tag.display()

            #select the necessary fields
            df_tag = df_tag.select('part_number', 'serial_number', 'automated_disposition', 'create_station', 'inspection_index', 'inspector_disposition', 'inspection_facility', 'operation_code', 'ppi_type', 'shop_visit', 'stream_id', 'inspection_date', 'inspection_time')

            #input new current timestamp 
            df_tag = df_tag.withColumn("load_timestamp", F.current_timestamp()) \
                        .withColumn("processed", lit(False)) \
                        .withColumn("inspection_index",df_tag.inspection_index.cast(IntegerType())) \
                        .withColumn("operation_code",df_tag.operation_code.cast(IntegerType())) \
                        .withColumn("inspection_date",df_tag.inspection_date.cast(DateType())) \
                        .withColumn("shop_visit",df_tag.shop_visit.cast(IntegerType()))

        return df_tag
        
    def test_inspection_rawtocurated(self):
        expected_df = self.mock_data_inspection_rawtocurated()
        output_df= self.inspection_rawtocurated()
        #display(output_df)
        print('exp')
        display(expected_df)
        #assertion statement for load_timestamp
        # load_timestamp_datatype= output_df['load_timestamp']
        # print('helllo',load_timestamp_datatype)
        print('output')
        
        # print(output_df['load_timestamp'].schema)
        # self.assertIsInstance(output_df.schema['load_timestamp'],TimestampType())
        #drop load time stamp
        output_df= output_df.drop('load_timestamp')
        display(output_df)
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))
    
    
if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)


########################################################
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType
from pyspark.sql.functions import lit

# Assuming 'spark' is your SparkSession
spark = SparkSession.builder.master("local").appName("mock_data").getOrCreate()

class UnitTestFunctions:
    
    def mock_data_inspection_rawtocurated(self):
        # Input mock data
        data = [
            ('part123', 'serial456', 'Pass', 'StationA', 1, 'Accept', 'Facility1', 101, 'PPI', 1, '20230607-163421_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-07', '12:34:56'),
            ('part456', 'serial789', 'Fail', 'StationB', 2, 'Reject', 'Facility2', 102, 'PPI', 0, '20230608-123456_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-08', '14:45:30'),
            ('part789', 'serial123', 'Pass', 'StationC', 3, 'Accept', 'Facility3', 103, 'PPI', 1, '20230609-045612_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-09', '09:15:00')
        ]

        schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('inspection_date', StringType(), True),
            StructField('inspection_time', StringType(), True),
        ])

        mock_data_input = spark.createDataFrame(data, schema=schema)

        # Expected mock data
        expected_data = [
            ('part123', 'serial456', 'Pass', 'StationA', 1, 'Accept', 'Facility1', 101, 'PPI', 1, '20230607-163421_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-07', '12:34:56', '2023-12-12T00:00:00', False),
            ('part456', 'serial789', 'Fail', 'StationB', 2, 'Reject', 'Facility2', 102, 'PPI', 0, '20230608-123456_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-08', '14:45:30', '2023-12-12T00:00:00', False),
            ('part789', 'serial123', 'Pass', 'StationC', 3, 'Accept', 'Facility3', 103, 'PPI', 1, '20230609-045612_375fabcd-afd7-4737-be57-3e67218a823f', '2023-06-09', '09:15:00', '2023-12-12T00:00:00', False)
        ]

        expected_schema = StructType([
            StructField('part_number', StringType(), True),
            StructField('serial_number', StringType(), True),
            StructField('automated_disposition', StringType(), True),
            StructField('create_station', StringType(), True),
            StructField('inspection_index', IntegerType(), True),
            StructField('inspector_disposition', StringType(), True),
            StructField('inspection_facility', StringType(), True),
            StructField('operation_code', IntegerType(), True),
            StructField('ppi_type', StringType(), True),
            StructField('shop_visit', IntegerType(), True),
            StructField('stream_id', StringType(), True),
            StructField('inspection_date', StringType(), True),
            StructField('inspection_time', StringType(), True),
            StructField('load_timestamp', StringType(), True),
            StructField('processed', BooleanType(), True),
        ])

        expected_data_df = spark.createDataFrame(expected_data, schema=expected_schema)

        return mock_data_input, expected_data_df

# Example of using the mock data function
unit_test_instance = UnitTestFunctions()
input_data, expected_output = unit_test_instance.mock_data_inspection_rawtocurated()
input_data.show()
expected_output.show()

########################################################################################
import pandas as pd
json_data = [
    {
        "inspection.uuid": "abc123",
        "obs.uuid": "123"
    },
    {
        "inspection.uuid": None,
        "obs.uuid": "456"
    },
    {
        "inspection.uuid": "ghi789",
        "obs.uuid": None
    }
]

df = pd.DataFrame(json_data)

def preprocess_validation_ppi(df_validations):
    exclude_files_dict = {'reason': [], 'value': []}

    for index, row in df_validations.iterrows():
        if pd.isnull(row['inspection.uuid']):
            exclude_files_dict['reason'].append('inspection.uuid value is Null')
            exclude_files_dict['value'].append(row.to_dict())
            df_validations

        if pd.isnull(row['obs.uuid']):
            exclude_files_dict['reason'].append('obs.uuid value is Null')
            exclude_files_dict['value'].append(row.to_dict())

        if pd.isnull(row['inspection.partNumber']):
            exclude_files_dict['reason'].append('inspection.partNumber value is Null')
            exclude_files_dict['value'].append(row.to_dict())

        if pd.isnull(row['inspection.partserialNumber']):
            exclude_files_dict['reason'].append('inspection.partserialNumber value is Null')
            exclude_files_dict['value'].append(row.to_dict())

        if pd.isnull(row['inspection.engineSerialNumber']):
            exclude_files_dict['reason'].append('inspection.engineSerialNumber value is Null')
            exclude_files_dict['value'].append(row.to_dict())
            
        if pd.isnull(row['obs.servicableLimit']):
            exclude_files_dict['reason'].append('obs.servicableLimit value is Null')
            exclude_files_dict['value'].append(row.to_dict())
        
        if pd.isnull(row['attribute.name']):
            exclude_files_dict['reason'].append('obs.servicableLimit value is Null')
            exclude_files_dict['value'].append(row.to_dict())

    return exclude_files_dict


excluded_files, exclude_files_dict = preprocess_validation_ppi(df)


print("Exclude Files Dictionary:")
display(pd.DataFrame(exclude_files_dict))

#########################################################
import warnings
import unittest
from pyspark.sql import functions as F
from pyspark.sql.functions import col

warnings.filterwarnings("ignore", category=DeprecationWarning, module="distutils")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pyarrow")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pyspark/sql/pandas/utils.py")



class UnitTestFunctions(unittest.TestCase):

    def mock_data(self):
        dictionary = {'event_id': '111111',
                      'event_name': 'new_event',
                      'engine_program': 'engine_new',
                      'data_inspection_uuid': '111-uuid',
                      'data_inspection_createdAt': '11/10/2023',
                      'data_inspection_name':'dummy_inspection_name',
                      'data_inspection_template_techData_id':'dummmy_description',
                      'data_inspection_part_partNumber':'1122334455',
                      'data_inspection_part_serialNumber':'SN1234567',
                      'data_inspection_part_batchNumber':'BN12345',
                      'data_inspection_part_tac':'tac1234',
                      'data_inspection_part_eot':'eot1234',
                      'data_inspection_part_efh':'efh1234',
                      'data_inspection_part_cso':'cso1234',
                      'data_inspection_part_tso':'tso1234',
                      'data_inspection_part_engineModel':'dummy_model',
                      'data_inspection_part_engineSerialNumber':'ESN12345',
                      'data_inspection_disposition_name':'dummy_disposition',
                      'data_observation_uuid':'222-uuid',
                      'data_observation_areaPath':'dummy_areaPath',
                      'data_observation_attributes_type':'dummy',
                      'data_observation_adHocArea':'',
                      'data_observation_conditionName':'',
                      'data_observation_standardConditionName':'',
                      'data_observation_disposition':'',
                      'data_observation_noneObserved':'',
                      'data_observation_servicableLimit':'',
                      'data_observation_repairableLimit':'',
                      'data_observation_attributes_name':'',
                      'data_observation_attributes_value':'',       
                      'data_observation_attributes_adHoc':''
                      }
        mock_data_df = spark.createDataFrame([dictionary])
        mock_data_df.createOrReplaceTempView("raw_table")
        return mock_data_df

    def function_event_raw_to_curated(self, raw_table):
        df_event = spark.sql(f"""
            select distinct(event_id),
                event_name,
                engine_program
            from {raw_table}
        """)
        return df_event
    
    def test_event_raw_to_curated(self):
        mock_df = self.mock_data()
        output_df = self.function_event_raw_to_curated("raw_table")
        #display(output_df)
        expected_df = mock_df.select(col('event_id'),
                                     col('event_name'),
                                     col('engine_program')).distinct()
        #display(expected_df)
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

    def function_inspection_raw_to_curated(self, raw_table):
        output_df = spark.sql(f"""
            select distinct(data_inspection_uuid) as insp_uuid,
                data_inspection_createdAt as insp_timestamp,
                data_inspection_name as insp_name,
                data_inspection_template_techData_id as insp_description,
                data_inspection_part_partNumber as insp_part_number,
                data_inspection_part_serialNumber as insp_serial_number,
                data_inspection_part_batchNumber as insp_batch_number,
                data_inspection_part_tac as insp_part_tac,
                data_inspection_part_eot as insp_part_eot,
                data_inspection_part_efh as insp_part_efh,
                data_inspection_part_cso as insp_part_cso,
                data_inspection_part_tso as insp_part_tso,
                data_inspection_part_engineModel as insp_engine_model,
                data_inspection_part_engineSerialNumber as insp_engine_serial_number,
                data_inspection_disposition_name as insp_disposition_name
            from {raw_table}
        """)

        return output_df

    def test_inspection_raw_to_curated(self):
        mock_data = self.mock_data()
        output_df = self.function_inspection_raw_to_curated("raw_table")
        #display(output_df)

        expected_df = mock_data.select(
            col('data_inspection_uuid').alias('insp_uuid'),
            col('data_inspection_createdAt').alias('insp_timestamp'),
            col('data_inspection_name').alias('insp_name'),
            col('data_inspection_template_techData_id').alias('insp_description'),
            col('data_inspection_part_partNumber').alias('insp_part_number'),
            col('data_inspection_part_serialNumber').alias('insp_serial_number'),
            col('data_inspection_part_batchNumber').alias('insp_batch_number'),
            col('data_inspection_part_tac').alias('insp_part_tac'),
            col('data_inspection_part_eot').alias('insp_part_eot'),
            col('data_inspection_part_efh').alias('insp_part_efh'),
            col('data_inspection_part_cso').alias('insp_part_cso'),
            col('data_inspection_part_tso').alias('insp_part_tso'),
            col('data_inspection_part_engineModel').alias('insp_engine_model'),
            col('data_inspection_part_engineSerialNumber').alias('insp_engine_serial_number'),
            col('data_inspection_disposition_name').alias('insp_disposition_name')).distinct()
        #display(expected_df)
        
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))
    
    def function_observation_raw_to_curated(self, raw_table):
        output_df = spark.sql(f"""
            select distinct data_observation_uuid as obs_uuid,
                data_observation_areaPath as obs_distress_name,
                data_observation_attributes_type as obs_type,
                data_observation_adHocArea as obs_adHocArea,
                data_observation_conditionName as obs_condition_name,
                data_observation_standardConditionName as obs_condition_standard_name,
                data_observation_disposition as obs_disposition_type,
                data_observation_noneObserved as obs_none_observed,
                data_observation_servicableLimit as obs_serviceable_limit_description,
                data_observation_repairableLimit as obs_repairable_limit_description
            from {raw_table}
        """)
        return output_df

    def test_observation_raw_to_curated(self):
        mock_data = self.mock_data()
        output_df = self.function_observation_raw_to_curated("raw_table")
        #display(output_df)

        expected_df = mock_data.select(
            col('data_observation_uuid').alias('obs_uuid'),
            col('data_observation_areaPath').alias('obs_distress_name'),
            col('data_observation_attributes_type').alias('obs_type'),
            col('data_observation_adHocArea').alias('obs_adHocArea'),
            col('data_observation_conditionName').alias('obs_condition_name'),
            col('data_observation_standardConditionName').alias('obs_condition_standard_name'),
            col('data_observation_disposition').alias('obs_disposition_type'),
            col('data_observation_noneObserved').alias('obs_none_observed'),
            col('data_observation_servicableLimit').alias('obs_serviceable_limit_description'),
            col('data_observation_repairableLimit').alias('obs_repairable_limit_description'))
        
        #display(expected_df)
        
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))



if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)

warnings.resetwarnings()
#######################################################******************************************88



import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col

###########################################

@pytest.fixture
def mock_data():
    dictionary = {'event_id':'111111',
    'event_name':'new_event',
    'engineProgram':'engine_new',
    'data_inspection_uuid':'111-uuid',
    'data_inspection_createdAt':'11/10/2023',
    'data_inspection_name':'dummy_inspection_name',
    'data_inspection_template_techData_id':'dummmy_description',
    'data_inspection_part_partNumber':'1122334455',
    'data_inspection_part_serialNumber':'SN1234567',
    'data_inspection_part_batchNumber':'BN12345',
    'data_inspection_part_tac':'tac1234',
    'data_inspection_part_eot':'eot1234',
    'data_inspection_part_efh':'efh1234',
    'data_inspection_part_cso':'cso1234',
    'data_inspection_part_tso':'tso1234',
    'data_inspection_part_engineModel':'dummy_model',
    'data_inspection_part_engineSerialNumber':'ESN12345',
    'data_inspection_disposition_name':'dummy_disposition',
    'data_observation_uuid':'222-uuid',
    'data_observation_areaPath':'dummy_areaPath',
    'data_observation_attributes_type':'dummy',
    'data_observation_adHocArea':'',
    'data_observation_conditionName':'',
    'data_observation_standardConditionName':'',
    'data_observation_disposition':'',
    'data_observation_noneObserved':'',
    'data_observation_servicableLimit':'',
    'data_observation_repairableLimit':'',
    'data_observation_attributes_name':'',
    'data_observation_attributes_value':'',       
    'data_observation_attributes_adHoc':''

    }
    mock_data_df = spark.createDataFrame([dictionary])
    mock_data_df.createOrReplaceTempView("raw_table")




#write the input and output here



###########################################
def function_event_raw_to_curated(raw_table):
    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
    """)
    return df_event

###########################################
def test_event_raw_to_curated(mock_data):
    output_df = function_event_raw_to_curated("raw_table")

    expected_df = mock_data.select(col('data_inspection_uuid').alias('event_id'),
                                   col('data_event_name').alias('event_name'),
                                   col('data_event_engineProgram').alias('engine_program')).distinct()

    # Assuming you want to compare the data, you can convert the DataFrames to Pandas and compare
    assert output_df.toPandas().equals(expected_df.toPandas())

def function_inspection_raw_to_curated(mock_data):
    output_df = spark.sql(f"""
        select distinct(data_inspection_uuid) as insp_uuid,
            data_inspection_createdAt as insp_timestamp,
            data_inspection_name as insp_name,
            data_inspection_template_techData_id as insp_description,
            data_inspection_part_partNumber as insp_part_number,
            trim(data_inspection_part_serialNumber) as insp_serial_number,
            data_inspection_part_batchNumber as insp_batch_number,
            data_inspection_part_tac as insp_part_tac,
            data_inspection_part_eot as insp_part_eot,
            data_inspection_part_efh as insp_part_efh,
            data_inspection_part_cso as insp_part_cso,
            data_inspection_part_tso as insp_part_tso,
            data_inspection_part_engineModel as insp_engine_model,
            trim(data_inspection_part_engineSerialNumber) as insp_engine_serial_number,
            data_inspection_disposition_name as insp_disposition_name
        from {raw_table}
    """)

    return output_df



# def function_timestamp_inspection_raw_to_curated(mock_data):
#     output_df = df_mock_data.withColumn("insp_timestamp",F.to_timestamp("insp_timestamp", "M/d[d]/yyyy H[H]:mm")) \
#         .withColumn("load_timeStamp", F.current_timestamp()) \
#         .withColumn("processed", F.lit(False))

#     expected_df =spark.sql(f"""
#         select *,
#         TO_TIMESTAMP(insp_timestamp,'MM/DD/YYYY HH24:MI') as insp_timestamp,
#         CURRENT_TIMESTAMP() AS load_timstamp,False as processed
#         from raw_table
#     """) 
    
def test_inspection_raw_to_curated(mock_data):
    output_df = function_one_of_inspection_raw_to_curated()

    expected_df = mock_data_df.select(col('data_inspection_uuid').alias('insp_uuid')/
    ,col('data_inspection_name').alias('insp_name')/
    ,col('data_inspection_template_techData_id').alias('insp_description')/
    ,col('data_inspection_part_partNumber').alias('insp_part_number')/
    ,trim(col('data_inspection_part_serialNumber')).alias('insp_serial_number')/
    ,col('data_inspection_part_batchNumber').alias('insp_batch_number')
    ,col('data_inspection_part_tac').alias('insp_part_tac')/
    ,col('data_inspection_part_eot').alias('insp_part_eot')/
    ,col('data_inspection_part_cso').alias('insp_part_cso')/
    ,col('data_inspection_part_tso').alias('insp_part_tso')/
    ,col('data_inspection_part_engineModel').alias('insp_engine_model')/
    ,trim(col('data_inspection_part_engineSerialNumber')).alias('insp_engine_serial_number')/
    ,col('data_inspection_disposition_name').alias('insp_disposition_name').distinct()
    )
    assert output_df == expected_df
    #function_two_of_inspection_raw_to_curated(mock_data)
    return 'Inspection Raw to Curated Function Passed'



############################################    
def function_observation_raw_to_curated(mock_data):
    output_df = spark.sql(f"""
        select distinct data_observation_uuid as obs_uuid,
            data_observation_areaPath as obs_distress_name,
            data_observation_attributes_type as obs_type,
            data_observation_adHocArea as obs_adHocArea,
            data_observation_conditionName as obs_condition_name,
            data_observation_standardConditionName as obs_condition_standard_name,
            data_observation_disposition as obs_disposition_type,
            data_observation_noneObserved as obs_none_observed,
            data_observation_servicableLimit as obs_serviceable_limit_description,
            data_observation_repairableLimit as obs_repairable_limit_description
        from {raw_table}
    """)
    return output_df


def test_observation_raw_to_curated(mock_data):
    output_df=function_observation_raw_to_curated(mock_data)

    expected_df= mock_data_df.select(col('data_observation_uuid').alias('obs_uuid')/
    ,col('data_observation_areaPath').alias('obs_distress_name')/
    ,col('data_observation_attributes_type').alias('obs_type')/
    ,col('data_observation_adHocArea').alias('obs_adHocArea')/
    ,col('data_observation_conditionName').alias('obs_condition_name')
    ,col('data_observation_standardConditionName').alias('obs_condition_standard_name')/
    ,col('data_observation_disposition').alias('obs_disposition_type')/
    ,col('data_observation_noneObserved').alias('obs_none_observed')/
    ,col('data_observation_servicableLimit').alias('obs_serviceable_limit_description')/
    ,col('data_inspection_part_engineModel').alias('insp_engine_model')/
    ,col('data_observation_repairableLimit').alias('obs_repairable_limit_description').distinct()
    )
    return 'Observation Raw to Curated Function Passed'
    

@pytest.fixture
def mock_data_attribute():
    dictionary_attribute_input = {
        'data_observation_uuid':['0rf3h574-234i-x439-4hdc8dfkls87','r1f3h574-10x4-x439-4hdc8djfhy13','hdf1sd5y-se89-1m4n-9dja9ce8u4hu'],
        'data_observation_attributes_name':['radius','comment',None],
        'data_observation_attributes_value':['0.323','Noticed tear on contract face',None],
        'data_observation_attributes_adHoc': [None,None,None]
    }
    df_input = spark.createDataframe([dictionary_attribute_input])
    df_input.createOrReplaceTempView('raw_table_attribute')
    dictionary_attribute_expected = {
        'observation_uuid':['0rf3h574-234i-x439-4hdc8dfkls87','r1f3h574-10x4-x439-4hdc8djfhy13','hdf1sd5y-se89-1m4n-9dja9ce8u4hu'],
        'attributes_name':['radius','comment',None],
        'value':['0.323',None,None],
        'comment':[None,'Noticed tear on contract face',None]
    }
    expected_df = spark.createDataframe([dictionary_attribute_expected])

def function_attribute_raw_to_curated(mock_data_attribute):
    # The following query returns duplicate records without the group by all columns clause
    df_attribute = spark.sql(f"""
        select  data_observation_uuid as observation_uuid,
            data_observation_attributes_name as attribute_name,
            case
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then data_observation_attributes_value
                else none
            end as value,
            case   
                when data_observation_attributes_value regexp '^[0-9]*\.?[0-9]+?' then null
                else data_observation_attributes_value
            end as comment,
            data_observation_attributes_adHoc as attribute_ad_hoc
        from {raw_table_attribute}
        group by observation_uuid,
            attribute_name,
            value,
            comment,
            attribute_ad_hoc
    """)
    return df_attribute


def test_attribute_raw_to_curated(mock_data_attribute):
    output_df = function_attribute_raw_to_curated(df_input)
    assert output_df.toPandas().equals(expected_df.toPandas())

######################################################################
# @pytest.fixture
# def mock_data_limit():
#     min_max_udf = F.udf(lambda x: extract_min_max(x), StringType())
#     #need to write input and expected data for testing function_one_of_limit_raw_to_curated
#     input_df_funtion_one = 
#     expected_df_function_one = 
#     output_df_function_one = function_one_of_limit_raw_to_curated(input_df_funtion_one)
#     assert expected_df_function_one == output_df_function_one

#     #need to write input and expected data for testing function_two_of_limit_raw_to_curated
#     input_df_funtion_two =  
#     expected_df_function_two = 
#     output_df_function_two = function_two_of_limit_raw_to_curated(input_df_funtion_two)
#     assert expected_df_function_two == output_df_function_two

#     #need to write input and expected data for testing function_two_of_limit_raw_to_curated
#     input_df_funtion_three = 
#     expected_df_function_three =
#     output_df_function_three = function_three_of_limit_raw_to_curated(input_df_funtion_three)
#     assert expected_df_function_three == output_df_function_three

@pytest.fixture
def mock_data_limit():
    dictionary_attribute_input = {
        'data_observation_areaPath':['/ADJUNCT SERVICES','/[3] INNER DIAMETER LOOP','/[11] UNDERRISE OF COOLING HOSE','/ANOTHER SERVICES'],
        'data_observation_conditionName':['SHARK HEDGES','NACKS','CRAZED MATERIAL','SCRAPES'],
        'data_inspection_createdAt':['2023-01-04-t21:58:17.461Z','2023-01-10-t35:37:18:501Z','2023-01-10-t35:37:18:501Z','2023-01-04-t21:58:17.461Z'],
        'data_observation_servicableLimit': ['Not remitted','0.89 inch (23.354 mm) maximum and radius must be minimum of .454 inch (16.234 mm)','1.445 - 1.983 inch (4.320 - 6.322 mm)','0.039 inch (2.402 mm) minimum']
    }
    df_input = spark.createDataframe([dictionary_attribute_input])
    df_input.createOrReplaceTempView('raw_table_limit')
    dictionary_attribute_expected = {
        'lim_distress_name':['/ADJUNCT SERVICES','/[3] INNER DIAMETER LOOP','/[11] UNDERRISE OF COOLING HOSE','/ANOTHER SERVICES'],
        'lim_condition_name':['SHARK HEDGES','NACKS','CRAZED MATERIAL','SCRAPES'],
        'lim_minimum':[None,None,1.445,0.039],
        'lim_maximum':[None,0.897,1.983,None],
        'lim_date_changed':['2023-01-04-t21:58:17.461Z','2023-01-10-t35:37:18:501Z','2023-01-10-t35:37:18:501Z','2023-01-04-t21:58:17.461Z'],
        'lim_comment':['Not remitted','0.897 inch (23.354 mm) maximum depth is remitted','1.445 - 1.983 inch (4.320 - 6.322 mm)','0.039 inch (2.402 mm) minimum']
    }
    expected_df = spark.createDataframe([dictionary_attribute_expected])
        

def extract_min_max(limit: str):
    _max = "null"
    _min = "null"
    regex = u"([0-9]+\.[0-9]+)\s+inch"
    try:
        limits = re.findall(regex, limit)
    except TypeError:
        return
    
    limit = limit.lower()
    if "maximum" not in limit and "no longer than" not in limit:
        regex = u"([0-9]+\.[0-9]+)(?:\s*-\s*([0-9]+\.[0-9]+))?\s+inch"
        limits = re.findall(regex, limit)
        try:
            _min = limits[0][0]
            _max = limits[0][1]
        except IndexError:
            pass

    else:
        _max = limits[0]
        if "minimum" in limit:
            _min = limits[1]
    return f"{_min}, {_max}"
##################################################
def function_one_of_limit_raw_to_curated():
    df_limits = spark.sql("""
        with latest_inspection as (
            select
                data_observation_areaPath,
                data_observation_conditionName,
                max(data_inspection_createdAt::timestamp) as latest_insp
            from pwi_raw.initial_10_17
            group by 1, 2
        ),

        full_table_latest_inspection as (
            -- join cte back to raw table in order to find the servicable limit with an inspection date that matches the latest
            select
                raw.data_observation_areaPath as lim_distress_name,
                raw.data_observation_conditionName as lim_condition_name,
                raw.data_inspection_createdAt::timestamp as lim_date_changed,
                raw.data_observation_servicableLimit as lim_comment
            from pwi_raw.initial_10_17 as raw
            left join latest_inspection
                on raw.data_observation_areaPath = latest_inspection.data_observation_areaPath
                and raw.data_observation_conditionName = latest_inspection.data_observation_conditionName
            where raw.data_inspection_createdAt::timestamp = latest_inspection.latest_insp
            group by
                lim_distress_name,
                lim_condition_name,
                lim_date_changed,
                lim_comment
        ),

        limits_w_count as (
            -- add a count of limits (i.e. count of different lim_comment values) per lim_distress_name/lim_condition_name/lim_date_changed combination
            select lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment,
            count(*) over (partition by lim_distress_name, lim_condition_name, lim_date_changed order by lim_distress_name) as limit_count
            from full_table_latest_inspection
        )

        -- for conflicting servicable limits (identified by having limit_count > 1), exclude those with value of 'not permitted'; this resolves most limit conflicts
        select
            lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment
        from limits_w_count
        where limit_count = 1 or (
            limit_count > 1 and lower(lim_comment) != 'not permitted'
        )
        order by limit_count desc, lim_distress_name, lim_condition_name
    """)
    return df_limits

def function_two_of_limit_raw_to_curated():
    df_limits = df_limits.withColumn("min_max", min_max_udf(F.col("lim_comment")))
    split_col = F.split(df_limits["min_max"], ", ")
    df_limits = df_limits.withColumn(
            "lim_minimum",
            F.when(split_col.getItem(0) == "null", None) \
                .otherwise(split_col.getItem(0)) \
                    .cast(FloatType())
    ) \
        .withColumn(
            "lim_maximum",
        F.when(split_col.getItem(1) == "null", None) \
            .otherwise(split_col.getItem(1)) \
                    .cast(FloatType())
        )
    return df_limits

def function_three_of_limit_raw_to_curated():
    df_limits = df_limits.select(
        "lim_distress_name",
        "lim_condition_name",
        "lim_minimum",
        "lim_maximum",
        "lim_date_changed",
        "lim_comment"
    )

    #df_limits still contains a few conflicting limits, 
    # so we select the lowest lim_minimum value per lim_distress_name/lim_condition_name/lim_date_changed combination and choose the record with the matching lim_minimum
    
    window_spec = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_distress_name")

    df_limits = df_limits.withColumn("min_minimum", F.min("lim_minimum").over(window_spec))

    df_filtered = df_limits.filter((F.col("lim_minimum").isNull()) | (F.col("lim_minimum") == F.col("min_minimum")))

    #in at least one case, two conflicting servicable limits have the same lim_min and lim_max values; in such cases, we simply pick the first

    window_spec_1 = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_date_changed")

    df_filtered = df_filtered.withColumn("row_num", F.row_number().over(window_spec_1))
    df_filtered = df_filtered.filter(F.col("row_num") == 1).drop("row_num")
    return df_filtered

def test_limit_raw_to_curated():

    input_string = "10.451 - 12.059 inches (345.534  - 423.454 mm)"
    expected_string = '10.451, 12.059'
    output_string = extract_min_max(input_string)
    assert output_string == expected_string

    



    



##############################################################
[12:06 PM] Lalit Avinash Bopalkar
%sql
create view if not exists gurit_da.silver_scada_sqlt_data_v as
select /*+ RANGE_JOIN(SourceTable, 10) */ *
  from
(
SELECT cast(dateadd(MINUTE,
                       330,
                       dateadd(second,
                cast(d.t_stamp as bigint)
                /1000,
                TIMESTAMP'1970-01-01 00:00:00'
                ))
                      as timestamp) as TimeStamp
    ,case when d.intvalue is not null then cast(d.intvalue as varchar(100))
          when d.floatvalue is not null then cast(d.floatvalue as varchar(100))
          when d.stringvalue is not null then cast(d.stringvalue as varchar(100))
          when d.datevalue is not null then cast(d.datevalue as varchar(100))
      end as value
    ,left(m.tagpath, charindex('/', m.tagpath) - 1) as extruder_line
    ,substring(m.tagpath,charindex('/', m.tagpath)+1,len(m.tagpath)-charindex('/', m.tagpath)) as parameter
    ,d.Pipeline_run_id
  FROM gurit_da.raw_scada_sqlt_data_d_yyyy_mm d
  inner join gurit_da.raw_scada_sqlth_te m on d.tagid=m.id
) as SourceTable
left join (
   select min(DateTime) as StartTime,
        max(DateTime) as EndTime,
        cast(BatchNo as varchar(100)) as Batch,
        cast(concat('Kerdyn Green ',NominalDensity) as varchar(100)) as Product
    FROM gurit_da.silver_scada_extruder_manual_data
    group by cast(BatchNo as varchar(100)),cast(concat('Kerdyn Green ',NominalDensity) as varchar(100))  
) BatchPeriods on SourceTable.TimeStamp between BatchPeriods.StartTime and BatchPeriods.EndTime
###############################################################
import tracemalloc
import unittest
from pyspark.sql.functions import col
test_data = {'data_event_id': '111111', 'data_event_name': 'new_event', 'data_event_engineProgram': 'engine_new',
             'data_inspection_uuid': '111-uuid', 'data_inspection_createdAt': '11/10/2023',
             'data_inspection_name': 'dummy_inspection_name'}
test_data_df = spark.createDataFrame([test_data])
test_data_df.createOrReplaceTempView("raw_table")

def function_event_raw_to_curated(raw_table):
    df_event = spark.sql(f"""
        SELECT DISTINCT(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        FROM {raw_table}
    """)
    return df_event

class TestEventRawToCurated(unittest.TestCase):
    def test_event_raw_to_curated(self):
        output_df = function_event_raw_to_curated("raw_table")
        expected_df = test_data_df.select(col('data_event_id').alias('event_id'),
                                          col('data_event_name').alias('event_name'),
                                          col('data_event_engineProgram').alias('engine_program')).distinct()
        self.assertTrue(output_df.toPandas().equals(expected_df.toPandas()))

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False, verbosity=2)

####################################################
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col

###########################################

@pytest.fixture
def test_data():
    dictionary = {
        'data_event_id': '111111',
        'data_event_name': 'new_event',
        'data_event_engineProgram': 'engine_new',
        'data_inspection_uuid':'111-uuid',
        'data_inspection_createdAt':'11/10/2023',
        'data_inspection_name':'dummy_inspection_name'
    }
    test_data_df = spark.createDataFrame([dictionary])
    test_data_df.createOrReplaceTempView("raw_table")
    print('hello1')
    return test_data_df

###########################################
def function_event_raw_to_curated(raw_table):
    df_event = spark.sql(f"""
        select distinct(data_event_id) as event_id,
            data_event_name as event_name,
            data_event_engineProgram as engine_program
        from {raw_table}
    """)
    print('hello2')
    return df_event

###########################################
def test_event_raw_to_curated(test_data):
    output_df = function_event_raw_to_curated("raw_table")

    expected_df = test_data.select(col('data_inspection_uuid').alias('event_id'),
                                   col('data_event_name').alias('event_name'),
                                   col('data_event_engineProgram').alias('engine_program')).distinct()

    # Assuming you want to compare the data, you can convert the DataFrames to Pandas and compare
    print('hello3')
    assert output_df.toPandas().equals(expected_df.toPandas())

#############################################################
def limit_raw_to_curated(version, raw_table_version=""):

    v = add_underscore(version)

    raw_table = f"pwi_raw.initial{add_underscore(raw_table_version)}"

    #min and max from the string field
    def extract_min_max(limit: str):
        _max = "null"
        _min = "null"
        regex = u"([0-9]+\.[0-9]+)\s+inch"
        try:
            limits = re.findall(regex, limit)
        except TypeError:
            return
        
        limit = limit.lower()
        if "maximum" not in limit and "no longer than" not in limit:
            regex = u"([0-9]+\.[0-9]+)(?:\s*-\s*([0-9]+\.[0-9]+))?\s+inch"
            limits = re.findall(regex, limit)
            try:
                _min = limits[0][0]
                _max = limits[0][1]
            except IndexError:
                pass

        else:
            _max = limits[0]
            if "minimum" in limit:
                _min = limits[1]
        return f"{_min}, {_max}"
####################################

    min_max_udf = F.udf(lambda x: extract_min_max(x), StringType())

    df_limits = spark.sql("""
        with latest_inspection as (
            select
                data_observation_areaPath,
                data_observation_conditionName,
                max(data_inspection_createdAt::timestamp) as latest_insp
            from pwi_raw.initial_10_17
            group by 1, 2
        ),

        full_table_latest_inspection as (
            -- join cte back to raw table in order to find the servicable limit with an inspection date that matches the latest
            select
                raw.data_observation_areaPath as lim_distress_name,
                raw.data_observation_conditionName as lim_condition_name,
                raw.data_inspection_createdAt::timestamp as lim_date_changed,
                raw.data_observation_servicableLimit as lim_comment
            from pwi_raw.initial_10_17 as raw
            left join latest_inspection
                on raw.data_observation_areaPath = latest_inspection.data_observation_areaPath
                and raw.data_observation_conditionName = latest_inspection.data_observation_conditionName
            where raw.data_inspection_createdAt::timestamp = latest_inspection.latest_insp
            group by
                lim_distress_name,
                lim_condition_name,
                lim_date_changed,
                lim_comment
        ),

        limits_w_count as (
            -- add a count of limits (i.e. count of different lim_comment values) per lim_distress_name/lim_condition_name/lim_date_changed combination
            select lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment,
            count(*) over (partition by lim_distress_name, lim_condition_name, lim_date_changed order by lim_distress_name) as limit_count
            from full_table_latest_inspection
        )

        -- for conflicting servicable limits (identified by having limit_count > 1), exclude those with value of 'not permitted'; this resolves most limit conflicts
        select
            lim_distress_name,
            lim_condition_name,
            lim_date_changed,
            lim_comment
        from limits_w_count
        where limit_count = 1 or (
            limit_count > 1 and lower(lim_comment) != 'not permitted'
        )
        order by limit_count desc, lim_distress_name, lim_condition_name
    """)
    ####################

    df_limits = df_limits.withColumn("min_max", min_max_udf(F.col("lim_comment")))
    split_col = F.split(df_limits["min_max"], ", ")
    df_limits = df_limits.withColumn(
            "lim_minimum",
            F.when(split_col.getItem(0) == "null", None) \
                .otherwise(split_col.getItem(0)) \
                    .cast(FloatType())
    ) \
        .withColumn(
            "lim_maximum",
        F.when(split_col.getItem(1) == "null", None) \
            .otherwise(split_col.getItem(1)) \
                    .cast(FloatType())
        )
###############################
    df_limits = df_limits.select(
        "lim_distress_name",
        "lim_condition_name",
        "lim_minimum",
        "lim_maximum",
        "lim_date_changed",
        "lim_comment"
    )

    # df_limits still contains a few conflicting limits, so we select the lowest lim_minimum value per lim_distress_name/lim_condition_name/lim_date_changed combination and choose the record with the matching lim_minimum
    window_spec = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_distress_name")#

    df_limits = df_limits.withColumn("min_minimum", F.min("lim_minimum").over(window_spec))

    df_filtered = df_limits.filter((F.col("lim_minimum").isNull()) | (F.col("lim_minimum") == F.col("min_minimum")))

    # in at least one case, two conflicting servicable limits have the same lim_min and lim_max values; in such cases, we simply pick the first

    window_spec_1 = Window.partitionBy(
        "lim_distress_name",
        "lim_condition_name",
        "lim_date_changed"
    ).orderBy("lim_date_changed")

    df_filtered = df_filtered.withColumn("row_num", F.row_number().over(window_spec_1))
    df_filtered = df_filtered.filter(F.col("row_num") == 1).drop("row_num")

